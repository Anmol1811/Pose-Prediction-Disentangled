{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "executive-rebel",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c1d5236a60c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    " \n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import glob\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import time\n",
    "\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "soviet-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class myDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, dtype,fname):\n",
    "        \n",
    "        self.args = args\n",
    "        self.dtype = dtype\n",
    "        print(\"Loading\",self.dtype)\n",
    "        sequence_centric = pd.read_csv(fname+self.dtype+\".csv\")\n",
    "        df = sequence_centric.copy()      \n",
    "        for v in list(df.columns.values):\n",
    "            print(v+' loaded')\n",
    "            try:\n",
    "                df.loc[:,v] = df.loc[:, v].apply(lambda x: literal_eval(x))\n",
    "            except:\n",
    "                continue\n",
    "        sequence_centric[df.columns] = df[df.columns]\n",
    "        self.data = sequence_centric.copy().reset_index(drop=True)\n",
    "    \n",
    "        print('*'*30)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        seq = self.data.iloc[index]\n",
    "        outputs = []\n",
    "\n",
    "        obs = torch.tensor([seq.Pose[i] for i in range(0,self.args.input,self.args.skip)])\n",
    "        obs_speed = (obs[1:] - obs[:-1])\n",
    "        outputs.append(obs_speed)\n",
    "        true = torch.tensor([seq.Future_Pose[i] for i in range(0,self.args.output,self.args.skip)])\n",
    "        true_speed = torch.cat(((true[0]-obs[-1]).unsqueeze(0), true[1:]-true[:-1]))\n",
    "        outputs.append(true_speed)\n",
    "        outputs.append(obs)\n",
    "        outputs.append(true)\n",
    "        \n",
    "        return tuple(outputs)    \n",
    "    \n",
    "    \n",
    "def data_loader(args,data,fname):\n",
    "    dataset = myDataset(args,data,fname)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=args.batch_size, shuffle=args.loader_shuffle,\n",
    "        pin_memory=args.pin_memory)\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "honest-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADE_c(pred, true):\n",
    "    b,n,p=pred.size()[0],pred.size()[1],pred.size()[2]\n",
    "#     print(b,n,p)\n",
    "    pred = torch.reshape(pred, (b,n,int(p/2),2))\n",
    "    true = torch.reshape(true, (b,n,int(p/2),2))\n",
    "    \n",
    "    displacement=torch.sqrt((pred[:,:,:,0]-true[:,:,:,0])**2+(pred[:,:,:,1]-true[:,:,:,1])**2)\n",
    "    ade = torch.mean(torch.mean(displacement,dim=1))\n",
    "\n",
    "    return ade\n",
    "\n",
    "\n",
    "def FDE_c(pred, true):\n",
    "    b,n,p=pred.size()[0],pred.size()[1],pred.size()[2]\n",
    "#     print(b,n,p)\n",
    "    pred = torch.reshape(pred, (b,n,int(p/2),2))\n",
    "    true = torch.reshape(true, (b,n,int(p/2),2))\n",
    "    \n",
    "    displacement=torch.sqrt((pred[:,-1,:,0]-true[:,-1,:,0])**2+(pred[:,-1,:,1]-true[:,-1,:,1])**2)\n",
    "\n",
    "    fde = torch.mean(torch.mean(displacement,dim=1))\n",
    "    \n",
    "    return fde\n",
    "\n",
    "def speed2pos(preds, obs_p):\n",
    "    pred_pos = torch.zeros(preds.shape[0], preds.shape[1], preds.shape[2]).to('cuda')\n",
    "    current = obs_p[:,-1,:]\n",
    "    \n",
    "    for i in range(preds.shape[1]):\n",
    "        pred_pos[:,i,:] = current + preds[:,i,:]\n",
    "        current = pred_pos[:,i,:]\n",
    "        \n",
    "    for i in range(preds.shape[2]):\n",
    "        pred_pos[:,:,i] = torch.min(pred_pos[:,:,i], 1920*torch.ones(pred_pos.shape[0], pred_pos.shape[1], device='cuda'))\n",
    "        pred_pos[:,:,i] = torch.max(pred_pos[:,:,i], torch.zeros(pred_pos.shape[0], pred_pos.shape[1], device='cuda'))\n",
    "        \n",
    "    return pred_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "occupied-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseDataset(Dataset):\n",
    "\n",
    "    def __init__(self,keypoints, transform=None):\n",
    "        self.data=keypoints\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = self.data[idx]\n",
    "\n",
    "        sample = {'keypoints': torch.from_numpy(image)}\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "roman-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader,valloader, NUM_EPOCHS, scheduler):\n",
    "    train_loss = []\n",
    "    val_losses = []\n",
    "    max_val_loss=10000\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        counter=0\n",
    "        ade_train=0\n",
    "        ade_val=0\n",
    "        \n",
    "        net.train()\n",
    "        running_loss = 0.0\n",
    "        running_val_loss = 0.0\n",
    "        for data in trainloader:\n",
    "            counter+=1\n",
    "            kp = data['keypoints']\n",
    "            kp = kp.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(kp.float())\n",
    "            \n",
    "            loss = criterion(outputs, kp.float())\n",
    "            \n",
    "            ade_train += float(ADE_c(outputs.unsqueeze(0), kp.unsqueeze(0)))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        loss = running_loss / len(trainloader)\n",
    "        train_loss.append(loss)\n",
    "        \n",
    "        ade_train/=counter\n",
    "        \n",
    "        net.eval()\n",
    "        counter=0\n",
    "        with torch.no_grad():\n",
    "          for data in valloader:\n",
    "              counter+=1\n",
    "              kp = data['keypoints']\n",
    "              kp = kp.to(device)\n",
    "              outputs = net(kp.float())\n",
    "              \n",
    "              ade_val += float(ADE_c(outputs.unsqueeze(0), kp.unsqueeze(0)))\n",
    "              \n",
    "              val_loss = criterion(outputs, kp.float())\n",
    "              running_val_loss += val_loss.item()\n",
    "          \n",
    "        val_loss = running_val_loss / len(valloader)\n",
    "        val_losses.append(val_loss)\n",
    "        ade_val/=counter\n",
    "         \n",
    "\n",
    "        scheduler.step(val_loss) \n",
    "\n",
    "#         if(epoch%20==19):\n",
    "        print('Epoch {} of {}, Train Loss: {:.3f}, Val Loss: {:.3f}, ADE_train : {:.3f},  ADE_val: {:.3f}'.format(\n",
    "            epoch+1, NUM_EPOCHS, loss, val_loss,ade_train, ade_val))\n",
    "              \n",
    "        if val_loss<max_val_loss:\n",
    "          max_val_loss=val_loss\n",
    "          torch.save(net.state_dict(), \"model_clean_auto.weight\")\n",
    "\n",
    "    return train_loss,val_losses\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "overhead-williams",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      "  (enc1): Linear(in_features=544, out_features=300, bias=True)\n",
      "  (enc2): Linear(in_features=300, out_features=300, bias=True)\n",
      "  (dec4): Linear(in_features=300, out_features=544, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # encoder\n",
    "        self.drop =  nn.Dropout(p=0.3)\n",
    "        self.enc1= nn.Linear(in_features=34*16, out_features=300)\n",
    "        self.enc2 = nn.Linear(in_features=300, out_features=300)\n",
    "#         self.enc3 = nn.Linear(in_features=150, out_features=75)\n",
    "#         self.enc4 = nn.Linear(in_features=300, out_features=10)\n",
    "        # decoder \n",
    "#         self.dec1 = nn.Linear(in_features=10, out_features=300)\n",
    "#         self.dec2 = nn.Linear(in_features=75, out_features=150)\n",
    "#         self.dec3 = nn.Linear(in_features=300, out_features=300)\n",
    "        self.dec4 = nn.Linear(in_features=300, out_features=34*16)\n",
    "        \n",
    "    def forward(self, x):\n",
    "                \n",
    "        x = F.relu(self.drop(x))\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "#         x = F.relu(self.enc3(x))\n",
    "#         x = F.relu(self.enc4(x))\n",
    "#         x = F.relu(self.dec1(x))\n",
    "#         x = F.relu(self.dec2(x))\n",
    "#         x = F.relu(self.dec3(x))\n",
    "        x = F.relu(self.dec4(x))\n",
    "        return x\n",
    "net = Autoencoder()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "earlier-commercial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95233\n",
      "4711\n",
      "(4711, 34)\n"
     ]
    }
   ],
   "source": [
    "kpa=[]\n",
    "keypoints=pickle.load(open(\"keypoints_openpifpaf.pickle\",\"rb\"))\n",
    "kpd={}\n",
    "for i in keypoints:\n",
    "\tkpd[i]=np.array(keypoints[i])\n",
    "print(len(kpd))\n",
    "kpa=[]\n",
    "for key in list(kpd.keys()):\n",
    "  x=key.replace(\".png.predictions.json\",\"\").split(\"_\")\n",
    "  vid_num=int(x[2])\n",
    "  frame_num=int(x[3])\n",
    "  ped_num=x[4]\n",
    "  if(len(ped_num)==11):\n",
    "    ped_num=int(x[4][10])\n",
    "  else:\n",
    "    ped_num=0\n",
    "    \n",
    "#   if np.count_nonzero(kpd[key]<=0)<=9:\n",
    "#       kpa.append([vid_num,frame_num,ped_num,kpd[key]])\n",
    "  vec=kpd[key][2::3]\n",
    "  if (vec > 0.25).all():\n",
    "      kpa.append([vid_num,frame_num,ped_num,kpd[key]])\n",
    "\n",
    "\n",
    "keypoints_array_sorted=sorted(kpa,key=lambda e:(e[0],e[2],e[1]))\n",
    "print(len(keypoints_array_sorted))\n",
    "thresarr=[]\n",
    "for i in range(len(keypoints_array_sorted)):\n",
    "  vec=keypoints_array_sorted[i][3][2::3]\n",
    "  if (vec > 0.25).all():\n",
    "      thresarr.append(keypoints_array_sorted[i][3])\n",
    "keypoints=np.array(thresarr)\n",
    "kp_train=np.delete(keypoints, list(range(2, keypoints.shape[1], 3)), axis=1)\n",
    "print(kp_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "timely-estate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4711\n",
      "244\n",
      "244\n",
      "84\n",
      "(84, 2, 16, 51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigbo\\miniconda3\\envs\\pv-lstm\\lib\\site-packages\\ipykernel_launcher.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "key_f = lambda e: (e[0],e[2])\n",
    "kp_seq=[]\n",
    "for key, group in itertools.groupby(keypoints_array_sorted, key_f):\n",
    "    lg=list(group)\n",
    "    # print(len(lg))\n",
    "    kp_seq.append(lg)\n",
    "\n",
    "print(len(keypoints_array_sorted))\n",
    "gps=[]\n",
    "for seq in kp_seq:\n",
    "  l=[]\n",
    "  for i in range(len(seq)):\n",
    "#     print(seq[i][3].shape)\n",
    "    vec=seq[i][3][2::3]\n",
    "    if (vec > 0.25).all():\n",
    "      l.append(keypoints_array_sorted[i][3])\n",
    "  gps.append(l)\n",
    "\n",
    "gps = list(filter(None, gps))\n",
    "print(len(gps))\n",
    "\n",
    "def consecutive(data, stepsize=1):\n",
    "    return np.split(data, np.where(np.diff(data[:,1]) != stepsize)[0]+1)\n",
    "\n",
    "print(len(kp_seq))\n",
    "sequences=[]\n",
    "seq_len=16\n",
    "slide=4\n",
    "c=0\n",
    "for lst in kp_seq:\n",
    "  c+=1\n",
    "  ll=np.array(lst)\n",
    "  x=[np.array(i) for i in consecutive(ll)]\n",
    "  \n",
    "  for tt in x:\n",
    "    if(len(tt)>=seq_len):\n",
    "\n",
    "      pp=[[tt[i:i + seq_len],tt[i+seq_len:i + 2*seq_len]] for i in range(0, len(tt)-(2*seq_len), slide)]\n",
    "      sequences.extend(pp)\n",
    "\n",
    "print(len(sequences))\n",
    "\n",
    "array_of_seq=[]\n",
    "tuples_to_save=[]\n",
    "tups_x=[]\n",
    "tups_y=[]\n",
    "array_of_data=[]\n",
    "for i in sequences:\n",
    "  # print(len(i[0]),len(i[1]))\n",
    "  x=np.array([p[3] for p in i[0]])\n",
    "  y=np.array([p[3] for p in i[1]])\n",
    "  array_of_seq.append([x,y])\n",
    "  tuples_to_save.extend([(p[0],p[1],p[2]) for p in i[0]])\n",
    "  tuples_to_save.extend([(p[0],p[1],p[2]) for p in i[1]])\n",
    "#     tups_y.append()\n",
    "  array_of_data.append([x,y,np.array([(p[0],p[1],p[2]) for p in i[0]]),np.array([(p[0],p[1],p[2]) for p in i[1]])])\n",
    "  \n",
    "array_of_seq=np.array(array_of_seq)\n",
    "print(array_of_seq.shape)\n",
    "# print(len(tuples_to_save),tuples_to_save[0])\n",
    "# array_of_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "closed-cookbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 2, 16, 51)\n",
      "                                                Pose  \\\n",
      "0  [[1731.22, 627.23, 0.65, 1735.22, 622.75, 0.63...   \n",
      "1  [[1534.6, 591.08, 0.86, 1538.25, 587.28, 0.91,...   \n",
      "2  [[1120.28, 697.56, 0.68, 1122.15, 694.75, 0.61...   \n",
      "3  [[292.51, 709.45, 0.9, 296.83, 704.17, 0.83, 2...   \n",
      "4  [[1170.75, 705.09, 0.54, 1173.13, 702.74, 0.56...   \n",
      "\n",
      "                                         Future_Pose  \n",
      "0  [[1775.1, 618.23, 0.82, 1780.46, 613.15, 0.83,...  \n",
      "1  [[1576.73, 582.43, 0.75, 1580.1, 578.38, 0.77,...  \n",
      "2  [[1125.55, 698.53, 0.65, 1127.6, 695.64, 0.59,...  \n",
      "3  [[276.08, 708.12, 0.75, 281.11, 702.83, 0.76, ...  \n",
      "4  [[1069.5, 695.04, 0.28, 1071.41, 692.14, 0.62,...  \n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "seq_out=array_of_seq\n",
    "print(seq_out.shape)\n",
    "sequences_all=[]\n",
    "sequences_obs_speed=[]\n",
    "sequences_true_speed=[]\n",
    "sequences_obs_pose=[]\n",
    "sequences_true_pose=[]\n",
    "sequences_true_imgs=[]\n",
    "sequences_obs_imgs=[]\n",
    "seq_len=16\n",
    "split=int(0.75*len(seq_out))\n",
    "for p in range(len(seq_out)):\n",
    "  outputs = []\n",
    "\n",
    "  observed = array_of_data[p][0]\n",
    "  future = array_of_data[p][1]\n",
    "  obs = torch.tensor([observed[i] for i in range(0,seq_len,1)])\n",
    "  true = torch.tensor([future[i] for i in range(0,seq_len,1)])\n",
    "  sequences_obs_pose.append(np.round(obs.numpy(),2).tolist())\n",
    "  sequences_true_pose.append(np.round(true.numpy(),2).tolist())\n",
    "#   sequences_true_imgs.append(['/frames/'+'video_'+str(i[0]).rjust(4, '0')+\"/\"+str(i[1]).rjust(5, '0')+'.png' for i in array_of_data[p][2]])\n",
    "#   sequences_obs_imgs.append(['/frames/'+'video_'+str(i[0]).rjust(4, '0')+\"/\"+str(i[1]).rjust(5, '0')+'.png' for i in array_of_data[p][3]])\n",
    "#   \n",
    "c = list(zip(sequences_obs_pose, sequences_true_pose))#,sequences_true_imgs,sequences_obs_imgs))\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "sequences_obs_pose, sequences_true_pose = zip(*c) #, sequences_true_imgs, sequences_obs_imgs\n",
    "\n",
    "sequences_all_train=sequences_all[:split]\n",
    "sequences_all_val=sequences_all[split:]\n",
    "\n",
    "sequences_all=np.array(sequences_all)\n",
    "\n",
    "data = {'Pose': sequences_obs_pose,#[:split]\n",
    "        'Future_Pose': sequences_true_pose,#[:split]\n",
    "       }\n",
    "#         'Pose_image':sequences_true_imgs[:split],\n",
    "#         'Future_image':sequences_obs_imgs[:split]\n",
    "#         }\n",
    "data_val = {'Pose': sequences_obs_pose[split:],\n",
    "        'Future_Pose': sequences_true_pose[split:],\n",
    "           }\n",
    "#         'Pose_image':sequences_true_imgs[split:],\n",
    "#         'Future_image':sequences_obs_imgs[split:]\n",
    "#         }\n",
    "df_train = pd.DataFrame (data, columns = ['Pose','Future_Pose'])#,'Pose_image','Future_image'])\n",
    "# df_val = pd.DataFrame (data_val, columns = ['Pose','Future_Pose','Pose_image','Future_image'])\n",
    "\n",
    "df_train.to_csv(\"./sequences_openpifpaf_complete.csv\",index=False)\n",
    "# df_val.to_csv(\"./sequences_openpose_wconfsc_wimage_val.csv\", index=False)\n",
    "\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "sticky-monte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1731.22, 627.23, 0.65, 1735.22, 622.75, 0.63, 1726.39, 623.11, 0.55, 1741.33, 623.83, 0.5, 1719.77, 624.4, 0.52, 1748.3, 644.41, 0.72, 1709.8, 645.18, 0.55, 1760.71, 674.04, 0.92, 1700.98, 688.03, 0.51, 1749.28, 643.29, 0.85, 1694.57, 722.97, 0.57, 1748.33, 715.04, 0.71, 1720.37, 716.53, 0.68, 1755.05, 775.45, 0.74, 1718.74, 775.83, 0.72, 1758.17, 827.18, 0.77, 1701.24, 826.15, 0.77]\n"
     ]
    }
   ],
   "source": [
    "print(sequences_obs_pose[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "bronze-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(kp_train)\n",
    "train_dataset=PoseDataset(kp_train)\n",
    "val_dataset=PoseDataset(kp_train)\n",
    "trainloader=DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "valloader=DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "abstract-racing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 300, Train Loss: 77.724, Val Loss: 30.204, ADE_train : 125.811,  ADE_val: 47.361\n",
      "Epoch 2 of 300, Train Loss: 77.268, Val Loss: 31.273, ADE_train : 125.119,  ADE_val: 50.251\n",
      "Epoch 3 of 300, Train Loss: 76.848, Val Loss: 27.347, ADE_train : 124.749,  ADE_val: 41.752\n",
      "Epoch 4 of 300, Train Loss: 76.571, Val Loss: 29.945, ADE_train : 124.053,  ADE_val: 48.083\n",
      "Epoch 5 of 300, Train Loss: 74.731, Val Loss: 28.471, ADE_train : 120.241,  ADE_val: 43.242\n",
      "Epoch 6 of 300, Train Loss: 77.387, Val Loss: 30.293, ADE_train : 125.471,  ADE_val: 48.296\n",
      "Epoch 7 of 300, Train Loss: 76.645, Val Loss: 33.665, ADE_train : 124.245,  ADE_val: 54.728\n",
      "Epoch 8 of 300, Train Loss: 74.624, Val Loss: 29.073, ADE_train : 120.739,  ADE_val: 46.294\n",
      "Epoch     9: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 9 of 300, Train Loss: 78.249, Val Loss: 33.028, ADE_train : 126.774,  ADE_val: 52.757\n",
      "Epoch 10 of 300, Train Loss: 76.607, Val Loss: 30.868, ADE_train : 123.880,  ADE_val: 48.765\n",
      "Epoch 11 of 300, Train Loss: 75.164, Val Loss: 28.259, ADE_train : 121.918,  ADE_val: 43.760\n",
      "Epoch 12 of 300, Train Loss: 75.336, Val Loss: 30.117, ADE_train : 122.199,  ADE_val: 48.183\n",
      "Epoch 13 of 300, Train Loss: 76.015, Val Loss: 33.875, ADE_train : 123.558,  ADE_val: 54.800\n",
      "Epoch 14 of 300, Train Loss: 75.859, Val Loss: 28.200, ADE_train : 123.030,  ADE_val: 44.168\n",
      "Epoch    15: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 15 of 300, Train Loss: 76.842, Val Loss: 28.253, ADE_train : 124.186,  ADE_val: 43.703\n",
      "Epoch 16 of 300, Train Loss: 76.040, Val Loss: 30.584, ADE_train : 123.220,  ADE_val: 48.875\n",
      "Epoch 17 of 300, Train Loss: 76.816, Val Loss: 28.967, ADE_train : 124.455,  ADE_val: 44.888\n",
      "Epoch 18 of 300, Train Loss: 76.939, Val Loss: 28.689, ADE_train : 124.823,  ADE_val: 44.762\n",
      "Epoch 19 of 300, Train Loss: 75.691, Val Loss: 29.511, ADE_train : 122.614,  ADE_val: 46.105\n",
      "Epoch 20 of 300, Train Loss: 75.569, Val Loss: 28.156, ADE_train : 122.676,  ADE_val: 43.219\n",
      "Epoch    21: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 21 of 300, Train Loss: 76.119, Val Loss: 28.204, ADE_train : 123.428,  ADE_val: 43.443\n",
      "Epoch 22 of 300, Train Loss: 75.860, Val Loss: 29.590, ADE_train : 123.059,  ADE_val: 46.618\n",
      "Epoch 23 of 300, Train Loss: 76.030, Val Loss: 29.408, ADE_train : 123.416,  ADE_val: 46.398\n",
      "Epoch 24 of 300, Train Loss: 75.725, Val Loss: 28.723, ADE_train : 122.861,  ADE_val: 44.786\n",
      "Epoch 25 of 300, Train Loss: 76.347, Val Loss: 30.402, ADE_train : 123.805,  ADE_val: 48.272\n",
      "Epoch 26 of 300, Train Loss: 76.454, Val Loss: 27.992, ADE_train : 123.412,  ADE_val: 43.410\n",
      "Epoch    27: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch 27 of 300, Train Loss: 75.456, Val Loss: 29.035, ADE_train : 121.954,  ADE_val: 45.694\n",
      "Epoch 28 of 300, Train Loss: 76.101, Val Loss: 29.149, ADE_train : 123.961,  ADE_val: 45.956\n",
      "Epoch 29 of 300, Train Loss: 75.041, Val Loss: 29.010, ADE_train : 121.586,  ADE_val: 45.685\n",
      "Epoch 30 of 300, Train Loss: 75.774, Val Loss: 28.509, ADE_train : 123.221,  ADE_val: 44.665\n",
      "Epoch 31 of 300, Train Loss: 76.766, Val Loss: 29.205, ADE_train : 124.674,  ADE_val: 46.020\n",
      "Epoch 32 of 300, Train Loss: 75.613, Val Loss: 29.799, ADE_train : 122.786,  ADE_val: 47.192\n",
      "Epoch    33: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch 33 of 300, Train Loss: 76.005, Val Loss: 30.100, ADE_train : 122.990,  ADE_val: 47.878\n",
      "Epoch 34 of 300, Train Loss: 76.931, Val Loss: 29.261, ADE_train : 124.650,  ADE_val: 46.148\n",
      "Epoch 35 of 300, Train Loss: 76.460, Val Loss: 29.051, ADE_train : 124.165,  ADE_val: 45.634\n",
      "Epoch 36 of 300, Train Loss: 74.907, Val Loss: 29.025, ADE_train : 121.247,  ADE_val: 45.551\n",
      "Epoch 37 of 300, Train Loss: 75.185, Val Loss: 29.150, ADE_train : 121.942,  ADE_val: 45.762\n",
      "Epoch 38 of 300, Train Loss: 75.422, Val Loss: 28.828, ADE_train : 121.839,  ADE_val: 45.029\n",
      "Epoch    39: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch 39 of 300, Train Loss: 75.936, Val Loss: 29.079, ADE_train : 123.853,  ADE_val: 45.540\n",
      "Epoch 40 of 300, Train Loss: 75.814, Val Loss: 29.307, ADE_train : 122.648,  ADE_val: 46.184\n",
      "Epoch 41 of 300, Train Loss: 74.824, Val Loss: 29.064, ADE_train : 121.757,  ADE_val: 45.682\n",
      "Epoch 42 of 300, Train Loss: 74.873, Val Loss: 29.329, ADE_train : 121.627,  ADE_val: 46.187\n",
      "Epoch 43 of 300, Train Loss: 77.445, Val Loss: 29.277, ADE_train : 125.838,  ADE_val: 46.051\n",
      "Epoch 44 of 300, Train Loss: 76.403, Val Loss: 29.464, ADE_train : 123.856,  ADE_val: 46.393\n",
      "Epoch    45: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch 45 of 300, Train Loss: 75.073, Val Loss: 29.559, ADE_train : 121.388,  ADE_val: 46.674\n",
      "Epoch 46 of 300, Train Loss: 76.300, Val Loss: 29.097, ADE_train : 123.966,  ADE_val: 45.699\n",
      "Epoch 47 of 300, Train Loss: 74.294, Val Loss: 28.991, ADE_train : 120.567,  ADE_val: 45.457\n",
      "Epoch 48 of 300, Train Loss: 73.963, Val Loss: 28.998, ADE_train : 119.666,  ADE_val: 45.484\n",
      "Epoch 49 of 300, Train Loss: 76.329, Val Loss: 29.137, ADE_train : 123.580,  ADE_val: 45.779\n",
      "Epoch 50 of 300, Train Loss: 75.103, Val Loss: 28.853, ADE_train : 121.880,  ADE_val: 45.151\n",
      "Epoch    51: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch 51 of 300, Train Loss: 76.225, Val Loss: 28.927, ADE_train : 124.049,  ADE_val: 45.334\n",
      "Epoch 52 of 300, Train Loss: 75.895, Val Loss: 28.847, ADE_train : 123.667,  ADE_val: 45.183\n",
      "Epoch 53 of 300, Train Loss: 76.025, Val Loss: 28.844, ADE_train : 123.209,  ADE_val: 45.169\n",
      "Epoch 54 of 300, Train Loss: 77.780, Val Loss: 28.614, ADE_train : 126.381,  ADE_val: 44.669\n",
      "Epoch 55 of 300, Train Loss: 77.801, Val Loss: 28.594, ADE_train : 126.526,  ADE_val: 44.651\n",
      "Epoch 56 of 300, Train Loss: 74.432, Val Loss: 28.674, ADE_train : 120.504,  ADE_val: 44.823\n",
      "Epoch    57: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch 57 of 300, Train Loss: 75.693, Val Loss: 28.772, ADE_train : 122.638,  ADE_val: 45.027\n",
      "Epoch 58 of 300, Train Loss: 75.951, Val Loss: 28.713, ADE_train : 123.063,  ADE_val: 44.914\n",
      "Epoch 59 of 300, Train Loss: 74.937, Val Loss: 28.713, ADE_train : 121.577,  ADE_val: 44.916\n",
      "Epoch 60 of 300, Train Loss: 76.892, Val Loss: 28.778, ADE_train : 124.835,  ADE_val: 45.057\n",
      "Epoch 61 of 300, Train Loss: 75.759, Val Loss: 28.783, ADE_train : 122.856,  ADE_val: 45.066\n",
      "Epoch 62 of 300, Train Loss: 75.062, Val Loss: 28.778, ADE_train : 121.691,  ADE_val: 45.053\n",
      "Epoch    63: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch 63 of 300, Train Loss: 74.873, Val Loss: 28.849, ADE_train : 121.204,  ADE_val: 45.207\n",
      "Epoch 64 of 300, Train Loss: 76.753, Val Loss: 28.876, ADE_train : 124.868,  ADE_val: 45.263\n",
      "Epoch 65 of 300, Train Loss: 74.959, Val Loss: 28.952, ADE_train : 120.884,  ADE_val: 45.429\n",
      "Epoch 66 of 300, Train Loss: 75.400, Val Loss: 28.981, ADE_train : 122.154,  ADE_val: 45.494\n",
      "Epoch 67 of 300, Train Loss: 76.224, Val Loss: 29.032, ADE_train : 123.250,  ADE_val: 45.603\n",
      "Epoch 68 of 300, Train Loss: 76.229, Val Loss: 28.962, ADE_train : 123.846,  ADE_val: 45.453\n",
      "Epoch    69: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch 69 of 300, Train Loss: 76.340, Val Loss: 28.921, ADE_train : 124.053,  ADE_val: 45.360\n",
      "Epoch 70 of 300, Train Loss: 76.772, Val Loss: 28.907, ADE_train : 125.070,  ADE_val: 45.329\n",
      "Epoch 71 of 300, Train Loss: 76.723, Val Loss: 28.915, ADE_train : 123.922,  ADE_val: 45.345\n",
      "Epoch 72 of 300, Train Loss: 75.847, Val Loss: 28.919, ADE_train : 122.933,  ADE_val: 45.352\n",
      "Epoch 73 of 300, Train Loss: 76.182, Val Loss: 28.929, ADE_train : 123.780,  ADE_val: 45.375\n",
      "Epoch 74 of 300, Train Loss: 74.985, Val Loss: 28.942, ADE_train : 121.389,  ADE_val: 45.404\n",
      "Epoch    75: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Epoch 75 of 300, Train Loss: 75.993, Val Loss: 28.966, ADE_train : 122.996,  ADE_val: 45.454\n",
      "Epoch 76 of 300, Train Loss: 75.811, Val Loss: 28.978, ADE_train : 122.564,  ADE_val: 45.482\n",
      "Epoch 77 of 300, Train Loss: 76.024, Val Loss: 28.985, ADE_train : 123.260,  ADE_val: 45.496\n",
      "Epoch 78 of 300, Train Loss: 75.476, Val Loss: 28.990, ADE_train : 122.354,  ADE_val: 45.507\n",
      "Epoch 79 of 300, Train Loss: 77.369, Val Loss: 28.996, ADE_train : 125.336,  ADE_val: 45.518\n",
      "Epoch 80 of 300, Train Loss: 74.661, Val Loss: 28.980, ADE_train : 121.142,  ADE_val: 45.484\n",
      "Epoch    81: reducing learning rate of group 0 to 1.2207e-07.\n",
      "Epoch 81 of 300, Train Loss: 76.683, Val Loss: 28.972, ADE_train : 124.348,  ADE_val: 45.465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 of 300, Train Loss: 76.134, Val Loss: 28.971, ADE_train : 123.310,  ADE_val: 45.463\n",
      "Epoch 83 of 300, Train Loss: 77.065, Val Loss: 28.970, ADE_train : 125.071,  ADE_val: 45.459\n",
      "Epoch 84 of 300, Train Loss: 75.211, Val Loss: 28.979, ADE_train : 122.040,  ADE_val: 45.480\n",
      "Epoch 85 of 300, Train Loss: 75.739, Val Loss: 28.976, ADE_train : 122.868,  ADE_val: 45.473\n",
      "Epoch 86 of 300, Train Loss: 74.369, Val Loss: 28.978, ADE_train : 120.237,  ADE_val: 45.478\n",
      "Epoch    87: reducing learning rate of group 0 to 6.1035e-08.\n",
      "Epoch 87 of 300, Train Loss: 74.728, Val Loss: 28.979, ADE_train : 121.024,  ADE_val: 45.479\n",
      "Epoch 88 of 300, Train Loss: 75.980, Val Loss: 28.979, ADE_train : 122.758,  ADE_val: 45.479\n",
      "Epoch 89 of 300, Train Loss: 74.376, Val Loss: 28.980, ADE_train : 120.546,  ADE_val: 45.481\n",
      "Epoch 90 of 300, Train Loss: 75.695, Val Loss: 28.977, ADE_train : 122.963,  ADE_val: 45.476\n",
      "Epoch 91 of 300, Train Loss: 74.695, Val Loss: 28.978, ADE_train : 120.798,  ADE_val: 45.477\n",
      "Epoch 92 of 300, Train Loss: 75.692, Val Loss: 28.974, ADE_train : 122.565,  ADE_val: 45.470\n",
      "Epoch    93: reducing learning rate of group 0 to 3.0518e-08.\n",
      "Epoch 93 of 300, Train Loss: 76.228, Val Loss: 28.971, ADE_train : 123.666,  ADE_val: 45.464\n",
      "Epoch 94 of 300, Train Loss: 75.142, Val Loss: 28.970, ADE_train : 122.040,  ADE_val: 45.461\n",
      "Epoch 95 of 300, Train Loss: 77.428, Val Loss: 28.970, ADE_train : 125.554,  ADE_val: 45.461\n",
      "Epoch 96 of 300, Train Loss: 76.336, Val Loss: 28.971, ADE_train : 123.448,  ADE_val: 45.462\n",
      "Epoch 97 of 300, Train Loss: 74.883, Val Loss: 28.971, ADE_train : 121.222,  ADE_val: 45.462\n",
      "Epoch 98 of 300, Train Loss: 75.590, Val Loss: 28.970, ADE_train : 122.826,  ADE_val: 45.460\n",
      "Epoch    99: reducing learning rate of group 0 to 1.5259e-08.\n",
      "Epoch 99 of 300, Train Loss: 75.018, Val Loss: 28.970, ADE_train : 121.866,  ADE_val: 45.461\n",
      "Epoch 100 of 300, Train Loss: 75.842, Val Loss: 28.970, ADE_train : 123.123,  ADE_val: 45.461\n",
      "Epoch 101 of 300, Train Loss: 74.919, Val Loss: 28.970, ADE_train : 121.809,  ADE_val: 45.461\n",
      "Epoch 102 of 300, Train Loss: 75.977, Val Loss: 28.970, ADE_train : 123.319,  ADE_val: 45.461\n",
      "Epoch 103 of 300, Train Loss: 75.049, Val Loss: 28.970, ADE_train : 121.588,  ADE_val: 45.460\n",
      "Epoch 104 of 300, Train Loss: 76.202, Val Loss: 28.970, ADE_train : 123.234,  ADE_val: 45.461\n",
      "Epoch 105 of 300, Train Loss: 76.501, Val Loss: 28.970, ADE_train : 123.859,  ADE_val: 45.461\n",
      "Epoch 106 of 300, Train Loss: 75.174, Val Loss: 28.970, ADE_train : 121.825,  ADE_val: 45.461\n",
      "Epoch 107 of 300, Train Loss: 76.034, Val Loss: 28.970, ADE_train : 123.978,  ADE_val: 45.461\n",
      "Epoch 108 of 300, Train Loss: 75.108, Val Loss: 28.970, ADE_train : 121.864,  ADE_val: 45.461\n",
      "Epoch 109 of 300, Train Loss: 74.443, Val Loss: 28.970, ADE_train : 120.465,  ADE_val: 45.461\n",
      "Epoch 110 of 300, Train Loss: 76.408, Val Loss: 28.970, ADE_train : 123.533,  ADE_val: 45.461\n",
      "Epoch 111 of 300, Train Loss: 74.843, Val Loss: 28.970, ADE_train : 121.251,  ADE_val: 45.461\n",
      "Epoch 112 of 300, Train Loss: 75.663, Val Loss: 28.970, ADE_train : 122.988,  ADE_val: 45.461\n",
      "Epoch 113 of 300, Train Loss: 76.878, Val Loss: 28.970, ADE_train : 124.514,  ADE_val: 45.461\n",
      "Epoch 114 of 300, Train Loss: 75.801, Val Loss: 28.970, ADE_train : 122.891,  ADE_val: 45.461\n",
      "Epoch 115 of 300, Train Loss: 76.317, Val Loss: 28.970, ADE_train : 123.882,  ADE_val: 45.461\n",
      "Epoch 116 of 300, Train Loss: 74.870, Val Loss: 28.970, ADE_train : 121.137,  ADE_val: 45.461\n",
      "Epoch 117 of 300, Train Loss: 75.316, Val Loss: 28.970, ADE_train : 121.946,  ADE_val: 45.461\n",
      "Epoch 118 of 300, Train Loss: 76.772, Val Loss: 28.970, ADE_train : 124.382,  ADE_val: 45.462\n",
      "Epoch 119 of 300, Train Loss: 75.901, Val Loss: 28.970, ADE_train : 123.452,  ADE_val: 45.461\n",
      "Epoch 120 of 300, Train Loss: 75.488, Val Loss: 28.970, ADE_train : 122.723,  ADE_val: 45.461\n",
      "Epoch 121 of 300, Train Loss: 75.709, Val Loss: 28.970, ADE_train : 122.988,  ADE_val: 45.461\n",
      "Epoch 122 of 300, Train Loss: 75.279, Val Loss: 28.970, ADE_train : 121.990,  ADE_val: 45.461\n",
      "Epoch 123 of 300, Train Loss: 76.221, Val Loss: 28.970, ADE_train : 123.145,  ADE_val: 45.461\n",
      "Epoch 124 of 300, Train Loss: 74.673, Val Loss: 28.970, ADE_train : 120.991,  ADE_val: 45.461\n",
      "Epoch 125 of 300, Train Loss: 75.889, Val Loss: 28.970, ADE_train : 122.927,  ADE_val: 45.461\n",
      "Epoch 126 of 300, Train Loss: 75.180, Val Loss: 28.970, ADE_train : 121.757,  ADE_val: 45.461\n",
      "Epoch 127 of 300, Train Loss: 75.497, Val Loss: 28.970, ADE_train : 122.390,  ADE_val: 45.460\n",
      "Epoch 128 of 300, Train Loss: 75.205, Val Loss: 28.970, ADE_train : 121.557,  ADE_val: 45.460\n",
      "Epoch 129 of 300, Train Loss: 77.412, Val Loss: 28.970, ADE_train : 125.255,  ADE_val: 45.460\n",
      "Epoch 130 of 300, Train Loss: 76.529, Val Loss: 28.970, ADE_train : 124.285,  ADE_val: 45.461\n",
      "Epoch 131 of 300, Train Loss: 74.774, Val Loss: 28.970, ADE_train : 120.948,  ADE_val: 45.460\n",
      "Epoch 132 of 300, Train Loss: 75.659, Val Loss: 28.970, ADE_train : 122.498,  ADE_val: 45.461\n",
      "Epoch 133 of 300, Train Loss: 75.220, Val Loss: 28.970, ADE_train : 121.749,  ADE_val: 45.460\n",
      "Epoch 134 of 300, Train Loss: 76.075, Val Loss: 28.970, ADE_train : 123.504,  ADE_val: 45.460\n",
      "Epoch 135 of 300, Train Loss: 75.140, Val Loss: 28.969, ADE_train : 121.864,  ADE_val: 45.460\n",
      "Epoch 136 of 300, Train Loss: 76.260, Val Loss: 28.970, ADE_train : 124.292,  ADE_val: 45.460\n",
      "Epoch 137 of 300, Train Loss: 75.566, Val Loss: 28.970, ADE_train : 122.652,  ADE_val: 45.460\n",
      "Epoch 138 of 300, Train Loss: 76.824, Val Loss: 28.970, ADE_train : 124.538,  ADE_val: 45.460\n",
      "Epoch 139 of 300, Train Loss: 75.400, Val Loss: 28.969, ADE_train : 122.022,  ADE_val: 45.459\n",
      "Epoch 140 of 300, Train Loss: 77.134, Val Loss: 28.969, ADE_train : 124.651,  ADE_val: 45.459\n",
      "Epoch 141 of 300, Train Loss: 76.296, Val Loss: 28.969, ADE_train : 123.323,  ADE_val: 45.459\n",
      "Epoch 142 of 300, Train Loss: 74.522, Val Loss: 28.969, ADE_train : 120.737,  ADE_val: 45.459\n",
      "Epoch 143 of 300, Train Loss: 76.410, Val Loss: 28.969, ADE_train : 123.824,  ADE_val: 45.458\n",
      "Epoch 144 of 300, Train Loss: 77.115, Val Loss: 28.969, ADE_train : 124.575,  ADE_val: 45.458\n",
      "Epoch 145 of 300, Train Loss: 76.215, Val Loss: 28.969, ADE_train : 123.342,  ADE_val: 45.458\n",
      "Epoch 146 of 300, Train Loss: 75.743, Val Loss: 28.969, ADE_train : 122.577,  ADE_val: 45.459\n",
      "Epoch 147 of 300, Train Loss: 75.888, Val Loss: 28.969, ADE_train : 123.023,  ADE_val: 45.459\n",
      "Epoch 148 of 300, Train Loss: 76.370, Val Loss: 28.969, ADE_train : 123.850,  ADE_val: 45.458\n",
      "Epoch 149 of 300, Train Loss: 75.701, Val Loss: 28.969, ADE_train : 122.673,  ADE_val: 45.458\n",
      "Epoch 150 of 300, Train Loss: 76.697, Val Loss: 28.969, ADE_train : 124.883,  ADE_val: 45.458\n",
      "Epoch 151 of 300, Train Loss: 76.549, Val Loss: 28.969, ADE_train : 124.578,  ADE_val: 45.458\n",
      "Epoch 152 of 300, Train Loss: 76.001, Val Loss: 28.968, ADE_train : 123.841,  ADE_val: 45.458\n",
      "Epoch 153 of 300, Train Loss: 75.847, Val Loss: 28.968, ADE_train : 123.216,  ADE_val: 45.457\n",
      "Epoch 154 of 300, Train Loss: 75.283, Val Loss: 28.968, ADE_train : 121.927,  ADE_val: 45.457\n",
      "Epoch 155 of 300, Train Loss: 76.970, Val Loss: 28.968, ADE_train : 124.887,  ADE_val: 45.457\n",
      "Epoch 156 of 300, Train Loss: 74.699, Val Loss: 28.968, ADE_train : 121.110,  ADE_val: 45.457\n",
      "Epoch 157 of 300, Train Loss: 75.702, Val Loss: 28.968, ADE_train : 122.354,  ADE_val: 45.457\n",
      "Epoch 158 of 300, Train Loss: 76.791, Val Loss: 28.968, ADE_train : 124.629,  ADE_val: 45.457\n",
      "Epoch 159 of 300, Train Loss: 74.187, Val Loss: 28.968, ADE_train : 120.227,  ADE_val: 45.457\n",
      "Epoch 160 of 300, Train Loss: 76.274, Val Loss: 28.968, ADE_train : 123.650,  ADE_val: 45.456\n",
      "Epoch 161 of 300, Train Loss: 76.660, Val Loss: 28.968, ADE_train : 124.140,  ADE_val: 45.456\n",
      "Epoch 162 of 300, Train Loss: 76.311, Val Loss: 28.968, ADE_train : 123.810,  ADE_val: 45.456\n",
      "Epoch 163 of 300, Train Loss: 76.162, Val Loss: 28.968, ADE_train : 123.667,  ADE_val: 45.456\n",
      "Epoch 164 of 300, Train Loss: 77.895, Val Loss: 28.968, ADE_train : 125.917,  ADE_val: 45.456\n",
      "Epoch 165 of 300, Train Loss: 75.796, Val Loss: 28.967, ADE_train : 122.962,  ADE_val: 45.455\n",
      "Epoch 166 of 300, Train Loss: 76.107, Val Loss: 28.967, ADE_train : 123.441,  ADE_val: 45.455\n",
      "Epoch 167 of 300, Train Loss: 75.026, Val Loss: 28.967, ADE_train : 121.992,  ADE_val: 45.455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168 of 300, Train Loss: 75.869, Val Loss: 28.967, ADE_train : 123.158,  ADE_val: 45.455\n",
      "Epoch 169 of 300, Train Loss: 77.444, Val Loss: 28.967, ADE_train : 126.182,  ADE_val: 45.455\n",
      "Epoch 170 of 300, Train Loss: 74.900, Val Loss: 28.967, ADE_train : 121.454,  ADE_val: 45.455\n",
      "Epoch 171 of 300, Train Loss: 74.567, Val Loss: 28.967, ADE_train : 120.421,  ADE_val: 45.455\n",
      "Epoch 172 of 300, Train Loss: 76.138, Val Loss: 28.967, ADE_train : 122.968,  ADE_val: 45.455\n",
      "Epoch 173 of 300, Train Loss: 74.601, Val Loss: 28.967, ADE_train : 120.643,  ADE_val: 45.455\n",
      "Epoch 174 of 300, Train Loss: 76.192, Val Loss: 28.967, ADE_train : 123.890,  ADE_val: 45.455\n",
      "Epoch 175 of 300, Train Loss: 76.781, Val Loss: 28.967, ADE_train : 124.346,  ADE_val: 45.455\n",
      "Epoch 176 of 300, Train Loss: 75.298, Val Loss: 28.967, ADE_train : 121.358,  ADE_val: 45.454\n",
      "Epoch 177 of 300, Train Loss: 76.470, Val Loss: 28.967, ADE_train : 124.336,  ADE_val: 45.455\n",
      "Epoch 178 of 300, Train Loss: 75.527, Val Loss: 28.967, ADE_train : 122.205,  ADE_val: 45.455\n",
      "Epoch 179 of 300, Train Loss: 76.005, Val Loss: 28.967, ADE_train : 123.209,  ADE_val: 45.455\n",
      "Epoch 180 of 300, Train Loss: 75.711, Val Loss: 28.967, ADE_train : 122.677,  ADE_val: 45.455\n",
      "Epoch 181 of 300, Train Loss: 75.296, Val Loss: 28.967, ADE_train : 122.051,  ADE_val: 45.455\n",
      "Epoch 182 of 300, Train Loss: 74.562, Val Loss: 28.967, ADE_train : 120.760,  ADE_val: 45.455\n",
      "Epoch 183 of 300, Train Loss: 74.826, Val Loss: 28.967, ADE_train : 121.421,  ADE_val: 45.455\n",
      "Epoch 184 of 300, Train Loss: 74.492, Val Loss: 28.967, ADE_train : 120.861,  ADE_val: 45.455\n",
      "Epoch 185 of 300, Train Loss: 76.915, Val Loss: 28.967, ADE_train : 124.900,  ADE_val: 45.455\n",
      "Epoch 186 of 300, Train Loss: 77.208, Val Loss: 28.967, ADE_train : 125.400,  ADE_val: 45.455\n",
      "Epoch 187 of 300, Train Loss: 76.228, Val Loss: 28.967, ADE_train : 123.709,  ADE_val: 45.455\n",
      "Epoch 188 of 300, Train Loss: 75.318, Val Loss: 28.967, ADE_train : 122.298,  ADE_val: 45.456\n",
      "Epoch 189 of 300, Train Loss: 77.508, Val Loss: 28.967, ADE_train : 125.746,  ADE_val: 45.455\n",
      "Epoch 190 of 300, Train Loss: 74.891, Val Loss: 28.967, ADE_train : 121.705,  ADE_val: 45.455\n",
      "Epoch 191 of 300, Train Loss: 75.684, Val Loss: 28.967, ADE_train : 122.799,  ADE_val: 45.455\n",
      "Epoch 192 of 300, Train Loss: 76.308, Val Loss: 28.967, ADE_train : 123.386,  ADE_val: 45.455\n",
      "Epoch 193 of 300, Train Loss: 77.272, Val Loss: 28.967, ADE_train : 125.425,  ADE_val: 45.455\n",
      "Epoch 194 of 300, Train Loss: 76.854, Val Loss: 28.967, ADE_train : 124.557,  ADE_val: 45.455\n",
      "Epoch 195 of 300, Train Loss: 77.441, Val Loss: 28.967, ADE_train : 125.559,  ADE_val: 45.455\n",
      "Epoch 196 of 300, Train Loss: 75.628, Val Loss: 28.967, ADE_train : 123.026,  ADE_val: 45.455\n",
      "Epoch 197 of 300, Train Loss: 74.287, Val Loss: 28.967, ADE_train : 119.928,  ADE_val: 45.455\n",
      "Epoch 198 of 300, Train Loss: 76.425, Val Loss: 28.967, ADE_train : 124.015,  ADE_val: 45.455\n",
      "Epoch 199 of 300, Train Loss: 75.267, Val Loss: 28.967, ADE_train : 121.928,  ADE_val: 45.454\n",
      "Epoch 200 of 300, Train Loss: 76.058, Val Loss: 28.967, ADE_train : 123.743,  ADE_val: 45.454\n",
      "Epoch 201 of 300, Train Loss: 74.741, Val Loss: 28.966, ADE_train : 120.902,  ADE_val: 45.453\n",
      "Epoch 202 of 300, Train Loss: 75.174, Val Loss: 28.966, ADE_train : 121.664,  ADE_val: 45.453\n",
      "Epoch 203 of 300, Train Loss: 76.164, Val Loss: 28.966, ADE_train : 123.845,  ADE_val: 45.453\n",
      "Epoch 204 of 300, Train Loss: 75.953, Val Loss: 28.966, ADE_train : 123.320,  ADE_val: 45.453\n",
      "Epoch 205 of 300, Train Loss: 76.092, Val Loss: 28.966, ADE_train : 123.434,  ADE_val: 45.453\n",
      "Epoch 206 of 300, Train Loss: 75.663, Val Loss: 28.966, ADE_train : 122.889,  ADE_val: 45.453\n",
      "Epoch 207 of 300, Train Loss: 76.426, Val Loss: 28.966, ADE_train : 124.131,  ADE_val: 45.453\n",
      "Epoch 208 of 300, Train Loss: 75.075, Val Loss: 28.966, ADE_train : 122.020,  ADE_val: 45.453\n",
      "Epoch 209 of 300, Train Loss: 75.908, Val Loss: 28.966, ADE_train : 123.206,  ADE_val: 45.453\n",
      "Epoch 210 of 300, Train Loss: 74.398, Val Loss: 28.967, ADE_train : 120.041,  ADE_val: 45.454\n",
      "Epoch 211 of 300, Train Loss: 75.887, Val Loss: 28.966, ADE_train : 122.938,  ADE_val: 45.453\n",
      "Epoch 212 of 300, Train Loss: 73.754, Val Loss: 28.966, ADE_train : 119.536,  ADE_val: 45.453\n",
      "Epoch 213 of 300, Train Loss: 76.769, Val Loss: 28.966, ADE_train : 124.168,  ADE_val: 45.453\n",
      "Epoch 214 of 300, Train Loss: 75.657, Val Loss: 28.966, ADE_train : 122.519,  ADE_val: 45.453\n",
      "Epoch 215 of 300, Train Loss: 75.234, Val Loss: 28.966, ADE_train : 122.015,  ADE_val: 45.453\n",
      "Epoch 216 of 300, Train Loss: 75.355, Val Loss: 28.966, ADE_train : 121.628,  ADE_val: 45.453\n",
      "Epoch 217 of 300, Train Loss: 75.657, Val Loss: 28.966, ADE_train : 122.623,  ADE_val: 45.454\n",
      "Epoch 218 of 300, Train Loss: 76.390, Val Loss: 28.967, ADE_train : 124.020,  ADE_val: 45.454\n",
      "Epoch 219 of 300, Train Loss: 74.524, Val Loss: 28.966, ADE_train : 120.604,  ADE_val: 45.452\n",
      "Epoch 220 of 300, Train Loss: 75.875, Val Loss: 28.966, ADE_train : 122.800,  ADE_val: 45.453\n",
      "Epoch 221 of 300, Train Loss: 75.110, Val Loss: 28.966, ADE_train : 121.832,  ADE_val: 45.453\n",
      "Epoch 222 of 300, Train Loss: 74.408, Val Loss: 28.966, ADE_train : 120.167,  ADE_val: 45.453\n",
      "Epoch 223 of 300, Train Loss: 76.049, Val Loss: 28.965, ADE_train : 122.842,  ADE_val: 45.452\n",
      "Epoch 224 of 300, Train Loss: 76.519, Val Loss: 28.966, ADE_train : 124.317,  ADE_val: 45.452\n",
      "Epoch 225 of 300, Train Loss: 76.161, Val Loss: 28.966, ADE_train : 123.892,  ADE_val: 45.452\n",
      "Epoch 226 of 300, Train Loss: 75.388, Val Loss: 28.966, ADE_train : 122.261,  ADE_val: 45.452\n",
      "Epoch 227 of 300, Train Loss: 75.247, Val Loss: 28.966, ADE_train : 121.868,  ADE_val: 45.452\n",
      "Epoch 228 of 300, Train Loss: 76.221, Val Loss: 28.965, ADE_train : 123.335,  ADE_val: 45.451\n",
      "Epoch 229 of 300, Train Loss: 75.357, Val Loss: 28.965, ADE_train : 121.750,  ADE_val: 45.451\n",
      "Epoch 230 of 300, Train Loss: 76.279, Val Loss: 28.965, ADE_train : 123.355,  ADE_val: 45.451\n",
      "Epoch 231 of 300, Train Loss: 75.466, Val Loss: 28.965, ADE_train : 122.022,  ADE_val: 45.451\n",
      "Epoch 232 of 300, Train Loss: 76.124, Val Loss: 28.965, ADE_train : 123.004,  ADE_val: 45.451\n",
      "Epoch 233 of 300, Train Loss: 77.393, Val Loss: 28.965, ADE_train : 125.278,  ADE_val: 45.450\n",
      "Epoch 234 of 300, Train Loss: 75.495, Val Loss: 28.965, ADE_train : 122.029,  ADE_val: 45.450\n",
      "Epoch 235 of 300, Train Loss: 75.502, Val Loss: 28.965, ADE_train : 122.425,  ADE_val: 45.450\n",
      "Epoch 236 of 300, Train Loss: 73.980, Val Loss: 28.964, ADE_train : 120.115,  ADE_val: 45.449\n",
      "Epoch 237 of 300, Train Loss: 77.067, Val Loss: 28.964, ADE_train : 125.025,  ADE_val: 45.449\n",
      "Epoch 238 of 300, Train Loss: 76.336, Val Loss: 28.964, ADE_train : 123.089,  ADE_val: 45.449\n",
      "Epoch 239 of 300, Train Loss: 76.519, Val Loss: 28.964, ADE_train : 124.363,  ADE_val: 45.448\n",
      "Epoch 240 of 300, Train Loss: 76.330, Val Loss: 28.964, ADE_train : 123.364,  ADE_val: 45.449\n",
      "Epoch 241 of 300, Train Loss: 76.136, Val Loss: 28.964, ADE_train : 123.346,  ADE_val: 45.448\n",
      "Epoch 242 of 300, Train Loss: 75.555, Val Loss: 28.964, ADE_train : 122.316,  ADE_val: 45.448\n",
      "Epoch 243 of 300, Train Loss: 75.713, Val Loss: 28.964, ADE_train : 123.159,  ADE_val: 45.448\n",
      "Epoch 244 of 300, Train Loss: 76.607, Val Loss: 28.964, ADE_train : 124.021,  ADE_val: 45.448\n",
      "Epoch 245 of 300, Train Loss: 77.617, Val Loss: 28.964, ADE_train : 126.320,  ADE_val: 45.448\n",
      "Epoch 246 of 300, Train Loss: 76.530, Val Loss: 28.964, ADE_train : 123.892,  ADE_val: 45.448\n",
      "Epoch 247 of 300, Train Loss: 75.483, Val Loss: 28.964, ADE_train : 121.594,  ADE_val: 45.448\n",
      "Epoch 248 of 300, Train Loss: 75.262, Val Loss: 28.964, ADE_train : 121.712,  ADE_val: 45.448\n",
      "Epoch 249 of 300, Train Loss: 75.432, Val Loss: 28.964, ADE_train : 122.278,  ADE_val: 45.448\n",
      "Epoch 250 of 300, Train Loss: 76.422, Val Loss: 28.963, ADE_train : 123.379,  ADE_val: 45.447\n",
      "Epoch 251 of 300, Train Loss: 76.870, Val Loss: 28.963, ADE_train : 124.677,  ADE_val: 45.447\n",
      "Epoch 252 of 300, Train Loss: 75.593, Val Loss: 28.963, ADE_train : 122.334,  ADE_val: 45.447\n",
      "Epoch 253 of 300, Train Loss: 75.982, Val Loss: 28.963, ADE_train : 123.268,  ADE_val: 45.446\n",
      "Epoch 254 of 300, Train Loss: 75.829, Val Loss: 28.963, ADE_train : 123.246,  ADE_val: 45.447\n",
      "Epoch 255 of 300, Train Loss: 75.563, Val Loss: 28.963, ADE_train : 122.267,  ADE_val: 45.447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256 of 300, Train Loss: 77.222, Val Loss: 28.963, ADE_train : 124.950,  ADE_val: 45.446\n",
      "Epoch 257 of 300, Train Loss: 76.243, Val Loss: 28.963, ADE_train : 123.292,  ADE_val: 45.446\n",
      "Epoch 258 of 300, Train Loss: 75.234, Val Loss: 28.963, ADE_train : 122.065,  ADE_val: 45.447\n",
      "Epoch 259 of 300, Train Loss: 76.066, Val Loss: 28.963, ADE_train : 123.703,  ADE_val: 45.447\n",
      "Epoch 260 of 300, Train Loss: 76.494, Val Loss: 28.963, ADE_train : 124.107,  ADE_val: 45.447\n",
      "Epoch 261 of 300, Train Loss: 74.617, Val Loss: 28.963, ADE_train : 121.212,  ADE_val: 45.447\n",
      "Epoch 262 of 300, Train Loss: 76.540, Val Loss: 28.963, ADE_train : 123.837,  ADE_val: 45.447\n",
      "Epoch 263 of 300, Train Loss: 75.332, Val Loss: 28.963, ADE_train : 121.939,  ADE_val: 45.447\n",
      "Epoch 264 of 300, Train Loss: 76.796, Val Loss: 28.963, ADE_train : 124.477,  ADE_val: 45.447\n",
      "Epoch 265 of 300, Train Loss: 76.295, Val Loss: 28.963, ADE_train : 123.590,  ADE_val: 45.448\n",
      "Epoch 266 of 300, Train Loss: 75.465, Val Loss: 28.964, ADE_train : 121.955,  ADE_val: 45.448\n",
      "Epoch 267 of 300, Train Loss: 75.167, Val Loss: 28.964, ADE_train : 122.038,  ADE_val: 45.448\n",
      "Epoch 268 of 300, Train Loss: 76.456, Val Loss: 28.964, ADE_train : 124.749,  ADE_val: 45.448\n",
      "Epoch 269 of 300, Train Loss: 76.183, Val Loss: 28.964, ADE_train : 123.795,  ADE_val: 45.448\n",
      "Epoch 270 of 300, Train Loss: 75.985, Val Loss: 28.964, ADE_train : 124.001,  ADE_val: 45.448\n",
      "Epoch 271 of 300, Train Loss: 75.234, Val Loss: 28.964, ADE_train : 121.708,  ADE_val: 45.448\n",
      "Epoch 272 of 300, Train Loss: 77.290, Val Loss: 28.964, ADE_train : 125.622,  ADE_val: 45.448\n",
      "Epoch 273 of 300, Train Loss: 75.618, Val Loss: 28.963, ADE_train : 122.864,  ADE_val: 45.448\n",
      "Epoch 274 of 300, Train Loss: 74.965, Val Loss: 28.964, ADE_train : 121.248,  ADE_val: 45.448\n",
      "Epoch 275 of 300, Train Loss: 75.518, Val Loss: 28.963, ADE_train : 122.379,  ADE_val: 45.447\n",
      "Epoch 276 of 300, Train Loss: 75.352, Val Loss: 28.963, ADE_train : 122.067,  ADE_val: 45.447\n",
      "Epoch 277 of 300, Train Loss: 75.470, Val Loss: 28.963, ADE_train : 122.065,  ADE_val: 45.446\n",
      "Epoch 278 of 300, Train Loss: 75.225, Val Loss: 28.963, ADE_train : 121.836,  ADE_val: 45.446\n",
      "Epoch 279 of 300, Train Loss: 75.265, Val Loss: 28.963, ADE_train : 121.973,  ADE_val: 45.446\n",
      "Epoch 280 of 300, Train Loss: 74.106, Val Loss: 28.963, ADE_train : 120.290,  ADE_val: 45.446\n",
      "Epoch 281 of 300, Train Loss: 77.275, Val Loss: 28.963, ADE_train : 125.636,  ADE_val: 45.447\n",
      "Epoch 282 of 300, Train Loss: 76.151, Val Loss: 28.963, ADE_train : 123.314,  ADE_val: 45.447\n",
      "Epoch 283 of 300, Train Loss: 76.676, Val Loss: 28.963, ADE_train : 124.342,  ADE_val: 45.447\n",
      "Epoch 284 of 300, Train Loss: 74.299, Val Loss: 28.963, ADE_train : 120.116,  ADE_val: 45.446\n",
      "Epoch 285 of 300, Train Loss: 76.781, Val Loss: 28.963, ADE_train : 124.374,  ADE_val: 45.446\n",
      "Epoch 286 of 300, Train Loss: 76.492, Val Loss: 28.963, ADE_train : 124.178,  ADE_val: 45.446\n",
      "Epoch 287 of 300, Train Loss: 75.261, Val Loss: 28.963, ADE_train : 121.772,  ADE_val: 45.446\n",
      "Epoch 288 of 300, Train Loss: 76.543, Val Loss: 28.963, ADE_train : 124.177,  ADE_val: 45.446\n",
      "Epoch 289 of 300, Train Loss: 76.083, Val Loss: 28.963, ADE_train : 123.446,  ADE_val: 45.447\n",
      "Epoch 290 of 300, Train Loss: 75.699, Val Loss: 28.963, ADE_train : 122.625,  ADE_val: 45.447\n",
      "Epoch 291 of 300, Train Loss: 75.859, Val Loss: 28.963, ADE_train : 122.794,  ADE_val: 45.447\n",
      "Epoch 292 of 300, Train Loss: 76.646, Val Loss: 28.963, ADE_train : 124.874,  ADE_val: 45.447\n",
      "Epoch 293 of 300, Train Loss: 75.111, Val Loss: 28.963, ADE_train : 121.814,  ADE_val: 45.446\n",
      "Epoch 294 of 300, Train Loss: 76.157, Val Loss: 28.963, ADE_train : 123.608,  ADE_val: 45.447\n",
      "Epoch 295 of 300, Train Loss: 76.539, Val Loss: 28.963, ADE_train : 124.296,  ADE_val: 45.447\n",
      "Epoch 296 of 300, Train Loss: 74.720, Val Loss: 28.963, ADE_train : 121.076,  ADE_val: 45.446\n",
      "Epoch 297 of 300, Train Loss: 75.236, Val Loss: 28.963, ADE_train : 122.022,  ADE_val: 45.447\n",
      "Epoch 298 of 300, Train Loss: 74.539, Val Loss: 28.963, ADE_train : 121.061,  ADE_val: 45.447\n",
      "Epoch 299 of 300, Train Loss: 75.390, Val Loss: 28.963, ADE_train : 122.087,  ADE_val: 45.447\n",
      "Epoch 300 of 300, Train Loss: 74.890, Val Loss: 28.963, ADE_train : 121.359,  ADE_val: 45.447\n"
     ]
    }
   ],
   "source": [
    "# net = Autoencoder()\n",
    "# device = torch.device('cuda:0')\n",
    "# net.to(device)\n",
    "# net = net.float()\n",
    "NUM_EPOCHS = 300\n",
    "BATCH_SIZE = 30\n",
    "# net.load_state_dict(torch.load('model_best.weight'))\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5,verbose=True, factor=0.5)\n",
    "train_loss, val_loss = train(net, trainloader, valloader, NUM_EPOCHS, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "resident-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"model_temp.weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "square-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_spat(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        '''\n",
    "           input: observed body poses and velocites \n",
    "           output: predicted poses(not trained), velocities of poses, and intentions\n",
    "        '''\n",
    "        super(LSTM_spat, self).__init__()\n",
    "        self.encoded_size=300\n",
    "         \n",
    "        self.pose_encoder = nn.LSTM(input_size=self.encoded_size, hidden_size=args.hidden_size, num_layers = 3)        \n",
    "        self.pose_embedding = nn.Sequential(nn.Linear(in_features=args.hidden_size, out_features=30),\n",
    "                                           nn.ReLU())\n",
    "        \n",
    "        self.pose_decoder = nn.LSTM(input_size=self.encoded_size, hidden_size=args.hidden_size, num_layers = 3)        \n",
    "        self.fc_pose   = nn.Linear(in_features=args.hidden_size, out_features=self.encoded_size)\n",
    "        \n",
    "        self.hardtanh = nn.Hardtanh(min_val=-1*args.hardtanh_limit,max_val=args.hardtanh_limit)\n",
    "        self.relu = nn.ReLU() \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.enc1= nn.Linear(in_features=34, out_features=300)\n",
    "        self.enc2 = nn.Linear(in_features=300, out_features=self.encoded_size)\n",
    "        self.dec = nn.Linear(in_features=self.encoded_size, out_features=34)\n",
    "        \n",
    "#         self.drop=n\n",
    "        \n",
    "#         self.encoder = lstm_encoder(input_size = 34, hidden_size = 1000, num_layers = 3)\n",
    "#         self.decoder = lstm_decoder(input_size = 34, hidden_size = 1000, num_layers = 3)\n",
    "\n",
    "#         self.teacher_forcing_ratio = 0.6\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "    def forward(self, pose=None, vel=None, target_pose=None):\n",
    "        \n",
    "        poses=pose.permute(1,0,2)\n",
    "#         target_poses=target_pose.permute(1,0,2)\n",
    "        \n",
    "# #         input_batch=pose\n",
    "# #         outputs = torch.zeros(target_len, input_batch.shape[1], input_batch.shape[2]).to('cuda')\n",
    "# #         # encoder outputs\n",
    "# #         encoder_output, encoder_hidden = model.encoder(input_batch)\n",
    "# #         # decoder with teacher forcing\n",
    "# #         decoder_input = input_batch[-1, :, :]   # shape: (batch_size, input_size)\n",
    "# #         decoder_hidden = encoder_hidden\n",
    "        \n",
    "# #         print(decoder_input.shape)\n",
    "\n",
    "# #         for t in range(16): \n",
    "# #             decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "# #             outputs[t] = decoder_output\n",
    "# #             decoder_input = decoder_output\n",
    "\n",
    "\n",
    "\n",
    "        pose_encoded = torch.tensor([], device=self.args.device)\n",
    "        pose_recs = torch.tensor([], device=self.args.device)\n",
    "#         target_pose_encoded = torch.tensor([], device=self.args.device)\n",
    "        \n",
    "        for i in range(pose.size()[0]):\n",
    "#             print(i)\n",
    "            x = self.relu(self.enc1(pose[i]))\n",
    "            x = self.relu(self.enc2(x))\n",
    "            recreated_pose=self.relu(self.dec(x))\n",
    "            pose_encoded = torch.cat((pose_encoded, x.unsqueeze(1)), dim = 1)\n",
    "            pose_recs = torch.cat((pose_recs, recreated_pose.unsqueeze(1)), dim = 1)\n",
    "        \n",
    "# #         vel_encoded = torch.tensor([], device=self.args.device)\n",
    "       \n",
    "# #         for i in range(vel.size()[0]):\n",
    "# # #             print(i)\n",
    "# #             x = self.enc1(vel[i])\n",
    "# #             x = self.enc2(x)\n",
    "# #             vel_encoded = torch.cat((vel_encoded, x.unsqueeze(1)), dim = 1)\n",
    "        \n",
    "# #         for i in range(target_pose.size()[0]):\n",
    "# # #             print(i)\n",
    "# #             x = self.relu(self.enc1(self.relu(target_pose[i])))\n",
    "# #             x = self.relu(self.enc2(x))\n",
    "# #             target_pose_encoded = torch.cat((target_pose_encoded, x.unsqueeze(1)), dim = 1)\n",
    "        \n",
    "# #         print(pose_encoded.size())\n",
    "# #         _, (hidden_vel, cell_vel) = self.vel_encoder()\n",
    "# #         hidden_vel = hidden_vel.squeeze(0)\n",
    "# #         cell_vel = cell_vel.squeeze(0)\n",
    "\n",
    "#         print(pose_encoded.shape)\n",
    "        \n",
    "        _, (hidden_st) = self.pose_encoder(pose_encoded)\n",
    "#         hidden_pose = hidden_pose#.squeeze(0)\n",
    "#         cell_pose = cell_pose#.squeeze(0)\n",
    "        \n",
    "        outputs = []\n",
    "        pose_outputs   = torch.tensor([], device=self.args.device)\n",
    "        PoseDec_inp = pose_encoded.permute(1,0,2)[:,-1,:].unsqueeze(0)\n",
    "        \n",
    "#         print(PoseDec_inp.size())\n",
    "        hidden_dec=hidden_st\n",
    "#         cell_dec=cell_pose\n",
    "#         print(PoseDec_inp.size(),hidden_dec.shape)\n",
    "        \n",
    "        for i in range(self.args.output//self.args.skip):\n",
    "            pose_output_l, hidden_dec = self.pose_decoder(PoseDec_inp, hidden_dec)\n",
    "#             print(hidden_dec)\n",
    "\n",
    "            pose_output_encoded  = self.fc_pose(pose_output_l.squeeze(0))\n",
    "            pose_output= self.dec(pose_output_encoded)\n",
    "            pose_outputs = torch.cat((pose_outputs, pose_output.unsqueeze(1)), dim = 1)\n",
    "            PoseDec_inp  = pose_output_encoded.detach().unsqueeze(0)\n",
    "        \n",
    "#         if random.random() < self.teacher_forcing_ratio:\n",
    "#             for i in range(self.args.output//self.args.skip):\n",
    "#                 hidden_dec, cell_dec = self.pose_decoder(PoseDec_inp, (hidden_dec, cell_dec))\n",
    "#                 pose_output_encoded  = self.fc_pose(hidden_dec)\n",
    "#                 pose_output= self.relu(self.dec(pose_output_encoded))\n",
    "#                 pose_outputs = torch.cat((pose_outputs, pose_output.unsqueeze(1)), dim = 1)\n",
    "#                 PoseDec_inp  = target_pose_encoded[i,:,:]\n",
    "\n",
    "#                 # predict recursively \n",
    "#         else:\n",
    "#             for i in range(self.args.output//self.args.skip):\n",
    "#                 hidden_dec, cell_dec = self.pose_decoder(PoseDec_inp, (hidden_dec, cell_dec))\n",
    "#                 pose_output_encoded  = self.fc_pose(hidden_dec)\n",
    "#                 pose_output= self.relu(self.dec(pose_output_encoded))\n",
    "#                 pose_outputs = torch.cat((pose_outputs, pose_output.unsqueeze(1)), dim = 1)\n",
    "#                 PoseDec_inp  = pose_output_encoded.detach()\n",
    "        \n",
    "#         print(pose_outputs.shape)\n",
    "#         pose_outputs=pose_outputs.permute(1,0,2)\n",
    "        outputs.append(pose_outputs)\n",
    "        outputs.append(pose_recs.permute(1,0,2))\n",
    "\n",
    "            \n",
    "        return tuple(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "simplified-endorsement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train\n",
      "Pose loaded\n",
      "Future_Pose loaded\n",
      "******************************\n",
      "Loading val\n",
      "Pose loaded\n",
      "Future_Pose loaded\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "class args():\n",
    "    def __init__(self):\n",
    "        self.loader_workers = 1\n",
    "        self.loader_shuffle = True\n",
    "        self.pin_memory     = False\n",
    "        self.device         = 'cuda'\n",
    "        self.batch_size     = 50\n",
    "        self.n_epochs       = 1000\n",
    "        self.hidden_size    = 1000\n",
    "        self.hardtanh_limit = 100\n",
    "        self.input  = 16\n",
    "        self.output = 16\n",
    "        self.stride = 16\n",
    "        self.skip   = 1\n",
    "        self.lr = 0.01\n",
    "args = args()\n",
    "\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "l1e = nn.L1Loss()\n",
    "train_s_scores = []\n",
    "train_pose_scores=[]\n",
    "val_pose_scores=[]\n",
    "train_c_scores = []\n",
    "val_s_scores   = []\n",
    "val_c_scores   = []\n",
    "\n",
    "# train_loader=data_loader(args,\"train\")\n",
    "# val_loader=data_loader(args,\"val\")\n",
    "\n",
    "train_loader=data_loader(args,\"train\",\"sequences_openpifpaf_\")\n",
    "val_loader=data_loader(args,\"val\",\"sequences_openpifpaf_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "interesting-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LSTM_spat(args).to(args.device)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=args.lr)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=15, \n",
    "#                                                  threshold = 1e-8, verbose=True)\n",
    "state_dict = torch.load('model_best.weight')\n",
    "# state_dict = torch.load('model_temp.weight')\n",
    "\n",
    "# print(state_dict)\n",
    "with torch.no_grad():\n",
    "    net.enc1.weight.copy_(state_dict['enc1.weight'])\n",
    "    net.enc1.bias.copy_(state_dict['enc1.bias'])\n",
    "    net.enc2.weight.copy_(state_dict['enc2.weight'])\n",
    "    net.enc2.bias.copy_(state_dict['enc2.bias'])\n",
    "    net.dec.weight.copy_(state_dict['dec4.weight'])\n",
    "    net.dec.bias.copy_(state_dict['dec4.bias'])\n",
    "    net.enc1.requires_grad = False\n",
    "    net.enc2.requires_grad = False\n",
    "    net.dec.requires_grad = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "rolled-intermediate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Training ...\n",
      "e: 0 | loss: 851.31 | val_loss: 862.31 | tpose: 851.31 | vpose: 862.31 | ade_train: 1257.86 | ade_val: 1286.46 | fde_train: 1256.09 | fde_val: 1284.93 | t:0.70\n",
      "e: 1 | loss: 794.99 | val_loss: 802.86 | tpose: 794.99 | vpose: 802.86 | ade_train: 1181.16 | ade_val: 1206.50 | fde_train: 1178.23 | fde_val: 1205.55 | t:0.67\n",
      "e: 2 | loss: 735.87 | val_loss: 745.27 | tpose: 735.87 | vpose: 745.27 | ade_train: 1101.59 | ade_val: 1129.24 | fde_train: 1099.40 | fde_val: 1128.64 | t:0.64\n",
      "e: 3 | loss: 677.12 | val_loss: 682.97 | tpose: 677.12 | vpose: 682.97 | ade_train: 1021.59 | ade_val: 1044.27 | fde_train: 1020.01 | fde_val: 1043.99 | t:0.65\n",
      "e: 4 | loss: 612.64 | val_loss: 614.84 | tpose: 612.64 | vpose: 614.84 | ade_train: 933.44 | ade_val: 953.09 | fde_train: 932.55 | fde_val: 953.28 | t:0.64\n",
      "e: 5 | loss: 544.68 | val_loss: 543.89 | tpose: 544.68 | vpose: 543.89 | ade_train: 841.84 | ade_val: 859.43 | fde_train: 841.48 | fde_val: 860.52 | t:0.63\n",
      "e: 6 | loss: 478.15 | val_loss: 478.45 | tpose: 478.15 | vpose: 478.45 | ade_train: 752.75 | ade_val: 770.09 | fde_train: 754.07 | fde_val: 773.02 | t:0.64\n",
      "e: 7 | loss: 421.38 | val_loss: 431.46 | tpose: 421.38 | vpose: 431.46 | ade_train: 673.22 | ade_val: 693.27 | fde_train: 676.28 | fde_val: 698.70 | t:0.65\n",
      "e: 8 | loss: 382.26 | val_loss: 391.82 | tpose: 382.26 | vpose: 391.82 | ade_train: 609.32 | ade_val: 629.60 | fde_train: 613.81 | fde_val: 637.00 | t:0.64\n",
      "e: 9 | loss: 347.40 | val_loss: 354.36 | tpose: 347.40 | vpose: 354.36 | ade_train: 556.65 | ade_val: 575.02 | fde_train: 562.35 | fde_val: 583.49 | t:0.64\n",
      "e: 10 | loss: 316.34 | val_loss: 317.60 | tpose: 316.34 | vpose: 317.60 | ade_train: 513.40 | ade_val: 526.62 | fde_train: 521.00 | fde_val: 536.04 | t:0.63\n",
      "e: 11 | loss: 285.68 | val_loss: 286.68 | tpose: 285.68 | vpose: 286.68 | ade_train: 475.52 | ade_val: 485.72 | fde_train: 484.19 | fde_val: 496.60 | t:0.65\n",
      "e: 12 | loss: 264.61 | val_loss: 266.20 | tpose: 264.61 | vpose: 266.20 | ade_train: 447.40 | ade_val: 451.77 | fde_train: 457.34 | fde_val: 464.58 | t:0.63\n",
      "e: 13 | loss: 248.02 | val_loss: 246.23 | tpose: 248.02 | vpose: 246.23 | ade_train: 423.07 | ade_val: 422.83 | fde_train: 433.74 | fde_val: 438.03 | t:0.65\n",
      "e: 14 | loss: 229.79 | val_loss: 228.58 | tpose: 229.79 | vpose: 228.58 | ade_train: 399.37 | ade_val: 401.48 | fde_train: 411.20 | fde_val: 419.04 | t:0.64\n",
      "e: 15 | loss: 215.90 | val_loss: 219.34 | tpose: 215.90 | vpose: 219.34 | ade_train: 382.25 | ade_val: 390.64 | fde_train: 395.24 | fde_val: 409.99 | t:0.64\n",
      "e: 16 | loss: 209.75 | val_loss: 217.56 | tpose: 209.75 | vpose: 217.56 | ade_train: 374.22 | ade_val: 387.29 | fde_train: 387.60 | fde_val: 407.96 | t:0.65\n",
      "e: 17 | loss: 210.29 | val_loss: 217.21 | tpose: 210.29 | vpose: 217.21 | ade_train: 373.36 | ade_val: 383.57 | fde_train: 387.47 | fde_val: 405.02 | t:0.69\n",
      "e: 18 | loss: 208.68 | val_loss: 210.98 | tpose: 208.68 | vpose: 210.98 | ade_train: 370.09 | ade_val: 375.24 | fde_train: 384.70 | fde_val: 397.45 | t:0.65\n",
      "e: 19 | loss: 204.63 | val_loss: 203.30 | tpose: 204.63 | vpose: 203.30 | ade_train: 368.44 | ade_val: 366.27 | fde_train: 383.55 | fde_val: 388.90 | t:0.69\n",
      "e: 20 | loss: 199.51 | val_loss: 200.77 | tpose: 199.51 | vpose: 200.77 | ade_train: 363.04 | ade_val: 362.96 | fde_train: 378.69 | fde_val: 385.60 | t:0.68\n",
      "e: 21 | loss: 199.62 | val_loss: 200.82 | tpose: 199.62 | vpose: 200.82 | ade_train: 363.60 | ade_val: 362.38 | fde_train: 379.63 | fde_val: 385.05 | t:0.64\n",
      "e: 22 | loss: 199.34 | val_loss: 200.48 | tpose: 199.34 | vpose: 200.48 | ade_train: 363.17 | ade_val: 362.97 | fde_train: 379.13 | fde_val: 385.89 | t:0.64\n",
      "e: 23 | loss: 198.16 | val_loss: 201.74 | tpose: 198.16 | vpose: 201.74 | ade_train: 361.84 | ade_val: 365.35 | fde_train: 377.35 | fde_val: 388.37 | t:0.64\n",
      "e: 24 | loss: 198.58 | val_loss: 202.11 | tpose: 198.58 | vpose: 202.11 | ade_train: 362.18 | ade_val: 366.38 | fde_train: 377.72 | fde_val: 389.51 | t:0.64\n",
      "e: 25 | loss: 197.10 | val_loss: 201.83 | tpose: 197.10 | vpose: 201.83 | ade_train: 359.98 | ade_val: 366.56 | fde_train: 375.19 | fde_val: 389.78 | t:0.65\n",
      "e: 26 | loss: 196.91 | val_loss: 201.56 | tpose: 196.91 | vpose: 201.56 | ade_train: 360.19 | ade_val: 366.27 | fde_train: 375.62 | fde_val: 389.48 | t:0.64\n",
      "e: 27 | loss: 196.97 | val_loss: 201.82 | tpose: 196.97 | vpose: 201.82 | ade_train: 360.34 | ade_val: 366.70 | fde_train: 375.79 | fde_val: 389.90 | t:0.64\n",
      "e: 28 | loss: 197.31 | val_loss: 202.20 | tpose: 197.31 | vpose: 202.20 | ade_train: 360.79 | ade_val: 367.46 | fde_train: 376.15 | fde_val: 390.70 | t:0.64\n",
      "e: 29 | loss: 196.51 | val_loss: 202.59 | tpose: 196.51 | vpose: 202.59 | ade_train: 359.36 | ade_val: 368.27 | fde_train: 374.47 | fde_val: 391.54 | t:0.66\n",
      "e: 30 | loss: 196.97 | val_loss: 202.72 | tpose: 196.97 | vpose: 202.72 | ade_train: 360.54 | ade_val: 368.59 | fde_train: 375.93 | fde_val: 391.87 | t:0.64\n",
      "Epoch    32: reducing learning rate of group 0 to 5.0000e-04.\n",
      "e: 31 | loss: 196.23 | val_loss: 203.16 | tpose: 196.23 | vpose: 203.16 | ade_train: 359.13 | ade_val: 369.49 | fde_train: 374.19 | fde_val: 392.78 | t:0.64\n",
      "e: 32 | loss: 196.73 | val_loss: 203.02 | tpose: 196.73 | vpose: 203.02 | ade_train: 360.20 | ade_val: 369.27 | fde_train: 375.30 | fde_val: 392.56 | t:0.64\n",
      "e: 33 | loss: 196.55 | val_loss: 203.02 | tpose: 196.55 | vpose: 203.02 | ade_train: 359.78 | ade_val: 369.34 | fde_train: 374.81 | fde_val: 392.63 | t:0.64\n",
      "e: 34 | loss: 196.42 | val_loss: 202.70 | tpose: 196.42 | vpose: 202.70 | ade_train: 359.70 | ade_val: 368.76 | fde_train: 374.98 | fde_val: 392.04 | t:0.64\n",
      "e: 35 | loss: 196.68 | val_loss: 202.46 | tpose: 196.68 | vpose: 202.46 | ade_train: 360.15 | ade_val: 368.31 | fde_train: 375.17 | fde_val: 391.58 | t:0.66\n",
      "e: 36 | loss: 195.96 | val_loss: 202.61 | tpose: 195.96 | vpose: 202.61 | ade_train: 358.82 | ade_val: 368.59 | fde_train: 373.88 | fde_val: 391.89 | t:0.65\n",
      "e: 37 | loss: 196.28 | val_loss: 202.79 | tpose: 196.28 | vpose: 202.79 | ade_train: 359.33 | ade_val: 368.87 | fde_train: 374.49 | fde_val: 392.20 | t:0.64\n",
      "e: 38 | loss: 195.81 | val_loss: 202.64 | tpose: 195.81 | vpose: 202.64 | ade_train: 358.40 | ade_val: 368.58 | fde_train: 373.65 | fde_val: 391.90 | t:0.65\n",
      "e: 39 | loss: 196.22 | val_loss: 202.43 | tpose: 196.22 | vpose: 202.43 | ade_train: 359.30 | ade_val: 368.24 | fde_train: 374.51 | fde_val: 391.53 | t:0.65\n",
      "Epoch    41: reducing learning rate of group 0 to 2.5000e-04.\n",
      "e: 40 | loss: 195.58 | val_loss: 202.45 | tpose: 195.58 | vpose: 202.45 | ade_train: 358.11 | ade_val: 368.31 | fde_train: 373.19 | fde_val: 391.59 | t:0.65\n",
      "e: 41 | loss: 196.27 | val_loss: 202.50 | tpose: 196.27 | vpose: 202.50 | ade_train: 359.32 | ade_val: 368.41 | fde_train: 374.25 | fde_val: 391.70 | t:0.66\n",
      "e: 42 | loss: 196.47 | val_loss: 202.49 | tpose: 196.47 | vpose: 202.49 | ade_train: 359.85 | ade_val: 368.38 | fde_train: 374.92 | fde_val: 391.67 | t:0.66\n",
      "e: 43 | loss: 196.31 | val_loss: 202.51 | tpose: 196.31 | vpose: 202.51 | ade_train: 359.38 | ade_val: 368.41 | fde_train: 374.59 | fde_val: 391.72 | t:0.64\n",
      "e: 44 | loss: 196.09 | val_loss: 202.47 | tpose: 196.09 | vpose: 202.47 | ade_train: 359.03 | ade_val: 368.33 | fde_train: 374.04 | fde_val: 391.63 | t:0.64\n",
      "e: 45 | loss: 195.93 | val_loss: 202.51 | tpose: 195.93 | vpose: 202.51 | ade_train: 358.71 | ade_val: 368.36 | fde_train: 374.03 | fde_val: 391.67 | t:0.67\n",
      "e: 46 | loss: 196.28 | val_loss: 202.53 | tpose: 196.28 | vpose: 202.53 | ade_train: 359.50 | ade_val: 368.40 | fde_train: 374.56 | fde_val: 391.72 | t:0.65\n",
      "e: 47 | loss: 196.60 | val_loss: 202.49 | tpose: 196.60 | vpose: 202.49 | ade_train: 359.95 | ade_val: 368.33 | fde_train: 375.01 | fde_val: 391.64 | t:0.65\n",
      "e: 48 | loss: 196.18 | val_loss: 202.51 | tpose: 196.18 | vpose: 202.51 | ade_train: 359.22 | ade_val: 368.38 | fde_train: 374.41 | fde_val: 391.70 | t:0.65\n",
      "Epoch    50: reducing learning rate of group 0 to 1.2500e-04.\n",
      "e: 49 | loss: 196.33 | val_loss: 202.35 | tpose: 196.33 | vpose: 202.35 | ade_train: 359.48 | ade_val: 368.09 | fde_train: 374.57 | fde_val: 391.39 | t:0.65\n",
      "e: 50 | loss: 196.19 | val_loss: 202.39 | tpose: 196.19 | vpose: 202.39 | ade_train: 359.32 | ade_val: 368.15 | fde_train: 374.39 | fde_val: 391.45 | t:0.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 51 | loss: 196.05 | val_loss: 202.40 | tpose: 196.05 | vpose: 202.40 | ade_train: 358.93 | ade_val: 368.17 | fde_train: 374.05 | fde_val: 391.47 | t:0.65\n",
      "e: 52 | loss: 195.95 | val_loss: 202.37 | tpose: 195.95 | vpose: 202.37 | ade_train: 358.83 | ade_val: 368.12 | fde_train: 374.15 | fde_val: 391.42 | t:0.65\n",
      "e: 53 | loss: 196.17 | val_loss: 202.38 | tpose: 196.17 | vpose: 202.38 | ade_train: 359.16 | ade_val: 368.15 | fde_train: 374.38 | fde_val: 391.45 | t:0.65\n",
      "e: 54 | loss: 196.08 | val_loss: 202.40 | tpose: 196.08 | vpose: 202.40 | ade_train: 359.07 | ade_val: 368.17 | fde_train: 374.37 | fde_val: 391.48 | t:0.65\n",
      "e: 55 | loss: 195.96 | val_loss: 202.41 | tpose: 195.96 | vpose: 202.41 | ade_train: 358.88 | ade_val: 368.18 | fde_train: 374.13 | fde_val: 391.49 | t:0.64\n",
      "e: 56 | loss: 196.05 | val_loss: 202.45 | tpose: 196.05 | vpose: 202.45 | ade_train: 358.98 | ade_val: 368.27 | fde_train: 374.05 | fde_val: 391.58 | t:0.65\n",
      "e: 57 | loss: 196.19 | val_loss: 202.48 | tpose: 196.19 | vpose: 202.48 | ade_train: 359.18 | ade_val: 368.32 | fde_train: 374.30 | fde_val: 391.63 | t:0.65\n",
      "Epoch    59: reducing learning rate of group 0 to 6.2500e-05.\n",
      "e: 58 | loss: 195.87 | val_loss: 202.51 | tpose: 195.87 | vpose: 202.51 | ade_train: 358.66 | ade_val: 368.38 | fde_train: 373.76 | fde_val: 391.70 | t:0.65\n",
      "e: 59 | loss: 196.42 | val_loss: 202.49 | tpose: 196.42 | vpose: 202.49 | ade_train: 359.75 | ade_val: 368.34 | fde_train: 374.82 | fde_val: 391.66 | t:0.65\n",
      "e: 60 | loss: 195.83 | val_loss: 202.49 | tpose: 195.83 | vpose: 202.49 | ade_train: 358.67 | ade_val: 368.34 | fde_train: 373.57 | fde_val: 391.65 | t:0.66\n",
      "e: 61 | loss: 196.17 | val_loss: 202.50 | tpose: 196.17 | vpose: 202.50 | ade_train: 359.11 | ade_val: 368.37 | fde_train: 374.17 | fde_val: 391.69 | t:0.65\n",
      "e: 62 | loss: 195.65 | val_loss: 202.51 | tpose: 195.65 | vpose: 202.51 | ade_train: 358.37 | ade_val: 368.40 | fde_train: 373.27 | fde_val: 391.72 | t:0.65\n",
      "e: 63 | loss: 196.59 | val_loss: 202.51 | tpose: 196.59 | vpose: 202.51 | ade_train: 360.02 | ade_val: 368.39 | fde_train: 375.16 | fde_val: 391.71 | t:0.65\n",
      "e: 64 | loss: 196.24 | val_loss: 202.51 | tpose: 196.24 | vpose: 202.51 | ade_train: 359.37 | ade_val: 368.39 | fde_train: 374.33 | fde_val: 391.71 | t:0.65\n",
      "e: 65 | loss: 196.16 | val_loss: 202.50 | tpose: 196.16 | vpose: 202.50 | ade_train: 359.28 | ade_val: 368.36 | fde_train: 374.27 | fde_val: 391.68 | t:0.64\n",
      "e: 66 | loss: 196.20 | val_loss: 202.49 | tpose: 196.20 | vpose: 202.49 | ade_train: 359.30 | ade_val: 368.35 | fde_train: 374.41 | fde_val: 391.66 | t:0.65\n",
      "Epoch    68: reducing learning rate of group 0 to 3.1250e-05.\n",
      "e: 67 | loss: 196.52 | val_loss: 202.52 | tpose: 196.52 | vpose: 202.52 | ade_train: 359.90 | ade_val: 368.41 | fde_train: 374.84 | fde_val: 391.73 | t:0.67\n",
      "e: 68 | loss: 195.95 | val_loss: 202.53 | tpose: 195.95 | vpose: 202.53 | ade_train: 358.77 | ade_val: 368.42 | fde_train: 373.92 | fde_val: 391.74 | t:0.65\n",
      "e: 69 | loss: 196.18 | val_loss: 202.52 | tpose: 196.18 | vpose: 202.52 | ade_train: 359.25 | ade_val: 368.42 | fde_train: 374.55 | fde_val: 391.74 | t:0.65\n",
      "e: 70 | loss: 195.86 | val_loss: 202.52 | tpose: 195.86 | vpose: 202.52 | ade_train: 358.58 | ade_val: 368.42 | fde_train: 373.72 | fde_val: 391.74 | t:0.67\n",
      "e: 71 | loss: 195.85 | val_loss: 202.52 | tpose: 195.85 | vpose: 202.52 | ade_train: 358.67 | ade_val: 368.42 | fde_train: 373.85 | fde_val: 391.74 | t:0.65\n",
      "e: 72 | loss: 196.27 | val_loss: 202.53 | tpose: 196.27 | vpose: 202.53 | ade_train: 359.37 | ade_val: 368.42 | fde_train: 374.28 | fde_val: 391.74 | t:0.65\n",
      "e: 73 | loss: 195.92 | val_loss: 202.52 | tpose: 195.92 | vpose: 202.52 | ade_train: 358.78 | ade_val: 368.41 | fde_train: 374.03 | fde_val: 391.72 | t:0.65\n",
      "e: 74 | loss: 196.27 | val_loss: 202.52 | tpose: 196.27 | vpose: 202.52 | ade_train: 359.34 | ade_val: 368.41 | fde_train: 374.44 | fde_val: 391.73 | t:0.65\n",
      "e: 75 | loss: 196.23 | val_loss: 202.51 | tpose: 196.23 | vpose: 202.51 | ade_train: 359.35 | ade_val: 368.39 | fde_train: 374.44 | fde_val: 391.71 | t:0.65\n",
      "Epoch    77: reducing learning rate of group 0 to 1.5625e-05.\n",
      "e: 76 | loss: 196.46 | val_loss: 202.52 | tpose: 196.46 | vpose: 202.52 | ade_train: 359.71 | ade_val: 368.42 | fde_train: 374.69 | fde_val: 391.74 | t:0.67\n",
      "e: 77 | loss: 196.49 | val_loss: 202.52 | tpose: 196.49 | vpose: 202.52 | ade_train: 359.72 | ade_val: 368.41 | fde_train: 374.86 | fde_val: 391.73 | t:0.64\n",
      "e: 78 | loss: 196.38 | val_loss: 202.52 | tpose: 196.38 | vpose: 202.52 | ade_train: 359.72 | ade_val: 368.40 | fde_train: 374.77 | fde_val: 391.72 | t:0.65\n",
      "e: 79 | loss: 196.02 | val_loss: 202.52 | tpose: 196.02 | vpose: 202.52 | ade_train: 358.95 | ade_val: 368.41 | fde_train: 374.30 | fde_val: 391.73 | t:0.64\n",
      "e: 80 | loss: 196.23 | val_loss: 202.52 | tpose: 196.23 | vpose: 202.52 | ade_train: 359.23 | ade_val: 368.41 | fde_train: 374.39 | fde_val: 391.73 | t:0.65\n",
      "e: 81 | loss: 196.59 | val_loss: 202.52 | tpose: 196.59 | vpose: 202.52 | ade_train: 360.02 | ade_val: 368.41 | fde_train: 375.10 | fde_val: 391.73 | t:0.66\n",
      "e: 82 | loss: 196.65 | val_loss: 202.52 | tpose: 196.65 | vpose: 202.52 | ade_train: 360.05 | ade_val: 368.40 | fde_train: 375.17 | fde_val: 391.72 | t:0.63\n",
      "e: 83 | loss: 196.25 | val_loss: 202.51 | tpose: 196.25 | vpose: 202.51 | ade_train: 359.41 | ade_val: 368.39 | fde_train: 374.66 | fde_val: 391.71 | t:0.65\n",
      "e: 84 | loss: 195.65 | val_loss: 202.51 | tpose: 195.65 | vpose: 202.51 | ade_train: 358.25 | ade_val: 368.39 | fde_train: 373.38 | fde_val: 391.71 | t:0.68\n",
      "Epoch    86: reducing learning rate of group 0 to 7.8125e-06.\n",
      "e: 85 | loss: 196.39 | val_loss: 202.51 | tpose: 196.39 | vpose: 202.51 | ade_train: 359.48 | ade_val: 368.39 | fde_train: 374.45 | fde_val: 391.71 | t:0.67\n",
      "e: 86 | loss: 196.38 | val_loss: 202.51 | tpose: 196.38 | vpose: 202.51 | ade_train: 359.59 | ade_val: 368.39 | fde_train: 374.75 | fde_val: 391.71 | t:0.65\n",
      "e: 87 | loss: 196.53 | val_loss: 202.51 | tpose: 196.53 | vpose: 202.51 | ade_train: 359.80 | ade_val: 368.40 | fde_train: 374.78 | fde_val: 391.72 | t:0.64\n",
      "e: 88 | loss: 196.22 | val_loss: 202.51 | tpose: 196.22 | vpose: 202.51 | ade_train: 359.31 | ade_val: 368.40 | fde_train: 374.35 | fde_val: 391.72 | t:0.65\n",
      "e: 89 | loss: 195.90 | val_loss: 202.51 | tpose: 195.90 | vpose: 202.51 | ade_train: 358.68 | ade_val: 368.40 | fde_train: 373.69 | fde_val: 391.72 | t:0.65\n",
      "e: 90 | loss: 196.40 | val_loss: 202.51 | tpose: 196.40 | vpose: 202.51 | ade_train: 359.62 | ade_val: 368.40 | fde_train: 374.76 | fde_val: 391.72 | t:0.65\n",
      "e: 91 | loss: 196.37 | val_loss: 202.51 | tpose: 196.37 | vpose: 202.51 | ade_train: 359.61 | ade_val: 368.40 | fde_train: 374.67 | fde_val: 391.72 | t:0.65\n",
      "e: 92 | loss: 195.77 | val_loss: 202.51 | tpose: 195.77 | vpose: 202.51 | ade_train: 358.48 | ade_val: 368.40 | fde_train: 373.76 | fde_val: 391.72 | t:0.65\n",
      "e: 93 | loss: 196.33 | val_loss: 202.52 | tpose: 196.33 | vpose: 202.52 | ade_train: 359.47 | ade_val: 368.41 | fde_train: 374.54 | fde_val: 391.73 | t:0.68\n",
      "Epoch    95: reducing learning rate of group 0 to 3.9063e-06.\n",
      "e: 94 | loss: 196.61 | val_loss: 202.52 | tpose: 196.61 | vpose: 202.52 | ade_train: 360.10 | ade_val: 368.41 | fde_train: 375.32 | fde_val: 391.73 | t:0.69\n",
      "e: 95 | loss: 196.14 | val_loss: 202.52 | tpose: 196.14 | vpose: 202.52 | ade_train: 359.23 | ade_val: 368.40 | fde_train: 374.32 | fde_val: 391.73 | t:0.70\n",
      "e: 96 | loss: 196.08 | val_loss: 202.52 | tpose: 196.08 | vpose: 202.52 | ade_train: 359.05 | ade_val: 368.40 | fde_train: 374.20 | fde_val: 391.72 | t:0.66\n",
      "e: 97 | loss: 195.56 | val_loss: 202.52 | tpose: 195.56 | vpose: 202.52 | ade_train: 358.17 | ade_val: 368.40 | fde_train: 373.27 | fde_val: 391.72 | t:0.66\n",
      "e: 98 | loss: 195.91 | val_loss: 202.52 | tpose: 195.91 | vpose: 202.52 | ade_train: 358.86 | ade_val: 368.40 | fde_train: 373.89 | fde_val: 391.72 | t:0.65\n",
      "e: 99 | loss: 196.51 | val_loss: 202.51 | tpose: 196.51 | vpose: 202.51 | ade_train: 359.75 | ade_val: 368.40 | fde_train: 374.78 | fde_val: 391.72 | t:0.65\n",
      "e: 100 | loss: 196.18 | val_loss: 202.51 | tpose: 196.18 | vpose: 202.51 | ade_train: 359.13 | ade_val: 368.40 | fde_train: 374.33 | fde_val: 391.72 | t:0.67\n",
      "e: 101 | loss: 196.17 | val_loss: 202.51 | tpose: 196.17 | vpose: 202.51 | ade_train: 359.33 | ade_val: 368.40 | fde_train: 374.36 | fde_val: 391.72 | t:0.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 102 | loss: 196.79 | val_loss: 202.51 | tpose: 196.79 | vpose: 202.51 | ade_train: 360.37 | ade_val: 368.40 | fde_train: 375.45 | fde_val: 391.72 | t:0.65\n",
      "Epoch   104: reducing learning rate of group 0 to 1.9531e-06.\n",
      "e: 103 | loss: 196.05 | val_loss: 202.52 | tpose: 196.05 | vpose: 202.52 | ade_train: 359.05 | ade_val: 368.40 | fde_train: 374.23 | fde_val: 391.72 | t:0.65\n",
      "e: 104 | loss: 195.89 | val_loss: 202.52 | tpose: 195.89 | vpose: 202.52 | ade_train: 358.73 | ade_val: 368.40 | fde_train: 373.81 | fde_val: 391.72 | t:0.65\n",
      "e: 105 | loss: 195.88 | val_loss: 202.52 | tpose: 195.88 | vpose: 202.52 | ade_train: 358.71 | ade_val: 368.40 | fde_train: 373.69 | fde_val: 391.72 | t:0.68\n",
      "e: 106 | loss: 196.42 | val_loss: 202.52 | tpose: 196.42 | vpose: 202.52 | ade_train: 359.69 | ade_val: 368.40 | fde_train: 374.92 | fde_val: 391.72 | t:0.73\n",
      "e: 107 | loss: 196.22 | val_loss: 202.52 | tpose: 196.22 | vpose: 202.52 | ade_train: 359.27 | ade_val: 368.40 | fde_train: 374.31 | fde_val: 391.72 | t:0.69\n",
      "e: 108 | loss: 196.27 | val_loss: 202.52 | tpose: 196.27 | vpose: 202.52 | ade_train: 359.47 | ade_val: 368.40 | fde_train: 374.63 | fde_val: 391.72 | t:0.69\n",
      "e: 109 | loss: 196.33 | val_loss: 202.52 | tpose: 196.33 | vpose: 202.52 | ade_train: 359.56 | ade_val: 368.40 | fde_train: 374.63 | fde_val: 391.72 | t:0.66\n",
      "e: 110 | loss: 195.99 | val_loss: 202.52 | tpose: 195.99 | vpose: 202.52 | ade_train: 358.93 | ade_val: 368.40 | fde_train: 373.94 | fde_val: 391.72 | t:0.65\n",
      "e: 111 | loss: 195.92 | val_loss: 202.52 | tpose: 195.92 | vpose: 202.52 | ade_train: 358.77 | ade_val: 368.40 | fde_train: 374.04 | fde_val: 391.72 | t:0.71\n",
      "Epoch   113: reducing learning rate of group 0 to 9.7656e-07.\n",
      "e: 112 | loss: 196.76 | val_loss: 202.52 | tpose: 196.76 | vpose: 202.52 | ade_train: 360.30 | ade_val: 368.41 | fde_train: 375.41 | fde_val: 391.73 | t:0.67\n",
      "e: 113 | loss: 196.36 | val_loss: 202.52 | tpose: 196.36 | vpose: 202.52 | ade_train: 359.48 | ade_val: 368.41 | fde_train: 374.60 | fde_val: 391.73 | t:0.75\n",
      "e: 114 | loss: 196.46 | val_loss: 202.52 | tpose: 196.46 | vpose: 202.52 | ade_train: 359.79 | ade_val: 368.40 | fde_train: 374.88 | fde_val: 391.72 | t:0.67\n",
      "e: 115 | loss: 196.88 | val_loss: 202.52 | tpose: 196.88 | vpose: 202.52 | ade_train: 360.52 | ade_val: 368.40 | fde_train: 375.89 | fde_val: 391.72 | t:0.65\n",
      "e: 116 | loss: 195.62 | val_loss: 202.52 | tpose: 195.62 | vpose: 202.52 | ade_train: 358.20 | ade_val: 368.40 | fde_train: 373.20 | fde_val: 391.72 | t:0.66\n",
      "e: 117 | loss: 196.05 | val_loss: 202.52 | tpose: 196.05 | vpose: 202.52 | ade_train: 359.04 | ade_val: 368.40 | fde_train: 374.05 | fde_val: 391.72 | t:0.67\n",
      "e: 118 | loss: 196.20 | val_loss: 202.52 | tpose: 196.20 | vpose: 202.52 | ade_train: 359.34 | ade_val: 368.40 | fde_train: 374.48 | fde_val: 391.72 | t:0.66\n",
      "e: 119 | loss: 196.80 | val_loss: 202.52 | tpose: 196.80 | vpose: 202.52 | ade_train: 360.33 | ade_val: 368.40 | fde_train: 375.34 | fde_val: 391.72 | t:0.72\n",
      "e: 120 | loss: 196.36 | val_loss: 202.52 | tpose: 196.36 | vpose: 202.52 | ade_train: 359.50 | ade_val: 368.40 | fde_train: 374.49 | fde_val: 391.72 | t:0.74\n",
      "Epoch   122: reducing learning rate of group 0 to 4.8828e-07.\n",
      "e: 121 | loss: 196.50 | val_loss: 202.52 | tpose: 196.50 | vpose: 202.52 | ade_train: 359.81 | ade_val: 368.40 | fde_train: 374.95 | fde_val: 391.72 | t:0.71\n",
      "e: 122 | loss: 195.49 | val_loss: 202.52 | tpose: 195.49 | vpose: 202.52 | ade_train: 357.87 | ade_val: 368.40 | fde_train: 373.10 | fde_val: 391.72 | t:0.70\n",
      "e: 123 | loss: 196.01 | val_loss: 202.52 | tpose: 196.01 | vpose: 202.52 | ade_train: 358.78 | ade_val: 368.40 | fde_train: 373.90 | fde_val: 391.72 | t:0.69\n",
      "e: 124 | loss: 196.10 | val_loss: 202.52 | tpose: 196.10 | vpose: 202.52 | ade_train: 359.05 | ade_val: 368.40 | fde_train: 374.22 | fde_val: 391.72 | t:0.69\n",
      "e: 125 | loss: 195.90 | val_loss: 202.52 | tpose: 195.90 | vpose: 202.52 | ade_train: 358.66 | ade_val: 368.40 | fde_train: 373.76 | fde_val: 391.72 | t:0.67\n",
      "e: 126 | loss: 195.71 | val_loss: 202.52 | tpose: 195.71 | vpose: 202.52 | ade_train: 358.38 | ade_val: 368.40 | fde_train: 373.60 | fde_val: 391.72 | t:0.65\n",
      "e: 127 | loss: 195.97 | val_loss: 202.52 | tpose: 195.97 | vpose: 202.52 | ade_train: 358.83 | ade_val: 368.40 | fde_train: 373.94 | fde_val: 391.72 | t:0.65\n",
      "e: 128 | loss: 196.55 | val_loss: 202.52 | tpose: 196.55 | vpose: 202.52 | ade_train: 359.98 | ade_val: 368.40 | fde_train: 375.07 | fde_val: 391.72 | t:0.72\n",
      "e: 129 | loss: 196.60 | val_loss: 202.52 | tpose: 196.60 | vpose: 202.52 | ade_train: 360.04 | ade_val: 368.40 | fde_train: 375.07 | fde_val: 391.72 | t:0.68\n",
      "Epoch   131: reducing learning rate of group 0 to 2.4414e-07.\n",
      "e: 130 | loss: 196.33 | val_loss: 202.52 | tpose: 196.33 | vpose: 202.52 | ade_train: 359.59 | ade_val: 368.40 | fde_train: 374.76 | fde_val: 391.72 | t:0.68\n",
      "e: 131 | loss: 196.13 | val_loss: 202.52 | tpose: 196.13 | vpose: 202.52 | ade_train: 359.17 | ade_val: 368.40 | fde_train: 374.18 | fde_val: 391.72 | t:0.90\n",
      "e: 132 | loss: 196.02 | val_loss: 202.52 | tpose: 196.02 | vpose: 202.52 | ade_train: 358.92 | ade_val: 368.40 | fde_train: 374.24 | fde_val: 391.72 | t:0.77\n",
      "e: 133 | loss: 195.98 | val_loss: 202.52 | tpose: 195.98 | vpose: 202.52 | ade_train: 358.80 | ade_val: 368.40 | fde_train: 373.80 | fde_val: 391.72 | t:0.71\n",
      "e: 134 | loss: 196.11 | val_loss: 202.52 | tpose: 196.11 | vpose: 202.52 | ade_train: 359.16 | ade_val: 368.40 | fde_train: 374.27 | fde_val: 391.72 | t:0.70\n",
      "e: 135 | loss: 196.41 | val_loss: 202.52 | tpose: 196.41 | vpose: 202.52 | ade_train: 359.63 | ade_val: 368.40 | fde_train: 374.70 | fde_val: 391.72 | t:0.70\n",
      "e: 136 | loss: 196.20 | val_loss: 202.52 | tpose: 196.20 | vpose: 202.52 | ade_train: 359.26 | ade_val: 368.40 | fde_train: 374.46 | fde_val: 391.72 | t:0.70\n",
      "e: 137 | loss: 196.07 | val_loss: 202.52 | tpose: 196.07 | vpose: 202.52 | ade_train: 358.92 | ade_val: 368.40 | fde_train: 374.05 | fde_val: 391.72 | t:0.70\n",
      "e: 138 | loss: 196.15 | val_loss: 202.52 | tpose: 196.15 | vpose: 202.52 | ade_train: 359.27 | ade_val: 368.40 | fde_train: 374.39 | fde_val: 391.72 | t:0.69\n",
      "Epoch   140: reducing learning rate of group 0 to 1.2207e-07.\n",
      "e: 139 | loss: 195.95 | val_loss: 202.52 | tpose: 195.95 | vpose: 202.52 | ade_train: 358.80 | ade_val: 368.40 | fde_train: 373.86 | fde_val: 391.72 | t:0.75\n",
      "e: 140 | loss: 195.46 | val_loss: 202.52 | tpose: 195.46 | vpose: 202.52 | ade_train: 357.90 | ade_val: 368.40 | fde_train: 372.88 | fde_val: 391.72 | t:0.69\n",
      "e: 141 | loss: 196.55 | val_loss: 202.52 | tpose: 196.55 | vpose: 202.52 | ade_train: 359.79 | ade_val: 368.40 | fde_train: 374.90 | fde_val: 391.72 | t:0.68\n",
      "e: 142 | loss: 196.51 | val_loss: 202.52 | tpose: 196.51 | vpose: 202.52 | ade_train: 359.81 | ade_val: 368.40 | fde_train: 374.92 | fde_val: 391.72 | t:0.69\n",
      "e: 143 | loss: 196.54 | val_loss: 202.52 | tpose: 196.54 | vpose: 202.52 | ade_train: 359.83 | ade_val: 368.40 | fde_train: 375.05 | fde_val: 391.72 | t:0.66\n",
      "e: 144 | loss: 196.33 | val_loss: 202.52 | tpose: 196.33 | vpose: 202.52 | ade_train: 359.50 | ade_val: 368.40 | fde_train: 374.53 | fde_val: 391.72 | t:0.71\n",
      "e: 145 | loss: 196.23 | val_loss: 202.52 | tpose: 196.23 | vpose: 202.52 | ade_train: 359.50 | ade_val: 368.40 | fde_train: 374.73 | fde_val: 391.72 | t:0.67\n",
      "e: 146 | loss: 197.12 | val_loss: 202.52 | tpose: 197.12 | vpose: 202.52 | ade_train: 360.99 | ade_val: 368.40 | fde_train: 376.17 | fde_val: 391.72 | t:0.74\n",
      "e: 147 | loss: 195.87 | val_loss: 202.52 | tpose: 195.87 | vpose: 202.52 | ade_train: 358.62 | ade_val: 368.40 | fde_train: 373.72 | fde_val: 391.72 | t:0.78\n",
      "Epoch   149: reducing learning rate of group 0 to 6.1035e-08.\n",
      "e: 148 | loss: 196.43 | val_loss: 202.52 | tpose: 196.43 | vpose: 202.52 | ade_train: 359.76 | ade_val: 368.40 | fde_train: 374.91 | fde_val: 391.72 | t:0.75\n",
      "e: 149 | loss: 196.28 | val_loss: 202.52 | tpose: 196.28 | vpose: 202.52 | ade_train: 359.47 | ade_val: 368.40 | fde_train: 374.66 | fde_val: 391.72 | t:0.73\n",
      "e: 150 | loss: 196.49 | val_loss: 202.52 | tpose: 196.49 | vpose: 202.52 | ade_train: 359.83 | ade_val: 368.40 | fde_train: 374.96 | fde_val: 391.72 | t:0.71\n",
      "e: 151 | loss: 196.27 | val_loss: 202.52 | tpose: 196.27 | vpose: 202.52 | ade_train: 359.45 | ade_val: 368.40 | fde_train: 374.73 | fde_val: 391.72 | t:0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 152 | loss: 196.69 | val_loss: 202.52 | tpose: 196.69 | vpose: 202.52 | ade_train: 360.19 | ade_val: 368.40 | fde_train: 375.29 | fde_val: 391.72 | t:0.77\n",
      "e: 153 | loss: 196.16 | val_loss: 202.52 | tpose: 196.16 | vpose: 202.52 | ade_train: 359.21 | ade_val: 368.40 | fde_train: 374.30 | fde_val: 391.72 | t:0.73\n",
      "e: 154 | loss: 196.25 | val_loss: 202.52 | tpose: 196.25 | vpose: 202.52 | ade_train: 359.27 | ade_val: 368.40 | fde_train: 374.25 | fde_val: 391.72 | t:0.74\n",
      "e: 155 | loss: 196.20 | val_loss: 202.52 | tpose: 196.20 | vpose: 202.52 | ade_train: 359.18 | ade_val: 368.40 | fde_train: 374.20 | fde_val: 391.72 | t:0.69\n",
      "e: 156 | loss: 196.17 | val_loss: 202.52 | tpose: 196.17 | vpose: 202.52 | ade_train: 359.18 | ade_val: 368.40 | fde_train: 374.15 | fde_val: 391.72 | t:0.76\n",
      "Epoch   158: reducing learning rate of group 0 to 3.0518e-08.\n",
      "e: 157 | loss: 196.13 | val_loss: 202.52 | tpose: 196.13 | vpose: 202.52 | ade_train: 359.16 | ade_val: 368.40 | fde_train: 374.40 | fde_val: 391.72 | t:0.77\n",
      "e: 158 | loss: 196.48 | val_loss: 202.52 | tpose: 196.48 | vpose: 202.52 | ade_train: 359.75 | ade_val: 368.40 | fde_train: 374.97 | fde_val: 391.72 | t:0.78\n",
      "e: 159 | loss: 196.04 | val_loss: 202.52 | tpose: 196.04 | vpose: 202.52 | ade_train: 358.93 | ade_val: 368.40 | fde_train: 373.96 | fde_val: 391.72 | t:0.79\n",
      "e: 160 | loss: 195.81 | val_loss: 202.52 | tpose: 195.81 | vpose: 202.52 | ade_train: 358.55 | ade_val: 368.40 | fde_train: 373.57 | fde_val: 391.72 | t:0.78\n",
      "e: 161 | loss: 195.69 | val_loss: 202.52 | tpose: 195.69 | vpose: 202.52 | ade_train: 358.21 | ade_val: 368.40 | fde_train: 373.37 | fde_val: 391.72 | t:0.76\n",
      "e: 162 | loss: 195.91 | val_loss: 202.52 | tpose: 195.91 | vpose: 202.52 | ade_train: 358.71 | ade_val: 368.40 | fde_train: 373.87 | fde_val: 391.72 | t:0.75\n",
      "e: 163 | loss: 195.96 | val_loss: 202.52 | tpose: 195.96 | vpose: 202.52 | ade_train: 358.83 | ade_val: 368.40 | fde_train: 373.84 | fde_val: 391.72 | t:0.71\n",
      "e: 164 | loss: 195.99 | val_loss: 202.52 | tpose: 195.99 | vpose: 202.52 | ade_train: 358.89 | ade_val: 368.40 | fde_train: 373.89 | fde_val: 391.72 | t:0.74\n",
      "e: 165 | loss: 196.41 | val_loss: 202.52 | tpose: 196.41 | vpose: 202.52 | ade_train: 359.71 | ade_val: 368.40 | fde_train: 374.95 | fde_val: 391.72 | t:0.79\n",
      "Epoch   167: reducing learning rate of group 0 to 1.5259e-08.\n",
      "e: 166 | loss: 196.22 | val_loss: 202.52 | tpose: 196.22 | vpose: 202.52 | ade_train: 359.27 | ade_val: 368.40 | fde_train: 374.50 | fde_val: 391.72 | t:0.74\n",
      "e: 167 | loss: 196.55 | val_loss: 202.52 | tpose: 196.55 | vpose: 202.52 | ade_train: 359.84 | ade_val: 368.40 | fde_train: 374.89 | fde_val: 391.72 | t:0.71\n",
      "e: 168 | loss: 195.74 | val_loss: 202.52 | tpose: 195.74 | vpose: 202.52 | ade_train: 358.46 | ade_val: 368.40 | fde_train: 373.54 | fde_val: 391.72 | t:0.72\n",
      "e: 169 | loss: 196.45 | val_loss: 202.52 | tpose: 196.45 | vpose: 202.52 | ade_train: 359.74 | ade_val: 368.40 | fde_train: 375.00 | fde_val: 391.72 | t:0.78\n",
      "e: 170 | loss: 195.78 | val_loss: 202.52 | tpose: 195.78 | vpose: 202.52 | ade_train: 358.46 | ade_val: 368.40 | fde_train: 373.71 | fde_val: 391.72 | t:0.79\n",
      "e: 171 | loss: 195.97 | val_loss: 202.52 | tpose: 195.97 | vpose: 202.52 | ade_train: 358.85 | ade_val: 368.40 | fde_train: 374.02 | fde_val: 391.72 | t:0.69\n",
      "e: 172 | loss: 196.23 | val_loss: 202.52 | tpose: 196.23 | vpose: 202.52 | ade_train: 359.36 | ade_val: 368.40 | fde_train: 374.53 | fde_val: 391.72 | t:0.76\n",
      "e: 173 | loss: 196.08 | val_loss: 202.52 | tpose: 196.08 | vpose: 202.52 | ade_train: 359.03 | ade_val: 368.40 | fde_train: 374.16 | fde_val: 391.72 | t:0.76\n",
      "e: 174 | loss: 196.17 | val_loss: 202.52 | tpose: 196.17 | vpose: 202.52 | ade_train: 359.26 | ade_val: 368.40 | fde_train: 374.43 | fde_val: 391.72 | t:0.76\n",
      "e: 175 | loss: 196.41 | val_loss: 202.52 | tpose: 196.41 | vpose: 202.52 | ade_train: 359.60 | ade_val: 368.40 | fde_train: 374.69 | fde_val: 391.72 | t:0.76\n",
      "e: 176 | loss: 196.33 | val_loss: 202.52 | tpose: 196.33 | vpose: 202.52 | ade_train: 359.60 | ade_val: 368.40 | fde_train: 374.97 | fde_val: 391.72 | t:0.74\n",
      "e: 177 | loss: 196.02 | val_loss: 202.52 | tpose: 196.02 | vpose: 202.52 | ade_train: 359.04 | ade_val: 368.40 | fde_train: 374.24 | fde_val: 391.72 | t:0.74\n",
      "e: 178 | loss: 196.41 | val_loss: 202.52 | tpose: 196.41 | vpose: 202.52 | ade_train: 359.76 | ade_val: 368.40 | fde_train: 374.97 | fde_val: 391.72 | t:0.74\n",
      "e: 179 | loss: 196.33 | val_loss: 202.52 | tpose: 196.33 | vpose: 202.52 | ade_train: 359.50 | ade_val: 368.40 | fde_train: 374.66 | fde_val: 391.72 | t:0.74\n",
      "e: 180 | loss: 196.59 | val_loss: 202.52 | tpose: 196.59 | vpose: 202.52 | ade_train: 359.96 | ade_val: 368.40 | fde_train: 375.01 | fde_val: 391.72 | t:0.73\n",
      "e: 181 | loss: 195.84 | val_loss: 202.52 | tpose: 195.84 | vpose: 202.52 | ade_train: 358.62 | ade_val: 368.40 | fde_train: 373.70 | fde_val: 391.72 | t:0.70\n",
      "e: 182 | loss: 195.85 | val_loss: 202.52 | tpose: 195.85 | vpose: 202.52 | ade_train: 358.60 | ade_val: 368.40 | fde_train: 373.77 | fde_val: 391.72 | t:0.74\n",
      "e: 183 | loss: 196.37 | val_loss: 202.52 | tpose: 196.37 | vpose: 202.52 | ade_train: 359.63 | ade_val: 368.40 | fde_train: 374.67 | fde_val: 391.72 | t:0.80\n",
      "e: 184 | loss: 196.03 | val_loss: 202.52 | tpose: 196.03 | vpose: 202.52 | ade_train: 359.00 | ade_val: 368.40 | fde_train: 374.10 | fde_val: 391.72 | t:0.80\n",
      "e: 185 | loss: 195.76 | val_loss: 202.52 | tpose: 195.76 | vpose: 202.52 | ade_train: 358.41 | ade_val: 368.40 | fde_train: 373.46 | fde_val: 391.72 | t:1.01\n",
      "e: 186 | loss: 196.10 | val_loss: 202.52 | tpose: 196.10 | vpose: 202.52 | ade_train: 359.05 | ade_val: 368.40 | fde_train: 374.41 | fde_val: 391.72 | t:0.96\n",
      "e: 187 | loss: 196.24 | val_loss: 202.52 | tpose: 196.24 | vpose: 202.52 | ade_train: 359.23 | ade_val: 368.40 | fde_train: 374.49 | fde_val: 391.72 | t:1.07\n",
      "e: 188 | loss: 196.77 | val_loss: 202.52 | tpose: 196.77 | vpose: 202.52 | ade_train: 360.32 | ade_val: 368.40 | fde_train: 375.29 | fde_val: 391.72 | t:1.08\n",
      "e: 189 | loss: 196.17 | val_loss: 202.52 | tpose: 196.17 | vpose: 202.52 | ade_train: 359.09 | ade_val: 368.40 | fde_train: 374.09 | fde_val: 391.72 | t:1.09\n",
      "e: 190 | loss: 196.48 | val_loss: 202.52 | tpose: 196.48 | vpose: 202.52 | ade_train: 359.66 | ade_val: 368.40 | fde_train: 374.66 | fde_val: 391.72 | t:1.09\n",
      "e: 191 | loss: 196.28 | val_loss: 202.52 | tpose: 196.28 | vpose: 202.52 | ade_train: 359.42 | ade_val: 368.40 | fde_train: 374.52 | fde_val: 391.72 | t:1.09\n",
      "e: 192 | loss: 196.02 | val_loss: 202.52 | tpose: 196.02 | vpose: 202.52 | ade_train: 359.00 | ade_val: 368.40 | fde_train: 374.29 | fde_val: 391.72 | t:1.07\n",
      "e: 193 | loss: 196.62 | val_loss: 202.52 | tpose: 196.62 | vpose: 202.52 | ade_train: 359.92 | ade_val: 368.40 | fde_train: 375.07 | fde_val: 391.72 | t:1.05\n",
      "e: 194 | loss: 196.26 | val_loss: 202.52 | tpose: 196.26 | vpose: 202.52 | ade_train: 359.25 | ade_val: 368.40 | fde_train: 374.27 | fde_val: 391.72 | t:1.15\n",
      "e: 195 | loss: 195.95 | val_loss: 202.52 | tpose: 195.95 | vpose: 202.52 | ade_train: 358.83 | ade_val: 368.40 | fde_train: 373.90 | fde_val: 391.72 | t:1.15\n",
      "e: 196 | loss: 196.12 | val_loss: 202.52 | tpose: 196.12 | vpose: 202.52 | ade_train: 359.15 | ade_val: 368.40 | fde_train: 374.23 | fde_val: 391.72 | t:1.15\n",
      "e: 197 | loss: 196.03 | val_loss: 202.52 | tpose: 196.03 | vpose: 202.52 | ade_train: 358.89 | ade_val: 368.40 | fde_train: 373.82 | fde_val: 391.72 | t:1.13\n",
      "e: 198 | loss: 196.39 | val_loss: 202.52 | tpose: 196.39 | vpose: 202.52 | ade_train: 359.60 | ade_val: 368.40 | fde_train: 374.83 | fde_val: 391.72 | t:1.09\n",
      "e: 199 | loss: 196.37 | val_loss: 202.52 | tpose: 196.37 | vpose: 202.52 | ade_train: 359.62 | ade_val: 368.40 | fde_train: 374.75 | fde_val: 391.72 | t:0.99\n",
      "e: 200 | loss: 196.11 | val_loss: 202.52 | tpose: 196.11 | vpose: 202.52 | ade_train: 359.02 | ade_val: 368.40 | fde_train: 374.10 | fde_val: 391.72 | t:0.94\n",
      "e: 201 | loss: 195.80 | val_loss: 202.52 | tpose: 195.80 | vpose: 202.52 | ade_train: 358.46 | ade_val: 368.40 | fde_train: 373.56 | fde_val: 391.72 | t:0.94\n",
      "e: 202 | loss: 196.62 | val_loss: 202.52 | tpose: 196.62 | vpose: 202.52 | ade_train: 360.13 | ade_val: 368.40 | fde_train: 375.26 | fde_val: 391.72 | t:0.95\n",
      "e: 203 | loss: 195.64 | val_loss: 202.52 | tpose: 195.64 | vpose: 202.52 | ade_train: 358.20 | ade_val: 368.40 | fde_train: 373.33 | fde_val: 391.72 | t:0.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 204 | loss: 196.47 | val_loss: 202.52 | tpose: 196.47 | vpose: 202.52 | ade_train: 359.79 | ade_val: 368.40 | fde_train: 374.91 | fde_val: 391.72 | t:0.77\n",
      "e: 205 | loss: 195.97 | val_loss: 202.52 | tpose: 195.97 | vpose: 202.52 | ade_train: 358.89 | ade_val: 368.40 | fde_train: 374.06 | fde_val: 391.72 | t:0.78\n",
      "e: 206 | loss: 196.03 | val_loss: 202.52 | tpose: 196.03 | vpose: 202.52 | ade_train: 359.07 | ade_val: 368.40 | fde_train: 374.17 | fde_val: 391.72 | t:0.72\n",
      "e: 207 | loss: 196.34 | val_loss: 202.52 | tpose: 196.34 | vpose: 202.52 | ade_train: 359.53 | ade_val: 368.40 | fde_train: 374.66 | fde_val: 391.72 | t:0.78\n",
      "e: 208 | loss: 196.07 | val_loss: 202.52 | tpose: 196.07 | vpose: 202.52 | ade_train: 359.01 | ade_val: 368.40 | fde_train: 374.18 | fde_val: 391.72 | t:0.75\n",
      "e: 209 | loss: 195.69 | val_loss: 202.52 | tpose: 195.69 | vpose: 202.52 | ade_train: 358.39 | ade_val: 368.40 | fde_train: 373.48 | fde_val: 391.72 | t:0.70\n",
      "e: 210 | loss: 196.27 | val_loss: 202.52 | tpose: 196.27 | vpose: 202.52 | ade_train: 359.28 | ade_val: 368.40 | fde_train: 374.36 | fde_val: 391.72 | t:0.80\n",
      "e: 211 | loss: 195.82 | val_loss: 202.52 | tpose: 195.82 | vpose: 202.52 | ade_train: 358.62 | ade_val: 368.40 | fde_train: 373.58 | fde_val: 391.72 | t:0.76\n",
      "e: 212 | loss: 196.28 | val_loss: 202.52 | tpose: 196.28 | vpose: 202.52 | ade_train: 359.45 | ade_val: 368.40 | fde_train: 374.47 | fde_val: 391.72 | t:0.75\n",
      "e: 213 | loss: 195.97 | val_loss: 202.52 | tpose: 195.97 | vpose: 202.52 | ade_train: 358.73 | ade_val: 368.40 | fde_train: 373.80 | fde_val: 391.72 | t:0.69\n",
      "e: 214 | loss: 195.70 | val_loss: 202.52 | tpose: 195.70 | vpose: 202.52 | ade_train: 358.28 | ade_val: 368.40 | fde_train: 373.47 | fde_val: 391.72 | t:0.66\n",
      "e: 215 | loss: 196.39 | val_loss: 202.52 | tpose: 196.39 | vpose: 202.52 | ade_train: 359.76 | ade_val: 368.40 | fde_train: 374.83 | fde_val: 391.72 | t:0.68\n",
      "e: 216 | loss: 196.08 | val_loss: 202.52 | tpose: 196.08 | vpose: 202.52 | ade_train: 359.01 | ade_val: 368.40 | fde_train: 374.20 | fde_val: 391.72 | t:0.70\n",
      "e: 217 | loss: 196.39 | val_loss: 202.52 | tpose: 196.39 | vpose: 202.52 | ade_train: 359.69 | ade_val: 368.40 | fde_train: 374.69 | fde_val: 391.72 | t:0.65\n",
      "e: 218 | loss: 196.09 | val_loss: 202.52 | tpose: 196.09 | vpose: 202.52 | ade_train: 359.16 | ade_val: 368.40 | fde_train: 374.21 | fde_val: 391.72 | t:0.63\n",
      "e: 219 | loss: 196.36 | val_loss: 202.52 | tpose: 196.36 | vpose: 202.52 | ade_train: 359.62 | ade_val: 368.40 | fde_train: 374.82 | fde_val: 391.72 | t:0.67\n",
      "e: 220 | loss: 195.89 | val_loss: 202.52 | tpose: 195.89 | vpose: 202.52 | ade_train: 358.68 | ade_val: 368.40 | fde_train: 373.83 | fde_val: 391.72 | t:0.71\n",
      "e: 221 | loss: 196.38 | val_loss: 202.52 | tpose: 196.38 | vpose: 202.52 | ade_train: 359.60 | ade_val: 368.40 | fde_train: 374.88 | fde_val: 391.72 | t:0.77\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-6d6c0ca0bd99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mavg_epoch_train_p_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mavg_epoch_train_p_loss\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr= 0.001)\n",
    "# optimizer = optim.Adadelta(net.parameters(),lr= 0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=8, \n",
    "                                                 threshold = 1e-12, verbose=True)\n",
    "print('='*100)\n",
    "print('Training ...')\n",
    "\n",
    "train_p_scores=[]\n",
    "val_p_scores=[]\n",
    "\n",
    "for epoch in range(500):\n",
    "    start = time.time()\n",
    "    \n",
    "    avg_epoch_train_p_loss   = 0\n",
    "    avg_epoch_val_p_loss     = 0 \n",
    "    ade  = 0\n",
    "    fde  = 0\n",
    "    ade_train  = 0\n",
    "    fde_train  = 0\n",
    "    counter = 0\n",
    "    net.train()\n",
    "    for idx, (obs_s, target_s, obs_pose, target_pose) in enumerate(train_loader):\n",
    "        counter += 1        \n",
    "        \n",
    "        \n",
    "        obs_pose    = obs_pose.to(device='cuda')\n",
    "        target_pose = target_pose.to(device='cuda')\n",
    " \n",
    "        net.zero_grad()\n",
    "    \n",
    "        (pose_preds,pose_recs) = net(pose=obs_pose, target_pose=target_pose)\n",
    "\n",
    "        loss  = l1e(pose_preds, target_pose)#+l1e(pose_recs, obs_pose)\n",
    "    \n",
    "        ade_train += float(ADE_c(pose_preds, target_pose))\n",
    "        fde_train += float(FDE_c(pose_preds, target_pose))\n",
    "        \n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "        avg_epoch_train_p_loss += float(loss)\n",
    "\n",
    "    avg_epoch_train_p_loss /= counter\n",
    "    train_p_scores.append(avg_epoch_train_p_loss)\n",
    "    ade_train  /= counter\n",
    "    fde_train  /= counter   \n",
    "    \n",
    "    \n",
    "  \n",
    "    counter=0\n",
    "    net.eval()\n",
    "    for idx, (obs_s, target_s, obs_pose, target_pose) in enumerate(val_loader):\n",
    "        counter+=1\n",
    "       \n",
    "        obs_pose    = obs_pose.to(device='cuda')\n",
    "        target_pose = target_pose.to(device='cuda')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            (pose_preds,pose_recs) = net(pose=obs_pose, target_pose=target_pose)\n",
    "          \n",
    "            val_loss  = l1e(pose_preds, target_pose)#+l1e(pose_recs, obs_pose)\n",
    "            avg_epoch_val_p_loss += float(val_loss)\n",
    "            \n",
    "            ade += float(ADE_c(pose_preds, target_pose))\n",
    "            fde += float(FDE_c(pose_preds, target_pose))\n",
    "\n",
    "        \n",
    "    avg_epoch_val_p_loss /= counter\n",
    "    val_p_scores.append(avg_epoch_val_p_loss)\n",
    "    \n",
    "    ade  /= counter\n",
    "    fde  /= counter     \n",
    "   \n",
    "    \n",
    "    \n",
    "    scheduler.step(avg_epoch_val_p_loss)\n",
    "    \n",
    "    print('e:', epoch,'| loss: %.2f'%avg_epoch_train_p_loss,'| val_loss: %.2f'% avg_epoch_val_p_loss, '| tpose: %.2f'% avg_epoch_train_p_loss, '| vpose: %.2f'% avg_epoch_val_p_loss, '| ade_train: %.2f'% ade_train, '| ade_val: %.2f'% ade, '| fde_train: %.2f'% fde_train,'| fde_val: %.2f'% fde,\n",
    "          '| t:%.2f'%(time.time()-start))\n",
    "\n",
    "\n",
    "print('='*100) \n",
    "# print('Saving ...')\n",
    "# torch.save(net.state_dict(), args.model_path)\n",
    "print('Done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-controversy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dying-priest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm_seq2seq(\n",
       "  (encoder): lstm_encoder(\n",
       "    (lstm): LSTM(34, 1000, num_layers=3)\n",
       "  )\n",
       "  (decoder): lstm_decoder(\n",
       "    (lstm): LSTM(34, 1000, num_layers=3)\n",
       "    (linear): Linear(in_features=1000, out_features=34, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os, errno\n",
    "import sys\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class lstm_encoder(nn.Module):\n",
    "    ''' Encodes time-series sequence '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
    "        \n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n",
    "        :                       2 stacked LSTMs)\n",
    "        '''\n",
    "        \n",
    "        super(lstm_encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
    "                            num_layers = num_layers)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        \n",
    "        '''\n",
    "        : param x_input:               input of shape (seq_len, # in batch, input_size)\n",
    "        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence;\n",
    "        :                              hidden gives the hidden state and cell state for the last\n",
    "        :                              element in the sequence \n",
    "        '''\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(x_input.view(x_input.shape[0], x_input.shape[1], self.input_size))\n",
    "        \n",
    "        return lstm_out, self.hidden     \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        '''\n",
    "        initialize hidden state\n",
    "        : param batch_size:    x_input.shape[1]\n",
    "        : return:              zeroed hidden state and cell state \n",
    "        '''\n",
    "        \n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "\n",
    "\n",
    "class lstm_decoder(nn.Module):\n",
    "    ''' Decodes hidden state output by encoder '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
    "\n",
    "        '''\n",
    "        : param input_size:     the number of features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        : param num_layers:     number of recurrent layers (i.e., 2 means there are\n",
    "        :                       2 stacked LSTMs)\n",
    "        '''\n",
    "        \n",
    "        super(lstm_decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
    "                            num_layers = num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, input_size)           \n",
    "\n",
    "    def forward(self, x_input, encoder_hidden_states):\n",
    "        \n",
    "        '''        \n",
    "        : param x_input:                    should be 2D (batch_size, input_size)\n",
    "        : param encoder_hidden_states:      hidden states\n",
    "        : return output, hidden:            output gives all the hidden states in the sequence;\n",
    "        :                                   hidden gives the hidden state and cell state for the last\n",
    "        :                                   element in the sequence \n",
    " \n",
    "        '''\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(x_input.unsqueeze(0), encoder_hidden_states)\n",
    "        output = self.linear(lstm_out.squeeze(0))     \n",
    "        \n",
    "        return output, self.hidden\n",
    "\n",
    "class lstm_seq2seq(nn.Module):\n",
    "    ''' train LSTM encoder-decoder and make predictions '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "\n",
    "        '''\n",
    "        : param input_size:     the number of expected features in the input X\n",
    "        : param hidden_size:    the number of features in the hidden state h\n",
    "        '''\n",
    "\n",
    "        super(lstm_seq2seq, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.encoder = lstm_encoder(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers)\n",
    "        self.decoder = lstm_decoder(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers)\n",
    "\n",
    "\n",
    "    def train_model(self, input_tensor, target_tensor, n_epochs, target_len, batch_size, training_prediction = 'recursive', teacher_forcing_ratio = 0.5, learning_rate = 0.01, dynamic_tf = False):\n",
    "        \n",
    "        '''\n",
    "        train lstm encoder-decoder\n",
    "        \n",
    "        : param input_tensor:              input data with shape (seq_len, # in batch, number features); PyTorch tensor    \n",
    "        : param target_tensor:             target data with shape (seq_len, # in batch, number features); PyTorch tensor\n",
    "        : param n_epochs:                  number of epochs \n",
    "        : param target_len:                number of values to predict \n",
    "        : param batch_size:                number of samples per gradient update\n",
    "        : param training_prediction:       type of prediction to make during training ('recursive', 'teacher_forcing', or\n",
    "        :                                  'mixed_teacher_forcing'); default is 'recursive'\n",
    "        : param teacher_forcing_ratio:     float [0, 1) indicating how much teacher forcing to use when\n",
    "        :                                  training_prediction = 'teacher_forcing.' For each batch in training, we generate a random\n",
    "        :                                  number. If the random number is less than teacher_forcing_ratio, we use teacher forcing.\n",
    "        :                                  Otherwise, we predict recursively. If teacher_forcing_ratio = 1, we train only using\n",
    "        :                                  teacher forcing.\n",
    "        : param learning_rate:             float >= 0; learning rate\n",
    "        : param dynamic_tf:                use dynamic teacher forcing (True/False); dynamic teacher forcing\n",
    "        :                                  reduces the amount of teacher forcing for each epoch\n",
    "        : return losses:                   array of loss function for each epoch\n",
    "        '''\n",
    "        \n",
    "        # initialize array of losses \n",
    "        losses = np.full(n_epochs, np.nan)\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "        criterion = nn.L1Loss()\n",
    "#         criterion = nn.MSELoss()\n",
    "\n",
    "        # calculate number of batch iterations\n",
    "        n_batches = int(input_tensor.shape[1] / batch_size)\n",
    "\n",
    "        with trange(n_epochs) as tr:\n",
    "            for it in tr:\n",
    "                \n",
    "                batch_loss = 0.\n",
    "                batch_loss_tf = 0.\n",
    "                batch_loss_no_tf = 0.\n",
    "                num_tf = 0\n",
    "                num_no_tf = 0\n",
    "\n",
    "                for b in range(n_batches):\n",
    "                    # select data \n",
    "                    input_batch = input_tensor[:, b: b + batch_size, :]\n",
    "                    target_batch = target_tensor[:, b: b + batch_size, :]\n",
    "\n",
    "                    # outputs tensor\n",
    "                    outputs = torch.zeros(target_len, batch_size, input_batch.shape[2])\n",
    "\n",
    "                    # initialize hidden state\n",
    "                    encoder_hidden = self.encoder.init_hidden(batch_size)\n",
    "\n",
    "                    # zero the gradient\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # encoder outputs\n",
    "                    encoder_output, encoder_hidden = self.encoder(input_batch)\n",
    "\n",
    "                    # decoder with teacher forcing\n",
    "                    decoder_input = input_batch[-1, :, :]   # shape: (batch_size, input_size)\n",
    "                    decoder_hidden = encoder_hidden\n",
    "\n",
    "                    if training_prediction == 'recursive':\n",
    "                        # predict recursively\n",
    "                        for t in range(target_len): \n",
    "                            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                            outputs[t] = decoder_output\n",
    "                            decoder_input = decoder_output\n",
    "\n",
    "                    if training_prediction == 'teacher_forcing':\n",
    "                        # use teacher forcing\n",
    "                        if random.random() < teacher_forcing_ratio:\n",
    "                            for t in range(target_len): \n",
    "                                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                                outputs[t] = decoder_output\n",
    "                                decoder_input = target_batch[t, :, :]\n",
    "\n",
    "                        # predict recursively \n",
    "                        else:\n",
    "                            for t in range(target_len): \n",
    "                                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                                outputs[t] = decoder_output\n",
    "                                decoder_input = decoder_output\n",
    "\n",
    "                    if training_prediction == 'mixed_teacher_forcing':\n",
    "                        # predict using mixed teacher forcing\n",
    "                        for t in range(target_len):\n",
    "                            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                            outputs[t] = decoder_output\n",
    "                            \n",
    "                            # predict with teacher forcing\n",
    "                            if random.random() < teacher_forcing_ratio:\n",
    "                                decoder_input = target_batch[t, :, :]\n",
    "                            \n",
    "                            # predict recursively \n",
    "                            else:\n",
    "                                decoder_input = decoder_output\n",
    "\n",
    "                    # compute the loss \n",
    "                    loss = criterion(outputs, target_batch)\n",
    "                    batch_loss += loss.item()\n",
    "                    \n",
    "                    # backpropagation\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # loss for epoch \n",
    "                batch_loss /= n_batches \n",
    "                losses[it] = batch_loss\n",
    "\n",
    "                # dynamic teacher forcing\n",
    "                if dynamic_tf and teacher_forcing_ratio > 0:\n",
    "                    teacher_forcing_ratio = teacher_forcing_ratio - 0.02 \n",
    "\n",
    "                # progress bar \n",
    "                tr.set_postfix(loss=\"{0:.3f}\".format(batch_loss))\n",
    "                    \n",
    "        return losses\n",
    "\n",
    "    def predict(self, input_tensor, target_len):\n",
    "        \n",
    "        '''\n",
    "        : param input_tensor:      input data (seq_len, input_size); PyTorch tensor \n",
    "        : param target_len:        number of target values to predict \n",
    "        : return np_outputs:       np.array containing predicted values; prediction done recursively \n",
    "        '''\n",
    "\n",
    "        # encode input_tensor\n",
    "        input_tensor = input_tensor.unsqueeze(1)     # add in batch size of 1\n",
    "        encoder_output, encoder_hidden = self.encoder(input_tensor)\n",
    "\n",
    "        # initialize tensor for predictions\n",
    "        outputs = torch.zeros(target_len, input_tensor.shape[2])\n",
    "\n",
    "        # decode input_tensor\n",
    "        decoder_input = input_tensor[-1, :, :]\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for t in range(target_len):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            outputs[t] = decoder_output.squeeze(0)\n",
    "            decoder_input = decoder_output\n",
    "            \n",
    "        np_outputs = outputs.detach().numpy()\n",
    "        \n",
    "        return np_output\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "model = lstm_seq2seq(input_size = 34, hidden_size = 1000, num_layers=3)\n",
    "device = torch.device('cuda:0')\n",
    "# print(device)\n",
    "model.to(device)\n",
    "\n",
    "# loss = model.train_model(X_train, Y_train, n_epochs = 50, target_len = 16, batch_size = 50, training_prediction = 'mixed_teacher_forcing', teacher_forcing_ratio = 0.6, learning_rate = 0.01, dynamic_tf = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "decimal-serum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train\n",
      "Pose loaded\n",
      "Future_Pose loaded\n",
      "******************************\n",
      "Loading val\n",
      "Pose loaded\n",
      "Future_Pose loaded\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "train_loader=data_loader(args,\"train\",\"sequences_openpifpaf_thres_9_\")\n",
    "val_loader=data_loader(args,\"val\",\"sequences_openpifpaf_thres_9_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "pharmaceutical-martial",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|▏| 34/200 [07:25<36:02, 13.03s/it, ade_train=47.959, ade_val=50.899, fde_train=72.369, fde_val=77.784, loss=45.869"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    35: reducing learning rate of group 0 to 5.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|▍| 84/200 [18:18<25:09, 13.01s/it, ade_train=44.782, ade_val=48.550, fde_train=64.805, fde_val=69.816, loss=43.745"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    85: reducing learning rate of group 0 to 2.5000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|▋| 149/200 [32:40<11:40, 13.74s/it, ade_train=41.830, ade_val=45.947, fde_train=58.395, fde_val=66.760, loss=41.76"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   150: reducing learning rate of group 0 to 1.2500e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█| 200/200 [43:58<00:00, 13.19s/it, ade_train=39.712, ade_val=47.172, fde_train=55.190, fde_val=69.253, loss=40.29\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "target_len = 16\n",
    "batch_size = 60\n",
    "training_prediction = 'recursive'\n",
    "teacher_forcing_ratio = 0.4\n",
    "learning_rate = 0.01\n",
    "dynamic_tf = True\n",
    "# initialize array of losses \n",
    "losses = np.full(n_epochs, np.nan)\n",
    "losses_val = np.full(n_epochs, np.nan)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.L1Loss()\n",
    "#         criterion = nn.MSELoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10, \n",
    "                                                 threshold = 1e-12, verbose=True)\n",
    "# calculate number of batch iterations\n",
    "# n_batches = int(input_tensor.shape[1] / batch_size)\n",
    "\n",
    "with trange(n_epochs) as tr:\n",
    "    \n",
    "    for it in tr:\n",
    "        ade=0.0\n",
    "        val_ade=0.0\n",
    "        fde=0.0\n",
    "        val_fde=0.0\n",
    "        \n",
    "        batch_loss = 0.\n",
    "        batch_loss_val = 0.\n",
    "        batch_loss_tf = 0.\n",
    "        batch_loss_no_tf = 0.\n",
    "        num_tf = 0\n",
    "        num_no_tf = 0\n",
    "        counter=0\n",
    "        \n",
    "        model.train()\n",
    "        for idx, (obs_s, target_s, obs_pose, target_pose) in enumerate(train_loader):\n",
    "            obs_s = obs_s.to(device)\n",
    "            target_s = target_s.to(device)\n",
    "            obs_pose = obs_pose.to(device)\n",
    "            target_pose = target_pose.to(device)\n",
    "            \n",
    "            counter+=1\n",
    "            # select data \n",
    "            input_batch = obs_s.permute(1,0,2)\n",
    "            target_batch = target_s.permute(1,0,2)\n",
    "\n",
    "            # outputs tensor\n",
    "            outputs = torch.zeros(target_len, input_batch.shape[1], input_batch.shape[2]).to(device)\n",
    "\n",
    "            # initialize hidden state\n",
    "            encoder_hidden = model.encoder.init_hidden(batch_size)\n",
    "\n",
    "            # zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # encoder outputs\n",
    "            encoder_output, encoder_hidden = model.encoder(input_batch)\n",
    "\n",
    "            # decoder with teacher forcing\n",
    "            decoder_input = input_batch[-1, :, :]   # shape: (batch_size, input_size)\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            if training_prediction == 'recursive':\n",
    "                # predict recursively\n",
    "                for t in range(target_len): \n",
    "                    decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden)\n",
    "                    outputs[t] = decoder_output\n",
    "                    decoder_input = decoder_output\n",
    "\n",
    "            if training_prediction == 'teacher_forcing':\n",
    "                # use teacher forcing\n",
    "                if random.random() < teacher_forcing_ratio:\n",
    "                    for t in range(target_len): \n",
    "                        decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden)\n",
    "                        outputs[t] = decoder_output\n",
    "                        decoder_input = target_batch[t, :, :]\n",
    "\n",
    "                # predict recursively \n",
    "                else:\n",
    "                    for t in range(target_len): \n",
    "                        decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden)\n",
    "                        outputs[t] = decoder_output\n",
    "                        decoder_input = decoder_output\n",
    "\n",
    "            if training_prediction == 'mixed_teacher_forcing':\n",
    "                # predict using mixed teacher forcing\n",
    "                for t in range(target_len):\n",
    "                    decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden)\n",
    "                    outputs[t] = decoder_output\n",
    "\n",
    "                    # predict with teacher forcing\n",
    "                    if random.random() < teacher_forcing_ratio:\n",
    "                        decoder_input = target_batch[t, :, :]\n",
    "\n",
    "                    # predict recursively \n",
    "                    else:\n",
    "                        decoder_input = decoder_output\n",
    "\n",
    "            # compute the loss \n",
    "#             print(outputs.shape)\n",
    "#             copy=torch.tensor(outputs)\n",
    "            out_pose=speed2pos(outputs.clone().detach().permute(1,0,2),obs_pose)\n",
    "            loss = criterion(outputs, target_s.permute(1,0,2))+criterion(out_pose, target_pose)\n",
    "            \n",
    "            ade += float(ADE_c(out_pose, target_pose))\n",
    "            fde += float(FDE_c(out_pose, target_pose))\n",
    "            batch_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            \n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # loss for epoch \n",
    "        batch_loss /= counter\n",
    "        losses[it] = batch_loss\n",
    "        scheduler.step(batch_loss)\n",
    "        ade/=counter\n",
    "        fde/=counter\n",
    "\n",
    "        # dynamic teacher forcing\n",
    "        if dynamic_tf and teacher_forcing_ratio > 0:\n",
    "            teacher_forcing_ratio = teacher_forcing_ratio - 0.02 \n",
    "         \n",
    "        counter=0\n",
    "        model.eval()\n",
    "        for idx, (obs_s, target_s, obs_pose, target_pose) in enumerate(val_loader):\n",
    "            obs_s = obs_s.to(device)\n",
    "            target_s = target_s.to(device)\n",
    "            obs_pose = obs_pose.to(device)\n",
    "            target_pose = target_pose.to(device)\n",
    "            \n",
    "            counter+=1\n",
    "            # select data \n",
    "            input_batch = obs_s.permute(1,0,2)\n",
    "            target_batch = target_s.permute(1,0,2)\n",
    "\n",
    "            # outputs tensor\n",
    "            outputs = torch.zeros(target_len, input_batch.shape[1], input_batch.shape[2]).to(device)\n",
    "\n",
    "            # initialize hidden state\n",
    "            encoder_hidden = model.encoder.init_hidden(batch_size)\n",
    "\n",
    "            # encoder outputs\n",
    "            encoder_output, encoder_hidden = model.encoder(input_batch)\n",
    "\n",
    "            # decoder with teacher forcing\n",
    "            decoder_input = input_batch[-1, :, :]   # shape: (batch_size, input_size)\n",
    "            decoder_hidden = encoder_hidden\n",
    "            \n",
    "            for t in range(target_len): \n",
    "                    decoder_output, decoder_hidden = model.decoder(decoder_input, decoder_hidden)\n",
    "                    outputs[t] = decoder_output\n",
    "                    decoder_input = decoder_output\n",
    "\n",
    "            out_pose=speed2pos(outputs.clone().detach().permute(1,0,2),obs_pose)\n",
    "            loss = criterion(outputs, target_s.permute(1,0,2))+criterion(out_pose, target_pose)\n",
    "            \n",
    "            val_ade += float(ADE_c(out_pose, target_pose))\n",
    "            val_fde += float(FDE_c(out_pose, target_pose))\n",
    "            batch_loss_val += loss.item()\n",
    "            \n",
    "        \n",
    "        # loss for epoch \n",
    "        batch_loss_val /= counter\n",
    "        losses_val[it] = batch_loss_val\n",
    "\n",
    "        val_ade/=counter\n",
    "        val_fde/=counter\n",
    "        # progress bar \n",
    "        tr.set_postfix(loss=\"{0:.3f}\".format(batch_loss),val_loss=\"{0:.3f}\".format(batch_loss_val),ade_train=\"{0:.3f}\".format(ade),ade_val=\"{0:.3f}\".format(val_ade),fde_train=\"{0:.3f}\".format(fde),fde_val=\"{0:.3f}\".format(val_fde))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './seq2seq_openpose.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fifteen-pakistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 544)\n"
     ]
    }
   ],
   "source": [
    "keypoints=np.array(sequences_obs_pose)\n",
    "kp_train=np.delete(keypoints, list(range(2, keypoints.shape[2], 3)), axis=2)\n",
    "kp_train=kp_train.reshape(kp_train.shape[0],-1)\n",
    "print(kp_train.shape)\n",
    "train_dataset=PoseDataset(kp_train[:82])\n",
    "val_dataset=PoseDataset(kp_train[82:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "sporting-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class OO_LSTM_vel(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        '''\n",
    "           input: observed body poses and velocites \n",
    "           output: predicted poses(not trained), velocities of poses, and intentions\n",
    "        '''\n",
    "        super(OO_LSTM_vel, self).__init__()\n",
    "         \n",
    "        self.encoded_size=300\n",
    "        self.pose_encoder = nn.LSTM(input_size=self.encoded_size, hidden_size=args.hidden_size)\n",
    "        self.vel_encoder = nn.LSTM(input_size=self.encoded_size, hidden_size=args.hidden_size)\n",
    "        \n",
    "        self.pose_embedding = nn.Sequential(nn.Linear(in_features=args.hidden_size, out_features=self.encoded_size),\n",
    "                                           nn.ReLU())\n",
    "        \n",
    "        #self.pose_decoder = nn.LSTMCell(input_size=34, hidden_size=args.hidden_size)\n",
    "        self.vel_decoder = nn.LSTMCell(input_size=self.encoded_size, hidden_size=args.hidden_size)\n",
    "\n",
    "      \n",
    "        self.fc_vel    = nn.Linear(in_features=args.hidden_size, out_features=self.encoded_size)\n",
    "        #self.fc_pose   = nn.Linear(in_features=args.hidden_size, out_features=34)\n",
    "        \n",
    "        self.hardtanh = nn.Hardtanh(min_val=-1*args.hardtanh_limit,max_val=args.hardtanh_limit)\n",
    "        self.relu = nn.ReLU() \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.enc1= nn.Linear(in_features=34, out_features=300)\n",
    "        self.enc2 = nn.Linear(in_features=300, out_features=self.encoded_size)\n",
    "        self.encp1= nn.Linear(in_features=34, out_features=300)\n",
    "        self.encp2 = nn.Linear(in_features=300, out_features=self.encoded_size)\n",
    "        self.dec = nn.Linear(in_features=self.encoded_size, out_features=34)\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "    def forward(self, pose=None, vel=None):\n",
    "        \n",
    "        vel_encoded = torch.tensor([], device=self.args.device)\n",
    "\n",
    "        for i in range(vel.size()[0]):\n",
    "            x = self.relu(self.enc1(self.relu(vel[i])))\n",
    "            x = self.relu(self.enc2(x))\n",
    "            vel_encoded = torch.cat((vel_encoded, x.unsqueeze(1)), dim = 1)\n",
    "        \n",
    "        pose_encoded = torch.tensor([], device=self.args.device)\n",
    "\n",
    "        for i in range(pose.size()[0]):\n",
    "            x = self.relu(self.encp1(pose[i]))\n",
    "            x = self.relu(self.encp2(x))\n",
    "            pose_encoded = torch.cat((pose_encoded, x.unsqueeze(1)), dim = 1)\n",
    "            \n",
    "        \n",
    "        _, (hidden_vel, cell_vel) = self.vel_encoder(vel_encoded)\n",
    "        hidden_vel = hidden_vel.squeeze(0)\n",
    "        cell_vel = cell_vel.squeeze(0)\n",
    "\n",
    "\n",
    "        _, (hidden_pose, cell_pose) = self.pose_encoder(pose_encoded)\n",
    "        hidden_pose = hidden_pose.squeeze(0)\n",
    "        cell_pose = cell_pose.squeeze(0)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        vel_outputs    = torch.tensor([], device=self.args.device)\n",
    "        #pose_outputs   = torch.tensor([], device=self.args.device)\n",
    "#         print(pose.shape)\n",
    "#         print(vel.shape)\n",
    "#         print(pose_encoded.shape)\n",
    "#         print(vel_encoded.shape)\n",
    "#         print(vel_encoded.shape)\n",
    "        vel_encoded=vel_encoded.permute(1,0,2)\n",
    "        pose_encoded=pose_encoded.permute(1,0,2)\n",
    "        VelDec_inp = vel_encoded[:,-1,:]\n",
    "#         print(VelDec_inp.shape)\n",
    "      \n",
    "        hidden_dec = hidden_vel+hidden_pose \n",
    "        cell_dec = cell_vel+cell_pose \n",
    "        \n",
    "#         print(hidden_dec.shape)\n",
    "        for i in range(self.args.output//self.args.skip):\n",
    "            hidden_dec, cell_dec = self.vel_decoder(VelDec_inp, (hidden_dec, cell_dec))\n",
    "#             print(hidden_dec.shape)\n",
    "            vel_output_encoded  = self.hardtanh(self.fc_vel(hidden_dec))\n",
    "            vel_output = self.relu(self.dec(vel_output_encoded))\n",
    "            vel_outputs = torch.cat((vel_outputs, vel_output.unsqueeze(1)), dim = 1)\n",
    "            VelDec_inp  = vel_output_encoded.detach()\n",
    "            \n",
    "        outputs.append(vel_outputs)     \n",
    "        \n",
    "        #         if random.random() < self.teacher_forcing_ratio:\n",
    "#             for i in range(self.args.output//self.args.skip):\n",
    "#                 hidden_dec, cell_dec = self.pose_decoder(PoseDec_inp, (hidden_dec, cell_dec))\n",
    "#                 pose_output_encoded  = self.fc_pose(hidden_dec)\n",
    "#                 pose_output= self.relu(self.dec(pose_output_encoded))\n",
    "#                 pose_outputs = torch.cat((pose_outputs, pose_output.unsqueeze(1)), dim = 1)\n",
    "#                 PoseDec_inp  = target_pose_encoded[i,:,:]\n",
    "\n",
    "#                 # predict recursively \n",
    "#         else:\n",
    "#             for i in range(self.args.output//self.args.skip):\n",
    "#                 hidden_dec, cell_dec = self.pose_decoder(PoseDec_inp, (hidden_dec, cell_dec))\n",
    "#                 pose_output_encoded  = self.fc_pose(hidden_dec)\n",
    "#                 pose_output= self.relu(self.dec(pose_output_encoded))\n",
    "#                 pose_outputs = torch.cat((pose_outputs, pose_output.unsqueeze(1)), dim = 1)\n",
    "#                 PoseDec_inp  = pose_output_encoded.detach()\n",
    "        #outputs.append(pose_outputs)\n",
    "\n",
    "            \n",
    "        return tuple(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "hungry-punishment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train\n",
      "Pose loaded\n",
      "Future_Pose loaded\n",
      "******************************\n",
      "Loading val\n",
      "Pose loaded\n",
      "Future_Pose loaded\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "class args():\n",
    "    def __init__(self):\n",
    "        self.loader_workers = 1\n",
    "        self.loader_shuffle = True\n",
    "        self.pin_memory     = False\n",
    "        self.device         = 'cuda'\n",
    "        self.batch_size     = 50\n",
    "        self.n_epochs       = 1000\n",
    "        self.hidden_size    = 1000\n",
    "        self.hardtanh_limit = 100\n",
    "        self.input  = 16\n",
    "        self.output = 16\n",
    "        self.stride = 16\n",
    "        self.skip   = 1\n",
    "        self.lr = 0.01\n",
    "args = args()\n",
    "\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "l1e = nn.L1Loss()\n",
    "train_s_scores = []\n",
    "train_pose_scores=[]\n",
    "val_pose_scores=[]\n",
    "train_c_scores = []\n",
    "val_s_scores   = []\n",
    "val_c_scores   = []\n",
    "\n",
    "train_loader=data_loader(args,\"train\",\"sequences_openpifpaf_\")\n",
    "val_loader=data_loader(args,\"val\",\"sequences_openpifpaf_\")\n",
    "# train_loader=data_loader(args,\"train\",\"sequences_openpifpaf_single_\")\n",
    "# val_loader=data_loader(args,\"val\",\"sequences_openpifpaf_single_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-smoke",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eleven-academy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Training ...\n",
      "e: 0 | loss: 5.63 | val_loss: 35.79 | tpose: 5.63 | vpose: 35.79 | ade_train: 83.66 | ade_val: 460.74 | fde_train: 152.59 | fde_val: 836.74 | t:0.49\n",
      "e: 1 | loss: 14.10 | val_loss: 3.12 | tpose: 14.10 | vpose: 3.12 | ade_train: 181.60 | ade_val: 40.84 | fde_train: 333.20 | fde_val: 77.04 | t:0.49\n",
      "e: 2 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.05 | ade_val: 40.84 | fde_train: 58.64 | fde_val: 77.04 | t:0.48\n",
      "e: 3 | loss: 2.53 | val_loss: 3.12 | tpose: 2.53 | vpose: 3.12 | ade_train: 30.99 | ade_val: 40.84 | fde_train: 58.49 | fde_val: 77.04 | t:0.47\n",
      "e: 4 | loss: 2.55 | val_loss: 3.17 | tpose: 2.55 | vpose: 3.17 | ade_train: 31.22 | ade_val: 41.68 | fde_train: 58.95 | fde_val: 77.67 | t:0.48\n",
      "e: 5 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.33 | ade_val: 40.84 | fde_train: 58.84 | fde_val: 77.04 | t:0.49\n",
      "e: 6 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.13 | ade_val: 40.84 | fde_train: 58.78 | fde_val: 77.04 | t:0.48\n",
      "e: 7 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.05 | ade_val: 40.84 | fde_train: 58.61 | fde_val: 77.04 | t:0.47\n",
      "e: 8 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.19 | ade_val: 40.84 | fde_train: 58.91 | fde_val: 77.04 | t:0.47\n",
      "e: 9 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.10 | ade_val: 40.84 | fde_train: 58.72 | fde_val: 77.04 | t:0.48\n",
      "e: 10 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.12 | ade_val: 40.84 | fde_train: 58.73 | fde_val: 77.04 | t:0.47\n",
      "e: 11 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.10 | ade_val: 40.84 | fde_train: 58.70 | fde_val: 77.04 | t:0.48\n",
      "Epoch    13: reducing learning rate of group 0 to 5.0000e-02.\n",
      "e: 12 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.26 | ade_val: 40.84 | fde_train: 59.07 | fde_val: 77.04 | t:0.49\n",
      "e: 13 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.19 | ade_val: 40.84 | fde_train: 58.89 | fde_val: 77.04 | t:0.57\n",
      "e: 14 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.18 | ade_val: 40.84 | fde_train: 58.87 | fde_val: 77.04 | t:0.47\n",
      "e: 15 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.16 | ade_val: 40.84 | fde_train: 58.87 | fde_val: 77.04 | t:0.52\n",
      "e: 16 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.17 | ade_val: 40.84 | fde_train: 58.85 | fde_val: 77.04 | t:0.47\n",
      "e: 17 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.12 | ade_val: 40.84 | fde_train: 58.74 | fde_val: 77.04 | t:0.48\n",
      "e: 18 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.06 | ade_val: 40.84 | fde_train: 58.61 | fde_val: 77.04 | t:0.48\n",
      "e: 19 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.04 | ade_val: 40.84 | fde_train: 58.58 | fde_val: 77.04 | t:0.48\n",
      "e: 20 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.18 | ade_val: 40.84 | fde_train: 58.89 | fde_val: 77.04 | t:0.47\n",
      "Epoch    22: reducing learning rate of group 0 to 2.5000e-02.\n",
      "e: 21 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.15 | ade_val: 40.84 | fde_train: 58.83 | fde_val: 77.04 | t:0.48\n",
      "e: 22 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.01 | ade_val: 40.84 | fde_train: 58.54 | fde_val: 77.04 | t:0.48\n",
      "e: 23 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.09 | ade_val: 40.84 | fde_train: 58.67 | fde_val: 77.04 | t:0.47\n",
      "e: 24 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.11 | ade_val: 40.84 | fde_train: 58.76 | fde_val: 77.04 | t:0.47\n",
      "e: 25 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.15 | ade_val: 40.84 | fde_train: 58.83 | fde_val: 77.04 | t:0.48\n",
      "e: 26 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.04 | ade_val: 40.84 | fde_train: 58.58 | fde_val: 77.04 | t:0.48\n",
      "e: 27 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.17 | ade_val: 40.84 | fde_train: 58.86 | fde_val: 77.04 | t:0.48\n",
      "e: 28 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.21 | ade_val: 40.84 | fde_train: 58.92 | fde_val: 77.04 | t:0.48\n",
      "e: 29 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.23 | ade_val: 40.84 | fde_train: 58.97 | fde_val: 77.04 | t:0.47\n",
      "Epoch    31: reducing learning rate of group 0 to 1.2500e-02.\n",
      "e: 30 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.17 | ade_val: 40.84 | fde_train: 58.86 | fde_val: 77.04 | t:0.48\n",
      "e: 31 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.25 | ade_val: 40.84 | fde_train: 58.99 | fde_val: 77.04 | t:0.48\n",
      "e: 32 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.27 | ade_val: 40.84 | fde_train: 59.07 | fde_val: 77.04 | t:0.48\n",
      "e: 33 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.13 | ade_val: 40.84 | fde_train: 58.76 | fde_val: 77.04 | t:0.49\n",
      "e: 34 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.19 | ade_val: 40.84 | fde_train: 58.93 | fde_val: 77.04 | t:0.49\n",
      "e: 35 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.12 | ade_val: 40.84 | fde_train: 58.77 | fde_val: 77.04 | t:0.48\n",
      "e: 36 | loss: 2.53 | val_loss: 3.12 | tpose: 2.53 | vpose: 3.12 | ade_train: 30.99 | ade_val: 40.84 | fde_train: 58.48 | fde_val: 77.04 | t:0.47\n",
      "e: 37 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.15 | ade_val: 40.84 | fde_train: 58.80 | fde_val: 77.04 | t:0.46\n",
      "e: 38 | loss: 2.53 | val_loss: 3.12 | tpose: 2.53 | vpose: 3.12 | ade_train: 31.01 | ade_val: 40.84 | fde_train: 58.53 | fde_val: 77.04 | t:0.48\n",
      "Epoch    40: reducing learning rate of group 0 to 6.2500e-03.\n",
      "e: 39 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.16 | ade_val: 40.84 | fde_train: 58.86 | fde_val: 77.04 | t:0.51\n",
      "e: 40 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.04 | ade_val: 40.84 | fde_train: 58.58 | fde_val: 77.04 | t:0.48\n",
      "e: 41 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.12 | ade_val: 40.84 | fde_train: 58.75 | fde_val: 77.04 | t:0.47\n",
      "e: 42 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.13 | ade_val: 40.84 | fde_train: 58.77 | fde_val: 77.04 | t:0.48\n",
      "e: 43 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.05 | ade_val: 40.84 | fde_train: 58.62 | fde_val: 77.04 | t:0.48\n",
      "e: 44 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.12 | ade_val: 40.84 | fde_train: 58.74 | fde_val: 77.04 | t:0.48\n",
      "e: 45 | loss: 2.53 | val_loss: 3.12 | tpose: 2.53 | vpose: 3.12 | ade_train: 30.98 | ade_val: 40.84 | fde_train: 58.48 | fde_val: 77.04 | t:0.48\n",
      "e: 46 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.20 | ade_val: 40.84 | fde_train: 58.93 | fde_val: 77.04 | t:0.48\n",
      "e: 47 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.14 | ade_val: 40.84 | fde_train: 58.79 | fde_val: 77.04 | t:0.47\n",
      "e: 48 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.07 | ade_val: 40.84 | fde_train: 58.67 | fde_val: 77.04 | t:0.47\n",
      "e: 49 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.07 | ade_val: 40.84 | fde_train: 58.66 | fde_val: 77.04 | t:0.48\n",
      "e: 50 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.28 | ade_val: 40.84 | fde_train: 59.06 | fde_val: 77.04 | t:0.47\n",
      "e: 51 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.03 | ade_val: 40.84 | fde_train: 58.57 | fde_val: 77.04 | t:0.48\n",
      "e: 52 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.10 | ade_val: 40.84 | fde_train: 58.72 | fde_val: 77.04 | t:0.49\n",
      "e: 53 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.13 | ade_val: 40.84 | fde_train: 58.77 | fde_val: 77.04 | t:0.48\n",
      "Epoch    55: reducing learning rate of group 0 to 3.1250e-03.\n",
      "e: 54 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.11 | ade_val: 40.84 | fde_train: 58.72 | fde_val: 77.04 | t:0.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 55 | loss: 2.53 | val_loss: 3.12 | tpose: 2.53 | vpose: 3.12 | ade_train: 31.00 | ade_val: 40.84 | fde_train: 58.49 | fde_val: 77.04 | t:0.49\n",
      "e: 56 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.19 | ade_val: 40.84 | fde_train: 58.90 | fde_val: 77.04 | t:0.48\n",
      "e: 57 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.27 | ade_val: 40.84 | fde_train: 59.08 | fde_val: 77.04 | t:0.49\n",
      "e: 58 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.07 | ade_val: 40.84 | fde_train: 58.67 | fde_val: 77.04 | t:0.47\n",
      "e: 59 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.08 | ade_val: 40.84 | fde_train: 58.66 | fde_val: 77.04 | t:0.48\n",
      "e: 60 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.14 | ade_val: 40.84 | fde_train: 58.79 | fde_val: 77.04 | t:0.47\n",
      "e: 61 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.29 | ade_val: 40.84 | fde_train: 59.11 | fde_val: 77.04 | t:0.48\n",
      "e: 62 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.28 | ade_val: 40.84 | fde_train: 59.09 | fde_val: 77.04 | t:0.47\n",
      "Epoch    64: reducing learning rate of group 0 to 1.5625e-03.\n",
      "e: 63 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.20 | ade_val: 40.84 | fde_train: 58.91 | fde_val: 77.04 | t:0.48\n",
      "e: 64 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.10 | ade_val: 40.84 | fde_train: 58.70 | fde_val: 77.04 | t:0.48\n",
      "e: 65 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.09 | ade_val: 40.84 | fde_train: 58.69 | fde_val: 77.04 | t:0.48\n",
      "e: 66 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.04 | ade_val: 40.84 | fde_train: 58.58 | fde_val: 77.04 | t:0.47\n",
      "e: 67 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.07 | ade_val: 40.84 | fde_train: 58.66 | fde_val: 77.04 | t:0.47\n",
      "e: 68 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.06 | ade_val: 40.84 | fde_train: 58.65 | fde_val: 77.04 | t:0.47\n",
      "e: 69 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.12 | ade_val: 40.84 | fde_train: 58.78 | fde_val: 77.04 | t:0.47\n",
      "e: 70 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.18 | ade_val: 40.84 | fde_train: 58.87 | fde_val: 77.04 | t:0.49\n",
      "e: 71 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.19 | ade_val: 40.84 | fde_train: 58.91 | fde_val: 77.04 | t:0.47\n",
      "Epoch    73: reducing learning rate of group 0 to 7.8125e-04.\n",
      "e: 72 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.09 | ade_val: 40.84 | fde_train: 58.70 | fde_val: 77.04 | t:0.47\n",
      "e: 73 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.04 | ade_val: 40.84 | fde_train: 58.59 | fde_val: 77.04 | t:0.47\n",
      "e: 74 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.22 | ade_val: 40.84 | fde_train: 58.96 | fde_val: 77.04 | t:0.48\n",
      "e: 75 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.08 | ade_val: 40.84 | fde_train: 58.66 | fde_val: 77.04 | t:0.47\n",
      "e: 76 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.22 | ade_val: 40.84 | fde_train: 58.98 | fde_val: 77.04 | t:0.48\n",
      "e: 77 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.07 | ade_val: 40.84 | fde_train: 58.68 | fde_val: 77.04 | t:0.50\n",
      "e: 78 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.09 | ade_val: 40.84 | fde_train: 58.73 | fde_val: 77.04 | t:0.50\n",
      "e: 79 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.16 | ade_val: 40.84 | fde_train: 58.82 | fde_val: 77.04 | t:0.48\n",
      "e: 80 | loss: 2.53 | val_loss: 3.12 | tpose: 2.53 | vpose: 3.12 | ade_train: 30.97 | ade_val: 40.84 | fde_train: 58.44 | fde_val: 77.04 | t:0.48\n",
      "e: 81 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.09 | ade_val: 40.84 | fde_train: 58.70 | fde_val: 77.04 | t:0.49\n",
      "e: 82 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.08 | ade_val: 40.84 | fde_train: 58.70 | fde_val: 77.04 | t:0.47\n",
      "e: 83 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.25 | ade_val: 40.84 | fde_train: 59.04 | fde_val: 77.04 | t:0.46\n",
      "e: 84 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.15 | ade_val: 40.84 | fde_train: 58.77 | fde_val: 77.04 | t:0.48\n",
      "e: 85 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.18 | ade_val: 40.84 | fde_train: 58.87 | fde_val: 77.04 | t:0.53\n",
      "e: 86 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.07 | ade_val: 40.84 | fde_train: 58.64 | fde_val: 77.04 | t:0.48\n",
      "e: 87 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.07 | ade_val: 40.84 | fde_train: 58.68 | fde_val: 77.04 | t:0.50\n",
      "e: 88 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.19 | ade_val: 40.84 | fde_train: 58.92 | fde_val: 77.04 | t:0.47\n",
      "Epoch    90: reducing learning rate of group 0 to 3.9063e-04.\n",
      "e: 89 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.03 | ade_val: 40.84 | fde_train: 58.56 | fde_val: 77.04 | t:0.49\n",
      "e: 90 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.04 | ade_val: 40.84 | fde_train: 58.61 | fde_val: 77.04 | t:0.48\n",
      "e: 91 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.08 | ade_val: 40.84 | fde_train: 58.66 | fde_val: 77.04 | t:0.47\n",
      "e: 92 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.18 | ade_val: 40.84 | fde_train: 58.92 | fde_val: 77.04 | t:0.48\n",
      "e: 93 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.12 | ade_val: 40.84 | fde_train: 58.76 | fde_val: 77.04 | t:0.48\n",
      "e: 94 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.28 | ade_val: 40.84 | fde_train: 59.08 | fde_val: 77.04 | t:0.48\n",
      "e: 95 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.20 | ade_val: 40.84 | fde_train: 58.92 | fde_val: 77.04 | t:0.48\n",
      "e: 96 | loss: 2.53 | val_loss: 3.12 | tpose: 2.53 | vpose: 3.12 | ade_train: 30.96 | ade_val: 40.84 | fde_train: 58.43 | fde_val: 77.04 | t:0.48\n",
      "e: 97 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.18 | ade_val: 40.84 | fde_train: 58.85 | fde_val: 77.04 | t:0.48\n",
      "Epoch    99: reducing learning rate of group 0 to 1.9531e-04.\n",
      "e: 98 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.08 | ade_val: 40.84 | fde_train: 58.68 | fde_val: 77.04 | t:0.48\n",
      "e: 99 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.23 | ade_val: 40.84 | fde_train: 59.00 | fde_val: 77.04 | t:0.48\n",
      "e: 100 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.05 | ade_val: 40.84 | fde_train: 58.59 | fde_val: 77.04 | t:0.48\n",
      "e: 101 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.11 | ade_val: 40.84 | fde_train: 58.71 | fde_val: 77.04 | t:0.48\n",
      "e: 102 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.24 | ade_val: 40.84 | fde_train: 58.98 | fde_val: 77.04 | t:0.48\n",
      "e: 103 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.07 | ade_val: 40.84 | fde_train: 58.66 | fde_val: 77.04 | t:0.50\n",
      "e: 104 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.06 | ade_val: 40.84 | fde_train: 58.61 | fde_val: 77.04 | t:0.49\n",
      "e: 105 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.06 | ade_val: 40.84 | fde_train: 58.65 | fde_val: 77.04 | t:0.47\n",
      "e: 106 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.13 | ade_val: 40.84 | fde_train: 58.77 | fde_val: 77.04 | t:0.48\n",
      "Epoch   108: reducing learning rate of group 0 to 9.7656e-05.\n",
      "e: 107 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.17 | ade_val: 40.84 | fde_train: 58.85 | fde_val: 77.04 | t:0.48\n",
      "e: 108 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.23 | ade_val: 40.84 | fde_train: 58.98 | fde_val: 77.04 | t:0.47\n",
      "e: 109 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.08 | ade_val: 40.84 | fde_train: 58.69 | fde_val: 77.04 | t:0.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 110 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.05 | ade_val: 40.84 | fde_train: 58.61 | fde_val: 77.04 | t:0.47\n",
      "e: 111 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.16 | ade_val: 40.84 | fde_train: 58.83 | fde_val: 77.04 | t:0.48\n",
      "e: 112 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.30 | ade_val: 40.84 | fde_train: 59.15 | fde_val: 77.04 | t:0.48\n",
      "e: 113 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.03 | ade_val: 40.84 | fde_train: 58.58 | fde_val: 77.04 | t:0.47\n",
      "e: 114 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.04 | ade_val: 40.84 | fde_train: 58.57 | fde_val: 77.04 | t:0.47\n",
      "e: 115 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.03 | ade_val: 40.84 | fde_train: 58.59 | fde_val: 77.04 | t:0.48\n",
      "Epoch   117: reducing learning rate of group 0 to 4.8828e-05.\n",
      "e: 116 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.07 | ade_val: 40.84 | fde_train: 58.67 | fde_val: 77.04 | t:0.47\n",
      "e: 117 | loss: 2.53 | val_loss: 3.12 | tpose: 2.53 | vpose: 3.12 | ade_train: 31.00 | ade_val: 40.84 | fde_train: 58.51 | fde_val: 77.04 | t:0.54\n",
      "e: 118 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.22 | ade_val: 40.84 | fde_train: 58.95 | fde_val: 77.04 | t:0.47\n",
      "e: 119 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.15 | ade_val: 40.84 | fde_train: 58.81 | fde_val: 77.04 | t:0.47\n",
      "e: 120 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.12 | ade_val: 40.84 | fde_train: 58.75 | fde_val: 77.04 | t:0.47\n",
      "e: 121 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.08 | ade_val: 40.84 | fde_train: 58.66 | fde_val: 77.04 | t:0.47\n",
      "e: 122 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.03 | ade_val: 40.84 | fde_train: 58.57 | fde_val: 77.04 | t:0.47\n",
      "e: 123 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.07 | ade_val: 40.84 | fde_train: 58.68 | fde_val: 77.04 | t:0.47\n",
      "e: 124 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.02 | ade_val: 40.84 | fde_train: 58.53 | fde_val: 77.04 | t:0.48\n",
      "Epoch   126: reducing learning rate of group 0 to 2.4414e-05.\n",
      "e: 125 | loss: 2.53 | val_loss: 3.12 | tpose: 2.53 | vpose: 3.12 | ade_train: 30.98 | ade_val: 40.84 | fde_train: 58.46 | fde_val: 77.04 | t:0.50\n",
      "e: 126 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.10 | ade_val: 40.84 | fde_train: 58.67 | fde_val: 77.04 | t:0.47\n",
      "e: 127 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.18 | ade_val: 40.84 | fde_train: 58.87 | fde_val: 77.04 | t:0.47\n",
      "e: 128 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.11 | ade_val: 40.84 | fde_train: 58.74 | fde_val: 77.04 | t:0.48\n",
      "e: 129 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.05 | ade_val: 40.84 | fde_train: 58.62 | fde_val: 77.04 | t:0.47\n",
      "e: 130 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.06 | ade_val: 40.84 | fde_train: 58.60 | fde_val: 77.04 | t:0.48\n",
      "e: 131 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.16 | ade_val: 40.84 | fde_train: 58.86 | fde_val: 77.04 | t:0.48\n",
      "e: 132 | loss: 2.53 | val_loss: 3.12 | tpose: 2.53 | vpose: 3.12 | ade_train: 30.99 | ade_val: 40.84 | fde_train: 58.49 | fde_val: 77.04 | t:0.51\n",
      "e: 133 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.26 | ade_val: 40.84 | fde_train: 59.03 | fde_val: 77.04 | t:0.48\n",
      "Epoch   135: reducing learning rate of group 0 to 1.2207e-05.\n",
      "e: 134 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.08 | ade_val: 40.84 | fde_train: 58.70 | fde_val: 77.04 | t:0.47\n",
      "e: 135 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.22 | ade_val: 40.84 | fde_train: 58.96 | fde_val: 77.04 | t:0.48\n",
      "e: 136 | loss: 2.53 | val_loss: 3.12 | tpose: 2.53 | vpose: 3.12 | ade_train: 30.99 | ade_val: 40.84 | fde_train: 58.47 | fde_val: 77.04 | t:0.49\n",
      "e: 137 | loss: 2.53 | val_loss: 3.12 | tpose: 2.53 | vpose: 3.12 | ade_train: 31.00 | ade_val: 40.84 | fde_train: 58.52 | fde_val: 77.04 | t:0.47\n",
      "e: 138 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.04 | ade_val: 40.84 | fde_train: 58.60 | fde_val: 77.04 | t:0.47\n",
      "e: 139 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.06 | ade_val: 40.84 | fde_train: 58.64 | fde_val: 77.04 | t:0.51\n",
      "e: 140 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.20 | ade_val: 40.84 | fde_train: 58.92 | fde_val: 77.04 | t:0.47\n",
      "e: 141 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.12 | ade_val: 40.84 | fde_train: 58.78 | fde_val: 77.04 | t:0.47\n",
      "e: 142 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.16 | ade_val: 40.84 | fde_train: 58.85 | fde_val: 77.04 | t:0.50\n",
      "Epoch   144: reducing learning rate of group 0 to 6.1035e-06.\n",
      "e: 143 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.19 | ade_val: 40.84 | fde_train: 58.92 | fde_val: 77.04 | t:0.52\n",
      "e: 144 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.04 | ade_val: 40.84 | fde_train: 58.58 | fde_val: 77.04 | t:0.50\n",
      "e: 145 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.30 | ade_val: 40.84 | fde_train: 59.11 | fde_val: 77.04 | t:0.47\n",
      "e: 146 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.11 | ade_val: 40.84 | fde_train: 58.74 | fde_val: 77.04 | t:0.47\n",
      "e: 147 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.16 | ade_val: 40.84 | fde_train: 58.84 | fde_val: 77.04 | t:0.47\n",
      "e: 148 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.10 | ade_val: 40.84 | fde_train: 58.72 | fde_val: 77.04 | t:0.47\n",
      "e: 149 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.08 | ade_val: 40.84 | fde_train: 58.68 | fde_val: 77.04 | t:0.47\n",
      "e: 150 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.06 | ade_val: 40.84 | fde_train: 58.64 | fde_val: 77.04 | t:0.47\n",
      "e: 151 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.24 | ade_val: 40.84 | fde_train: 59.02 | fde_val: 77.04 | t:0.47\n",
      "Epoch   153: reducing learning rate of group 0 to 3.0518e-06.\n",
      "e: 152 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.12 | ade_val: 40.84 | fde_train: 58.79 | fde_val: 77.04 | t:0.47\n",
      "e: 153 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.09 | ade_val: 40.84 | fde_train: 58.69 | fde_val: 77.04 | t:0.47\n",
      "e: 154 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.17 | ade_val: 40.84 | fde_train: 58.88 | fde_val: 77.04 | t:0.50\n",
      "e: 155 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.12 | ade_val: 40.84 | fde_train: 58.76 | fde_val: 77.04 | t:0.47\n",
      "e: 156 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.13 | ade_val: 40.84 | fde_train: 58.76 | fde_val: 77.04 | t:0.47\n",
      "e: 157 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.02 | ade_val: 40.84 | fde_train: 58.54 | fde_val: 77.04 | t:0.47\n",
      "e: 158 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.06 | ade_val: 40.84 | fde_train: 58.62 | fde_val: 77.04 | t:0.47\n",
      "e: 159 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.06 | ade_val: 40.84 | fde_train: 58.62 | fde_val: 77.04 | t:0.47\n",
      "e: 160 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.22 | ade_val: 40.84 | fde_train: 58.94 | fde_val: 77.04 | t:0.47\n",
      "Epoch   162: reducing learning rate of group 0 to 1.5259e-06.\n",
      "e: 161 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.26 | ade_val: 40.84 | fde_train: 59.02 | fde_val: 77.04 | t:0.49\n",
      "e: 162 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.14 | ade_val: 40.84 | fde_train: 58.80 | fde_val: 77.04 | t:0.47\n",
      "e: 163 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.01 | ade_val: 40.84 | fde_train: 58.52 | fde_val: 77.04 | t:0.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 164 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.20 | ade_val: 40.84 | fde_train: 58.93 | fde_val: 77.04 | t:0.48\n",
      "e: 165 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.04 | ade_val: 40.84 | fde_train: 58.57 | fde_val: 77.04 | t:0.48\n",
      "e: 166 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.06 | ade_val: 40.84 | fde_train: 58.64 | fde_val: 77.04 | t:0.47\n",
      "e: 167 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.12 | ade_val: 40.84 | fde_train: 58.75 | fde_val: 77.04 | t:0.48\n",
      "e: 168 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.24 | ade_val: 40.84 | fde_train: 59.02 | fde_val: 77.04 | t:0.48\n",
      "e: 169 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.13 | ade_val: 40.84 | fde_train: 58.77 | fde_val: 77.04 | t:0.48\n",
      "Epoch   171: reducing learning rate of group 0 to 7.6294e-07.\n",
      "e: 170 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.10 | ade_val: 40.84 | fde_train: 58.71 | fde_val: 77.04 | t:0.52\n",
      "e: 171 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.28 | ade_val: 40.84 | fde_train: 59.07 | fde_val: 77.04 | t:0.48\n",
      "e: 172 | loss: 2.53 | val_loss: 3.12 | tpose: 2.53 | vpose: 3.12 | ade_train: 30.96 | ade_val: 40.84 | fde_train: 58.45 | fde_val: 77.04 | t:0.48\n",
      "e: 173 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.15 | ade_val: 40.84 | fde_train: 58.81 | fde_val: 77.04 | t:0.51\n",
      "e: 174 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.04 | ade_val: 40.84 | fde_train: 58.58 | fde_val: 77.04 | t:0.47\n",
      "e: 175 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.29 | ade_val: 40.84 | fde_train: 59.09 | fde_val: 77.04 | t:0.48\n",
      "e: 176 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.14 | ade_val: 40.84 | fde_train: 58.79 | fde_val: 77.04 | t:0.48\n",
      "e: 177 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.09 | ade_val: 40.84 | fde_train: 58.69 | fde_val: 77.04 | t:0.57\n",
      "e: 178 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.07 | ade_val: 40.84 | fde_train: 58.64 | fde_val: 77.04 | t:0.47\n",
      "Epoch   180: reducing learning rate of group 0 to 3.8147e-07.\n",
      "e: 179 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.16 | ade_val: 40.84 | fde_train: 58.80 | fde_val: 77.04 | t:0.49\n",
      "e: 180 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.19 | ade_val: 40.84 | fde_train: 58.92 | fde_val: 77.04 | t:0.48\n",
      "e: 181 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.21 | ade_val: 40.84 | fde_train: 58.93 | fde_val: 77.04 | t:0.49\n",
      "e: 182 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.08 | ade_val: 40.84 | fde_train: 58.69 | fde_val: 77.04 | t:0.47\n",
      "e: 183 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.18 | ade_val: 40.84 | fde_train: 58.88 | fde_val: 77.04 | t:0.48\n",
      "e: 184 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.11 | ade_val: 40.84 | fde_train: 58.75 | fde_val: 77.04 | t:0.47\n",
      "e: 185 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.00 | ade_val: 40.84 | fde_train: 58.51 | fde_val: 77.04 | t:0.48\n",
      "e: 186 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.25 | ade_val: 40.84 | fde_train: 59.01 | fde_val: 77.04 | t:0.47\n",
      "e: 187 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.10 | ade_val: 40.84 | fde_train: 58.73 | fde_val: 77.04 | t:0.57\n",
      "Epoch   189: reducing learning rate of group 0 to 1.9073e-07.\n",
      "e: 188 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.24 | ade_val: 40.84 | fde_train: 58.97 | fde_val: 77.04 | t:0.47\n",
      "e: 189 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.08 | ade_val: 40.84 | fde_train: 58.65 | fde_val: 77.04 | t:0.47\n",
      "e: 190 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.05 | ade_val: 40.84 | fde_train: 58.61 | fde_val: 77.04 | t:0.47\n",
      "e: 191 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.20 | ade_val: 40.84 | fde_train: 58.90 | fde_val: 77.04 | t:0.48\n",
      "e: 192 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.09 | ade_val: 40.84 | fde_train: 58.70 | fde_val: 77.04 | t:0.47\n",
      "e: 193 | loss: 2.55 | val_loss: 3.12 | tpose: 2.55 | vpose: 3.12 | ade_train: 31.20 | ade_val: 40.84 | fde_train: 58.90 | fde_val: 77.04 | t:0.47\n",
      "e: 194 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.05 | ade_val: 40.84 | fde_train: 58.59 | fde_val: 77.04 | t:0.47\n",
      "e: 195 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.03 | ade_val: 40.84 | fde_train: 58.57 | fde_val: 77.04 | t:0.47\n",
      "e: 196 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.11 | ade_val: 40.84 | fde_train: 58.72 | fde_val: 77.04 | t:0.47\n",
      "Epoch   198: reducing learning rate of group 0 to 9.5367e-08.\n",
      "e: 197 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.02 | ade_val: 40.84 | fde_train: 58.58 | fde_val: 77.04 | t:0.47\n",
      "e: 198 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.10 | ade_val: 40.84 | fde_train: 58.71 | fde_val: 77.04 | t:0.48\n",
      "e: 199 | loss: 2.54 | val_loss: 3.12 | tpose: 2.54 | vpose: 3.12 | ade_train: 31.06 | ade_val: 40.84 | fde_train: 58.62 | fde_val: 77.04 | t:0.47\n",
      "====================================================================================================\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "net = OO_LSTM_vel(args).to(args.device)\n",
    "optimizer = optim.Adam(net.parameters(), lr= 0.1)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=8, \n",
    "                                                 threshold = 1e-12, verbose=True)\n",
    "print('='*100)\n",
    "print('Training ...')\n",
    "\n",
    "train_s_scores=[]\n",
    "val_s_scores=[]\n",
    "\n",
    "for epoch in range(200):\n",
    "    start = time.time()\n",
    "    \n",
    "    avg_epoch_train_s_loss   = 0\n",
    "    avg_epoch_val_s_loss     = 0 \n",
    "    ade  = 0\n",
    "    fde  = 0\n",
    "    ade_train  = 0\n",
    "    fde_train  = 0\n",
    "    counter = 0\n",
    "    net.train()\n",
    "    for idx, (obs_s, target_s, obs_pose, target_pose) in enumerate(train_loader):\n",
    "        counter += 1        \n",
    "        \n",
    "        \n",
    "        obs_pose    = obs_pose.to(device='cuda')\n",
    "        target_pose = target_pose.to(device='cuda')\n",
    "        obs_s    = obs_s.to(device='cuda')\n",
    "        target_s = target_s.to(device='cuda')\n",
    " \n",
    "        net.zero_grad()\n",
    "    \n",
    "        (vel_preds,) = net(pose=obs_pose, vel=obs_s)\n",
    "\n",
    "        loss  = l1e(vel_preds, target_s)\n",
    "        pose_preds=speed2pos(vel_preds,obs_pose)\n",
    "    \n",
    "        ade_train += float(ADE_c(pose_preds, target_pose))\n",
    "        fde_train += float(FDE_c(pose_preds, target_pose))\n",
    "        \n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "        avg_epoch_train_s_loss += float(loss)\n",
    "\n",
    "    avg_epoch_train_s_loss /= counter\n",
    "    train_s_scores.append(avg_epoch_train_s_loss)\n",
    "    ade_train  /= counter\n",
    "    fde_train  /= counter   \n",
    "    \n",
    "    \n",
    "  \n",
    "    counter=0\n",
    "    net.eval()\n",
    "    for idx, (obs_s, target_s, obs_pose, target_pose) in enumerate(val_loader):\n",
    "        counter+=1\n",
    "       \n",
    "        obs_pose    = obs_pose.to(device='cuda')\n",
    "        target_pose = target_pose.to(device='cuda')\n",
    "        obs_s    = obs_s.to(device='cuda')\n",
    "        target_s = target_s.to(device='cuda')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            (vel_preds,) = net(pose=obs_pose, vel=obs_s)\n",
    "\n",
    "            val_loss  = l1e(vel_preds, target_s)\n",
    "            pose_preds=speed2pos(vel_preds,obs_pose)\n",
    "            \n",
    "            avg_epoch_val_s_loss += float(val_loss)\n",
    "            \n",
    "            ade += float(ADE_c(pose_preds, target_pose))\n",
    "            fde += float(FDE_c(pose_preds, target_pose))\n",
    "\n",
    "        \n",
    "    avg_epoch_val_s_loss /= counter\n",
    "    val_s_scores.append(avg_epoch_val_s_loss)\n",
    "    \n",
    "    ade  /= counter\n",
    "    fde  /= counter     \n",
    "   \n",
    "    \n",
    "    \n",
    "    scheduler.step(avg_epoch_train_s_loss)\n",
    "    \n",
    "    print('e:', epoch,'| loss: %.2f'%avg_epoch_train_s_loss,'| val_loss: %.2f'% avg_epoch_val_s_loss, '| tpose: %.2f'% avg_epoch_train_s_loss, '| vpose: %.2f'% avg_epoch_val_s_loss, '| ade_train: %.2f'% ade_train, '| ade_val: %.2f'% ade, '| fde_train: %.2f'% fde_train,'| fde_val: %.2f'% fde,\n",
    "          '| t:%.2f'%(time.time()-start))\n",
    "\n",
    "\n",
    "print('='*100) \n",
    "# print('Saving ...')\n",
    "# torch.save(net.state_dict(), args.model_path)\n",
    "print('Done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "general-introduction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (enc1): Linear(in_features=544, out_features=300, bias=True)\n",
       "  (enc2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (dec3): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (dec4): Linear(in_features=300, out_features=544, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # encoder\n",
    "        self.drop =  nn.Dropout(p=0.5)\n",
    "        self.enc1= nn.Linear(in_features=34*16, out_features=300)\n",
    "        self.enc2 = nn.Linear(in_features=300, out_features=300)\n",
    "#         self.enc3 = nn.Linear(in_features=300, out_features=300)\n",
    "#         self.enc4 = nn.Linear(in_features=300, out_features=10)\n",
    "        # decoder \n",
    "#         self.dec1 = nn.Linear(in_features=10, out_features=300)\n",
    "#         self.dec2 = nn.Linear(in_features=75, out_features=150)\n",
    "        self.dec3 = nn.Linear(in_features=300, out_features=300)\n",
    "        self.dec4 = nn.Linear(in_features=300, out_features=34*16)\n",
    "        \n",
    "    def forward(self, x):\n",
    "                \n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "#         x = F.relu(self.enc3(x))\n",
    "#         x = F.relu(self.enc4(x))\n",
    "#         x = F.relu(self.dec1(x))\n",
    "#         x = F.relu(self.dec2(x))\n",
    "#         x = F.relu(self.dec3(x))\n",
    "        x = F.relu(self.dec4(x))\n",
    "        return x\n",
    "    \n",
    "net = Autoencoder()\n",
    "# print(net)\n",
    "net.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "italian-fireplace",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 544]' is invalid for input of size 25500",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-2656f66136e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mtarget_pose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#         print(obs_pose.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mobs_pose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobs_pose\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m34\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mtarget_pose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_pose\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m34\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 544]' is invalid for input of size 25500"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr= 0.1)\n",
    "# optimizer = optim.Adadelta(net.parameters(),lr= 0.1)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10, \n",
    "                                                 threshold = 1e-12, verbose=True)\n",
    "\n",
    "\n",
    "train_p_scores=[]\n",
    "val_p_scores=[]\n",
    "\n",
    "\n",
    "for epoch in range(300):\n",
    "    start = time.time()\n",
    "    avg_epoch_train_p_loss   = 0\n",
    "    avg_epoch_val_p_loss     = 0 \n",
    "    ade  = 0\n",
    "    fde  = 0\n",
    "    ade_train  = 0\n",
    "    fde_train  = 0\n",
    "    counter = 0\n",
    "    net.train()\n",
    "    for idx, (obs_s, target_s, obs_pose, target_pose) in enumerate(train_loader):\n",
    "        counter += 1        \n",
    "        \n",
    "        obs_pose    = obs_s.to(device='cuda')\n",
    "        target_pose = obs_s.to(device='cuda')\n",
    "#         print(obs_pose.shape)\n",
    "        obs_pose=obs_pose.reshape(-1,16*34)\n",
    "        target_pose=target_pose.reshape(-1,16*34)\n",
    "\n",
    "        net.zero_grad()\n",
    "    \n",
    "        pose_preds = net(obs_pose)\n",
    "\n",
    "        loss  = l1e(pose_preds, target_pose)\n",
    "    \n",
    "        ade_train += float(ADE_c(pose_preds.reshape(-1,16,34), target_pose.reshape(-1,15,34)))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        avg_epoch_train_p_loss += float(loss)\n",
    "\n",
    "    avg_epoch_train_p_loss /= counter\n",
    "    train_p_scores.append(avg_epoch_train_p_loss)\n",
    "    ade_train  /= counter\n",
    "    \n",
    "    counter=0\n",
    "    net.eval()\n",
    "    for idx, (obs_s, target_s, obs_pose, target_pose) in enumerate(val_loader):\n",
    "        counter+=1\n",
    "       \n",
    "        obs_pose    = obs_s.to(device='cuda')\n",
    "        target_pose = obs_s.to(device='cuda')\n",
    "        \n",
    "        obs_pose=obs_pose.reshape(-1,16*34)\n",
    "        target_pose=target_pose.reshape(-1,16*34)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pose_preds = net(obs_pose)\n",
    "            \n",
    "            val_loss  = l1e(pose_preds, target_pose)\n",
    "            avg_epoch_val_p_loss += float(val_loss)\n",
    "            \n",
    "            ade += float(ADE_c(pose_preds.reshape(-1,16,34), target_pose.reshape(-1,15,34)))\n",
    "            \n",
    "    avg_epoch_val_p_loss /= counter\n",
    "    val_p_scores.append(avg_epoch_val_p_loss)\n",
    "    \n",
    "    ade  /= counter\n",
    "    \n",
    "    scheduler.step(avg_epoch_val_p_loss)\n",
    "    \n",
    "    print('e:', epoch,'| loss: %.2f'%avg_epoch_train_p_loss,'| val_loss: %.2f'% avg_epoch_val_p_loss, '| ade_train: %.2f'% ade_train, '| ade_val: %.2f'% ade,\n",
    "          '| t:%.2f'%(time.time()-start))\n",
    "\n",
    "\n",
    "print('='*100) \n",
    "# print('Saving ...')\n",
    "# torch.save(net.state_dict(), args.model_path)\n",
    "print('Done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "informational-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_spat(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        '''\n",
    "           input: observed body poses and velocites \n",
    "           output: predicted poses(not trained), velocities of poses, and intentions\n",
    "        '''\n",
    "        super(LSTM_spat, self).__init__()\n",
    "#         self.encoded_tot=16*20\n",
    "        self.encoded_size=20\n",
    "         \n",
    "        self.pose_encoder = nn.LSTM(input_size=self.encoded_size, hidden_size=args.hidden_size)        \n",
    "        self.pose_embedding = nn.Sequential(nn.Linear(in_features=args.hidden_size, out_features=self.encoded_size),\n",
    "                                           nn.ReLU())\n",
    "        \n",
    "        self.pose_decoder = nn.LSTMCell(input_size=self.encoded_size, hidden_size=args.hidden_size)\n",
    "        self.fc_pose   = nn.Linear(in_features=args.hidden_size, out_features=self.encoded_size)\n",
    "        \n",
    "        self.hardtanh = nn.Hardtanh(min_val=-1*args.hardtanh_limit,max_val=args.hardtanh_limit)\n",
    "        self.relu = nn.ReLU() \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.enc1= nn.Linear(in_features=34, out_features=30)\n",
    "        self.enc2 = nn.Linear(in_features=30, out_features=self.encoded_size)\n",
    "        self.dec = nn.Linear(in_features=self.encoded_size, out_features=34)\n",
    "        \n",
    "#         self.drop=n\n",
    "        \n",
    "#         self.encoder = lstm_encoder(input_size = 34, hidden_size = 1000, num_layers = 3)\n",
    "#         self.decoder = lstm_decoder(input_size = 34, hidden_size = 1000, num_layers = 3)\n",
    "\n",
    "#         self.teacher_forcing_ratio = 0.6\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "    def forward(self, pose=None, vel=None, target_pose=None):\n",
    "        \n",
    "        poses=pose.permute(1,0,2)\n",
    "\n",
    "        pose_encoded = torch.tensor([], device=self.args.device)\n",
    "        pose_recs = torch.tensor([], device=self.args.device)\n",
    "        \n",
    "        for i in range(pose.size()[0]):\n",
    "            x = self.relu(self.enc1(pose[i]))\n",
    "            x = self.relu(self.enc2(x))\n",
    "            recreated_pose=self.relu(self.dec(x))\n",
    "            pose_encoded = torch.cat((pose_encoded, x.unsqueeze(1)), dim = 1)\n",
    "            pose_recs = torch.cat((pose_recs, recreated_pose.unsqueeze(1)), dim = 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        _, (hidden_pose, cell_pose) = self.pose_encoder(pose_encoded)\n",
    "        hidden_pose = hidden_pose.squeeze(0)\n",
    "        cell_pose = cell_pose.squeeze(0)\n",
    "        \n",
    "        outputs = []\n",
    "        pose_outputs   = torch.tensor([], device=self.args.device)\n",
    "        PoseDec_inp = pose_encoded.permute(1,0,2)[:,-1,:]\n",
    "        \n",
    "#         print(PoseDec_inp.shape)\n",
    "        hidden_dec=hidden_pose\n",
    "        cell_dec=cell_pose\n",
    "#         print(hidden_dec.shape)\n",
    "        for i in range(self.args.output//self.args.skip):\n",
    "        \n",
    "            hidden_dec, cell_dec = self.pose_decoder(PoseDec_inp, (hidden_dec, cell_dec))\n",
    "            pose_output_encoded  = self.fc_pose(hidden_dec)\n",
    "            \n",
    "            pose_output= self.relu(self.dec(pose_output_encoded))\n",
    "            pose_outputs = torch.cat((pose_outputs, pose_output.unsqueeze(1)), dim = 1)\n",
    "            PoseDec_inp  = pose_output_encoded.detach()\n",
    "            \n",
    "        \n",
    "        pose_recs= pose_recs.permute(1,0,2)\n",
    "        \n",
    "#         print(recreated_pose.shape)\n",
    "#         print(pose_outputs.shape)\n",
    "#         print(pose_outputs.shape,pose_recs.shape)\n",
    "        outputs.append(pose_outputs)\n",
    "        outputs.append(pose_recs)\n",
    "\n",
    "            \n",
    "        return tuple(outputs)\n",
    "\n",
    "net = LSTM_spat(args).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "listed-respect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Training ...\n",
      "e: 0 | loss: 825.09 | val_loss: 696.44 | tpose: 825.09 | vpose: 696.44 | ade_train: 1222.62 | ade_val: 1068.66 | fde_train: 1215.08 | fde_val: 1051.94 | t:0.36\n",
      "e: 1 | loss: 510.10 | val_loss: 397.66 | tpose: 510.10 | vpose: 397.66 | ade_train: 801.87 | ade_val: 611.26 | fde_train: 793.59 | fde_val: 617.65 | t:0.35\n",
      "e: 2 | loss: 434.34 | val_loss: 328.93 | tpose: 434.34 | vpose: 328.93 | ade_train: 669.38 | ade_val: 533.41 | fde_train: 694.11 | fde_val: 535.86 | t:0.34\n",
      "e: 3 | loss: 351.86 | val_loss: 392.16 | tpose: 351.86 | vpose: 392.16 | ade_train: 565.53 | ade_val: 615.04 | fde_train: 558.21 | fde_val: 594.49 | t:0.34\n",
      "e: 4 | loss: 340.28 | val_loss: 324.46 | tpose: 340.28 | vpose: 324.46 | ade_train: 545.77 | ade_val: 525.71 | fde_train: 530.88 | fde_val: 529.32 | t:0.34\n",
      "e: 5 | loss: 321.77 | val_loss: 297.82 | tpose: 321.77 | vpose: 297.82 | ade_train: 525.19 | ade_val: 507.84 | fde_train: 536.98 | fde_val: 520.92 | t:0.33\n",
      "e: 6 | loss: 301.73 | val_loss: 310.05 | tpose: 301.73 | vpose: 310.05 | ade_train: 503.20 | ade_val: 522.76 | fde_train: 508.66 | fde_val: 531.39 | t:0.33\n",
      "e: 7 | loss: 296.80 | val_loss: 300.94 | tpose: 296.80 | vpose: 300.94 | ade_train: 499.68 | ade_val: 501.79 | fde_train: 499.15 | fde_val: 511.71 | t:0.33\n",
      "e: 8 | loss: 292.05 | val_loss: 302.57 | tpose: 292.05 | vpose: 302.57 | ade_train: 493.06 | ade_val: 505.44 | fde_train: 499.51 | fde_val: 518.83 | t:0.32\n",
      "e: 9 | loss: 289.08 | val_loss: 307.00 | tpose: 289.08 | vpose: 307.00 | ade_train: 488.36 | ade_val: 510.48 | fde_train: 495.19 | fde_val: 520.08 | t:0.33\n",
      "e: 10 | loss: 290.22 | val_loss: 303.16 | tpose: 290.22 | vpose: 303.16 | ade_train: 492.50 | ade_val: 515.48 | fde_train: 499.06 | fde_val: 525.63 | t:0.33\n",
      "e: 11 | loss: 284.16 | val_loss: 289.98 | tpose: 284.16 | vpose: 289.98 | ade_train: 487.35 | ade_val: 491.01 | fde_train: 496.02 | fde_val: 504.70 | t:0.33\n",
      "e: 12 | loss: 283.36 | val_loss: 295.70 | tpose: 283.36 | vpose: 295.70 | ade_train: 487.40 | ade_val: 501.45 | fde_train: 497.73 | fde_val: 516.31 | t:0.32\n",
      "e: 13 | loss: 280.51 | val_loss: 287.75 | tpose: 280.51 | vpose: 287.75 | ade_train: 482.47 | ade_val: 492.13 | fde_train: 492.60 | fde_val: 508.45 | t:0.32\n",
      "e: 14 | loss: 281.12 | val_loss: 293.88 | tpose: 281.12 | vpose: 293.88 | ade_train: 486.27 | ade_val: 506.59 | fde_train: 496.55 | fde_val: 522.58 | t:0.33\n",
      "e: 15 | loss: 280.70 | val_loss: 286.69 | tpose: 280.70 | vpose: 286.69 | ade_train: 483.82 | ade_val: 494.13 | fde_train: 493.98 | fde_val: 511.08 | t:0.33\n",
      "e: 16 | loss: 277.14 | val_loss: 284.15 | tpose: 277.14 | vpose: 284.15 | ade_train: 479.99 | ade_val: 490.46 | fde_train: 490.43 | fde_val: 507.37 | t:0.33\n",
      "e: 17 | loss: 277.29 | val_loss: 287.28 | tpose: 277.29 | vpose: 287.28 | ade_train: 480.70 | ade_val: 496.15 | fde_train: 491.69 | fde_val: 513.10 | t:0.33\n",
      "e: 18 | loss: 277.55 | val_loss: 288.58 | tpose: 277.55 | vpose: 288.58 | ade_train: 480.89 | ade_val: 498.89 | fde_train: 491.51 | fde_val: 515.54 | t:0.32\n",
      "e: 19 | loss: 277.37 | val_loss: 286.45 | tpose: 277.37 | vpose: 286.45 | ade_train: 480.69 | ade_val: 494.72 | fde_train: 490.93 | fde_val: 512.06 | t:0.33\n",
      "e: 20 | loss: 276.21 | val_loss: 282.05 | tpose: 276.21 | vpose: 282.05 | ade_train: 478.75 | ade_val: 486.35 | fde_train: 489.62 | fde_val: 503.41 | t:0.33\n",
      "e: 21 | loss: 278.19 | val_loss: 291.79 | tpose: 278.19 | vpose: 291.79 | ade_train: 481.97 | ade_val: 501.44 | fde_train: 493.68 | fde_val: 517.84 | t:0.33\n",
      "e: 22 | loss: 278.58 | val_loss: 287.99 | tpose: 278.58 | vpose: 287.99 | ade_train: 480.74 | ade_val: 495.00 | fde_train: 489.99 | fde_val: 512.26 | t:0.34\n",
      "e: 23 | loss: 279.69 | val_loss: 288.93 | tpose: 279.69 | vpose: 288.93 | ade_train: 482.82 | ade_val: 497.07 | fde_train: 492.42 | fde_val: 511.96 | t:0.33\n",
      "e: 24 | loss: 278.66 | val_loss: 285.25 | tpose: 278.66 | vpose: 285.25 | ade_train: 481.05 | ade_val: 492.34 | fde_train: 492.04 | fde_val: 509.31 | t:0.33\n",
      "e: 25 | loss: 276.86 | val_loss: 286.01 | tpose: 276.86 | vpose: 286.01 | ade_train: 480.00 | ade_val: 494.19 | fde_train: 492.11 | fde_val: 510.64 | t:0.33\n",
      "e: 26 | loss: 275.69 | val_loss: 291.91 | tpose: 275.69 | vpose: 291.91 | ade_train: 478.18 | ade_val: 505.45 | fde_train: 488.75 | fde_val: 519.34 | t:0.37\n",
      "e: 27 | loss: 279.58 | val_loss: 292.38 | tpose: 279.58 | vpose: 292.38 | ade_train: 484.43 | ade_val: 505.75 | fde_train: 493.37 | fde_val: 521.86 | t:0.33\n",
      "e: 28 | loss: 276.22 | val_loss: 280.17 | tpose: 276.22 | vpose: 280.17 | ade_train: 478.72 | ade_val: 483.19 | fde_train: 489.17 | fde_val: 500.22 | t:0.33\n",
      "e: 29 | loss: 282.33 | val_loss: 284.07 | tpose: 282.33 | vpose: 284.07 | ade_train: 488.74 | ade_val: 487.35 | fde_train: 502.56 | fde_val: 505.82 | t:0.33\n",
      "e: 30 | loss: 276.88 | val_loss: 290.14 | tpose: 276.88 | vpose: 290.14 | ade_train: 479.44 | ade_val: 502.90 | fde_train: 491.42 | fde_val: 521.08 | t:0.35\n",
      "e: 31 | loss: 278.06 | val_loss: 289.11 | tpose: 278.06 | vpose: 289.11 | ade_train: 481.68 | ade_val: 498.91 | fde_train: 492.54 | fde_val: 517.29 | t:0.33\n",
      "e: 32 | loss: 276.06 | val_loss: 287.47 | tpose: 276.06 | vpose: 287.47 | ade_train: 478.39 | ade_val: 496.96 | fde_train: 489.96 | fde_val: 515.55 | t:0.33\n",
      "e: 33 | loss: 274.72 | val_loss: 284.84 | tpose: 274.72 | vpose: 284.84 | ade_train: 476.51 | ade_val: 492.51 | fde_train: 488.61 | fde_val: 511.95 | t:0.34\n",
      "e: 34 | loss: 277.28 | val_loss: 283.25 | tpose: 277.28 | vpose: 283.25 | ade_train: 480.82 | ade_val: 488.91 | fde_train: 492.66 | fde_val: 507.78 | t:0.33\n",
      "e: 35 | loss: 278.06 | val_loss: 291.89 | tpose: 278.06 | vpose: 291.89 | ade_train: 481.95 | ade_val: 503.01 | fde_train: 493.72 | fde_val: 520.67 | t:0.33\n",
      "e: 36 | loss: 278.95 | val_loss: 288.57 | tpose: 278.95 | vpose: 288.57 | ade_train: 482.83 | ade_val: 499.75 | fde_train: 493.07 | fde_val: 517.97 | t:0.33\n",
      "e: 37 | loss: 276.01 | val_loss: 285.63 | tpose: 276.01 | vpose: 285.63 | ade_train: 479.49 | ade_val: 491.76 | fde_train: 491.01 | fde_val: 511.64 | t:0.34\n",
      "e: 38 | loss: 276.23 | val_loss: 283.16 | tpose: 276.23 | vpose: 283.16 | ade_train: 479.07 | ade_val: 489.89 | fde_train: 491.45 | fde_val: 509.55 | t:0.33\n",
      "e: 39 | loss: 276.74 | val_loss: 288.15 | tpose: 276.74 | vpose: 288.15 | ade_train: 481.65 | ade_val: 499.21 | fde_train: 494.17 | fde_val: 517.41 | t:0.33\n",
      "e: 40 | loss: 276.22 | val_loss: 298.45 | tpose: 276.22 | vpose: 298.45 | ade_train: 479.03 | ade_val: 510.10 | fde_train: 490.68 | fde_val: 529.23 | t:0.34\n",
      "e: 41 | loss: 282.16 | val_loss: 289.52 | tpose: 282.16 | vpose: 289.52 | ade_train: 482.25 | ade_val: 494.92 | fde_train: 493.77 | fde_val: 513.39 | t:0.34\n",
      "Epoch    43: reducing learning rate of group 0 to 5.0000e-02.\n",
      "e: 42 | loss: 279.70 | val_loss: 283.43 | tpose: 279.70 | vpose: 283.43 | ade_train: 482.88 | ade_val: 490.05 | fde_train: 494.19 | fde_val: 508.85 | t:0.33\n",
      "e: 43 | loss: 276.27 | val_loss: 283.64 | tpose: 276.27 | vpose: 283.64 | ade_train: 479.36 | ade_val: 490.13 | fde_train: 492.09 | fde_val: 508.43 | t:0.33\n",
      "e: 44 | loss: 275.70 | val_loss: 283.05 | tpose: 275.70 | vpose: 283.05 | ade_train: 478.72 | ade_val: 489.63 | fde_train: 491.10 | fde_val: 509.00 | t:0.33\n",
      "e: 45 | loss: 275.93 | val_loss: 285.98 | tpose: 275.93 | vpose: 285.98 | ade_train: 479.52 | ade_val: 494.84 | fde_train: 492.08 | fde_val: 514.34 | t:0.33\n",
      "e: 46 | loss: 275.66 | val_loss: 287.73 | tpose: 275.66 | vpose: 287.73 | ade_train: 478.78 | ade_val: 498.63 | fde_train: 490.55 | fde_val: 518.42 | t:0.34\n",
      "e: 47 | loss: 274.67 | val_loss: 285.96 | tpose: 274.67 | vpose: 285.96 | ade_train: 477.47 | ade_val: 495.47 | fde_train: 489.67 | fde_val: 513.93 | t:0.33\n",
      "e: 48 | loss: 274.06 | val_loss: 285.45 | tpose: 274.06 | vpose: 285.45 | ade_train: 476.26 | ade_val: 494.32 | fde_train: 488.37 | fde_val: 512.97 | t:0.33\n",
      "e: 49 | loss: 274.38 | val_loss: 284.99 | tpose: 274.38 | vpose: 284.99 | ade_train: 477.10 | ade_val: 493.46 | fde_train: 489.25 | fde_val: 512.57 | t:0.36\n",
      "e: 50 | loss: 274.85 | val_loss: 286.17 | tpose: 274.85 | vpose: 286.17 | ade_train: 477.78 | ade_val: 495.63 | fde_train: 490.07 | fde_val: 515.10 | t:0.33\n",
      "e: 51 | loss: 274.31 | val_loss: 284.85 | tpose: 274.31 | vpose: 284.85 | ade_train: 476.78 | ade_val: 493.50 | fde_train: 488.88 | fde_val: 512.53 | t:0.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 52 | loss: 275.30 | val_loss: 284.51 | tpose: 275.30 | vpose: 284.51 | ade_train: 478.77 | ade_val: 492.01 | fde_train: 491.33 | fde_val: 511.01 | t:0.33\n",
      "e: 53 | loss: 276.21 | val_loss: 286.03 | tpose: 276.21 | vpose: 286.03 | ade_train: 479.34 | ade_val: 494.54 | fde_train: 491.59 | fde_val: 513.64 | t:0.35\n",
      "e: 54 | loss: 276.71 | val_loss: 288.72 | tpose: 276.71 | vpose: 288.72 | ade_train: 481.27 | ade_val: 500.53 | fde_train: 493.18 | fde_val: 519.92 | t:0.33\n",
      "e: 55 | loss: 274.81 | val_loss: 284.92 | tpose: 274.81 | vpose: 284.92 | ade_train: 477.68 | ade_val: 493.12 | fde_train: 489.64 | fde_val: 512.12 | t:0.33\n",
      "e: 56 | loss: 275.46 | val_loss: 283.21 | tpose: 275.46 | vpose: 283.21 | ade_train: 478.60 | ade_val: 489.14 | fde_train: 491.44 | fde_val: 508.41 | t:0.32\n",
      "Epoch    58: reducing learning rate of group 0 to 2.5000e-02.\n",
      "e: 57 | loss: 275.51 | val_loss: 281.96 | tpose: 275.51 | vpose: 281.96 | ade_train: 477.87 | ade_val: 487.63 | fde_train: 490.83 | fde_val: 507.00 | t:0.34\n",
      "e: 58 | loss: 275.71 | val_loss: 283.28 | tpose: 275.71 | vpose: 283.28 | ade_train: 478.96 | ade_val: 490.31 | fde_train: 491.90 | fde_val: 509.69 | t:0.33\n",
      "e: 59 | loss: 275.31 | val_loss: 286.52 | tpose: 275.31 | vpose: 286.52 | ade_train: 478.50 | ade_val: 496.03 | fde_train: 490.76 | fde_val: 515.71 | t:0.32\n",
      "e: 60 | loss: 274.96 | val_loss: 284.86 | tpose: 274.96 | vpose: 284.86 | ade_train: 478.12 | ade_val: 493.41 | fde_train: 490.44 | fde_val: 512.57 | t:0.32\n",
      "e: 61 | loss: 274.34 | val_loss: 286.52 | tpose: 274.34 | vpose: 286.52 | ade_train: 476.86 | ade_val: 496.10 | fde_train: 489.17 | fde_val: 515.59 | t:0.32\n",
      "e: 62 | loss: 274.58 | val_loss: 286.00 | tpose: 274.58 | vpose: 286.00 | ade_train: 477.38 | ade_val: 495.49 | fde_train: 489.53 | fde_val: 514.92 | t:0.32\n",
      "e: 63 | loss: 274.72 | val_loss: 286.31 | tpose: 274.72 | vpose: 286.31 | ade_train: 477.64 | ade_val: 496.27 | fde_train: 489.79 | fde_val: 515.71 | t:0.32\n",
      "e: 64 | loss: 274.21 | val_loss: 286.73 | tpose: 274.21 | vpose: 286.73 | ade_train: 476.96 | ade_val: 497.00 | fde_train: 489.19 | fde_val: 516.64 | t:0.33\n",
      "e: 65 | loss: 274.64 | val_loss: 285.11 | tpose: 274.64 | vpose: 285.11 | ade_train: 477.64 | ade_val: 494.06 | fde_train: 489.67 | fde_val: 513.50 | t:0.32\n",
      "Epoch    67: reducing learning rate of group 0 to 1.2500e-02.\n",
      "e: 66 | loss: 274.34 | val_loss: 285.30 | tpose: 274.34 | vpose: 285.30 | ade_train: 477.05 | ade_val: 494.31 | fde_train: 489.23 | fde_val: 513.72 | t:0.32\n",
      "e: 67 | loss: 274.23 | val_loss: 285.46 | tpose: 274.23 | vpose: 285.46 | ade_train: 477.05 | ade_val: 494.51 | fde_train: 489.44 | fde_val: 514.02 | t:0.33\n",
      "e: 68 | loss: 273.97 | val_loss: 285.73 | tpose: 273.97 | vpose: 285.73 | ade_train: 476.40 | ade_val: 494.89 | fde_train: 488.72 | fde_val: 514.50 | t:0.32\n",
      "e: 69 | loss: 274.44 | val_loss: 285.58 | tpose: 274.44 | vpose: 285.58 | ade_train: 477.31 | ade_val: 494.92 | fde_train: 489.49 | fde_val: 514.44 | t:0.33\n",
      "e: 70 | loss: 274.32 | val_loss: 284.81 | tpose: 274.32 | vpose: 284.81 | ade_train: 477.22 | ade_val: 493.49 | fde_train: 489.61 | fde_val: 512.85 | t:0.33\n",
      "e: 71 | loss: 274.21 | val_loss: 285.49 | tpose: 274.21 | vpose: 285.49 | ade_train: 477.06 | ade_val: 494.69 | fde_train: 489.36 | fde_val: 514.27 | t:0.33\n",
      "e: 72 | loss: 274.45 | val_loss: 285.35 | tpose: 274.45 | vpose: 285.35 | ade_train: 477.46 | ade_val: 494.26 | fde_train: 489.90 | fde_val: 513.86 | t:0.32\n",
      "e: 73 | loss: 274.45 | val_loss: 285.47 | tpose: 274.45 | vpose: 285.47 | ade_train: 477.36 | ade_val: 494.54 | fde_train: 489.76 | fde_val: 514.12 | t:0.32\n",
      "e: 74 | loss: 274.69 | val_loss: 285.85 | tpose: 274.69 | vpose: 285.85 | ade_train: 477.79 | ade_val: 495.36 | fde_train: 489.92 | fde_val: 514.93 | t:0.32\n",
      "e: 75 | loss: 274.10 | val_loss: 285.77 | tpose: 274.10 | vpose: 285.77 | ade_train: 476.71 | ade_val: 495.34 | fde_train: 488.89 | fde_val: 514.84 | t:0.33\n",
      "e: 76 | loss: 274.26 | val_loss: 285.72 | tpose: 274.26 | vpose: 285.72 | ade_train: 477.11 | ade_val: 495.23 | fde_train: 489.38 | fde_val: 514.77 | t:0.34\n",
      "Epoch    78: reducing learning rate of group 0 to 6.2500e-03.\n",
      "e: 77 | loss: 274.53 | val_loss: 285.75 | tpose: 274.53 | vpose: 285.75 | ade_train: 477.33 | ade_val: 494.98 | fde_train: 489.59 | fde_val: 514.64 | t:0.33\n",
      "e: 78 | loss: 274.21 | val_loss: 285.45 | tpose: 274.21 | vpose: 285.45 | ade_train: 476.94 | ade_val: 494.51 | fde_train: 489.35 | fde_val: 514.12 | t:0.32\n",
      "e: 79 | loss: 273.92 | val_loss: 285.32 | tpose: 273.92 | vpose: 285.32 | ade_train: 476.51 | ade_val: 494.41 | fde_train: 488.88 | fde_val: 513.95 | t:0.33\n",
      "e: 80 | loss: 274.54 | val_loss: 285.02 | tpose: 274.54 | vpose: 285.02 | ade_train: 477.62 | ade_val: 493.86 | fde_train: 489.99 | fde_val: 513.32 | t:0.32\n",
      "e: 81 | loss: 274.40 | val_loss: 284.95 | tpose: 274.40 | vpose: 284.95 | ade_train: 477.26 | ade_val: 493.73 | fde_train: 489.64 | fde_val: 513.19 | t:0.32\n",
      "e: 82 | loss: 274.08 | val_loss: 285.52 | tpose: 274.08 | vpose: 285.52 | ade_train: 476.79 | ade_val: 494.66 | fde_train: 489.08 | fde_val: 514.30 | t:0.33\n",
      "e: 83 | loss: 274.25 | val_loss: 286.04 | tpose: 274.25 | vpose: 286.04 | ade_train: 476.98 | ade_val: 495.46 | fde_train: 489.27 | fde_val: 515.19 | t:0.33\n",
      "e: 84 | loss: 274.36 | val_loss: 285.78 | tpose: 274.36 | vpose: 285.78 | ade_train: 477.15 | ade_val: 495.18 | fde_train: 489.58 | fde_val: 514.84 | t:0.34\n",
      "e: 85 | loss: 274.47 | val_loss: 285.24 | tpose: 274.47 | vpose: 285.24 | ade_train: 477.56 | ade_val: 494.33 | fde_train: 489.64 | fde_val: 513.82 | t:0.32\n",
      "e: 86 | loss: 274.33 | val_loss: 285.01 | tpose: 274.33 | vpose: 285.01 | ade_train: 477.24 | ade_val: 493.84 | fde_train: 489.75 | fde_val: 513.28 | t:0.32\n",
      "e: 87 | loss: 274.42 | val_loss: 285.33 | tpose: 274.42 | vpose: 285.33 | ade_train: 477.27 | ade_val: 494.45 | fde_train: 489.57 | fde_val: 514.00 | t:0.32\n",
      "Epoch    89: reducing learning rate of group 0 to 3.1250e-03.\n",
      "e: 88 | loss: 274.11 | val_loss: 285.94 | tpose: 274.11 | vpose: 285.94 | ade_train: 476.96 | ade_val: 495.39 | fde_train: 489.16 | fde_val: 515.08 | t:0.35\n",
      "e: 89 | loss: 274.57 | val_loss: 286.01 | tpose: 274.57 | vpose: 286.01 | ade_train: 477.57 | ade_val: 495.52 | fde_train: 489.82 | fde_val: 515.22 | t:0.33\n",
      "e: 90 | loss: 274.60 | val_loss: 285.81 | tpose: 274.60 | vpose: 285.81 | ade_train: 477.69 | ade_val: 495.27 | fde_train: 490.04 | fde_val: 514.92 | t:0.34\n",
      "e: 91 | loss: 273.71 | val_loss: 285.43 | tpose: 273.71 | vpose: 285.43 | ade_train: 476.10 | ade_val: 494.65 | fde_train: 488.30 | fde_val: 514.20 | t:0.33\n",
      "e: 92 | loss: 274.29 | val_loss: 285.27 | tpose: 274.29 | vpose: 285.27 | ade_train: 477.17 | ade_val: 494.36 | fde_train: 489.42 | fde_val: 513.87 | t:0.33\n",
      "e: 93 | loss: 274.26 | val_loss: 285.32 | tpose: 274.26 | vpose: 285.32 | ade_train: 477.14 | ade_val: 494.45 | fde_train: 489.44 | fde_val: 513.98 | t:0.34\n",
      "e: 94 | loss: 273.67 | val_loss: 285.32 | tpose: 273.67 | vpose: 285.32 | ade_train: 476.10 | ade_val: 494.44 | fde_train: 488.37 | fde_val: 514.00 | t:0.33\n",
      "e: 95 | loss: 274.36 | val_loss: 285.57 | tpose: 274.36 | vpose: 285.57 | ade_train: 477.28 | ade_val: 494.87 | fde_train: 489.56 | fde_val: 514.49 | t:0.34\n",
      "e: 96 | loss: 273.74 | val_loss: 285.73 | tpose: 273.74 | vpose: 285.73 | ade_train: 476.16 | ade_val: 495.11 | fde_train: 488.40 | fde_val: 514.77 | t:0.35\n",
      "e: 97 | loss: 274.53 | val_loss: 285.60 | tpose: 274.53 | vpose: 285.60 | ade_train: 477.63 | ade_val: 494.87 | fde_train: 489.88 | fde_val: 514.52 | t:0.33\n",
      "e: 98 | loss: 273.91 | val_loss: 285.48 | tpose: 273.91 | vpose: 285.48 | ade_train: 476.47 | ade_val: 494.67 | fde_train: 488.71 | fde_val: 514.29 | t:0.36\n",
      "e: 99 | loss: 274.12 | val_loss: 285.39 | tpose: 274.12 | vpose: 285.39 | ade_train: 476.89 | ade_val: 494.52 | fde_train: 489.21 | fde_val: 514.12 | t:0.33\n",
      "====================================================================================================\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr= 0.1)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=8, \n",
    "                                                 threshold = 1e-12, verbose=True)\n",
    "print('='*100)\n",
    "print('Training ...')\n",
    "\n",
    "train_p_scores=[]\n",
    "val_p_scores=[]\n",
    "\n",
    "for epoch in range(100):\n",
    "    start = time.time()\n",
    "    \n",
    "    avg_epoch_train_p_loss   = 0\n",
    "    avg_epoch_val_p_loss     = 0 \n",
    "    ade  = 0\n",
    "    fde  = 0\n",
    "    ade_train  = 0\n",
    "    fde_train  = 0\n",
    "    counter = 0\n",
    "    net.train()\n",
    "    for idx, (obs_s, target_s, obs_pose, target_pose) in enumerate(train_loader):\n",
    "        counter += 1        \n",
    "        \n",
    "        \n",
    "        obs_pose    = obs_pose.to(device='cuda')\n",
    "        target_pose = target_pose.to(device='cuda')\n",
    "        \n",
    "        obs_s    = obs_s.to(device='cuda')\n",
    "        target_s = target_s.to(device='cuda')\n",
    " \n",
    "        net.zero_grad()\n",
    "    \n",
    "        (pose_preds,pose_recs) = net(pose=obs_pose, target_pose=target_pose,vel=obs_s)\n",
    "\n",
    "        loss  = l1e(pose_preds, target_pose)#+l1e(pose_recs, obs_pose)\n",
    "    \n",
    "        ade_train += float(ADE_c(pose_preds, target_pose))\n",
    "        fde_train += float(FDE_c(pose_preds, target_pose))\n",
    "        \n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "        avg_epoch_train_p_loss += float(loss)\n",
    "\n",
    "    avg_epoch_train_p_loss /= counter\n",
    "    train_p_scores.append(avg_epoch_train_p_loss)\n",
    "    ade_train  /= counter\n",
    "    fde_train  /= counter   \n",
    "    \n",
    "    \n",
    "  \n",
    "    counter=0\n",
    "    net.eval()\n",
    "    for idx, (obs_s, target_s, obs_pose, target_pose) in enumerate(val_loader):\n",
    "        counter+=1\n",
    "       \n",
    "        obs_pose    = obs_pose.to(device='cuda')\n",
    "        target_pose = target_pose.to(device='cuda')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            (pose_preds,pose_recs) = net(pose=obs_pose, target_pose=target_pose)\n",
    "          \n",
    "            val_loss  = l1e(pose_preds, target_pose)#+l1e(pose_recs, obs_pose)\n",
    "            avg_epoch_val_p_loss += float(val_loss)\n",
    "            \n",
    "            ade += float(ADE_c(pose_preds, target_pose))\n",
    "            fde += float(FDE_c(pose_preds, target_pose))\n",
    "\n",
    "        \n",
    "    avg_epoch_val_p_loss /= counter\n",
    "    val_p_scores.append(avg_epoch_val_p_loss)\n",
    "    \n",
    "    ade  /= counter\n",
    "    fde  /= counter     \n",
    "   \n",
    "    \n",
    "    \n",
    "    scheduler.step(avg_epoch_train_p_loss)\n",
    "    \n",
    "    print('e:', epoch,'| loss: %.2f'%avg_epoch_train_p_loss,'| val_loss: %.2f'% avg_epoch_val_p_loss, '| tpose: %.2f'% avg_epoch_train_p_loss, '| vpose: %.2f'% avg_epoch_val_p_loss, '| ade_train: %.2f'% ade_train, '| ade_val: %.2f'% ade, '| fde_train: %.2f'% fde_train,'| fde_val: %.2f'% fde,\n",
    "          '| t:%.2f'%(time.time()-start))\n",
    "\n",
    "\n",
    "print('='*100) \n",
    "# print('Saving ...')\n",
    "# torch.save(net.state_dict(), args.model_path)\n",
    "print('Done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "express-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset_DE(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, dtype):\n",
    "        \n",
    "        self.args = args\n",
    "        self.dtype = dtype\n",
    "        print(\"Loading\",self.dtype)\n",
    "        \n",
    "        sequence_centric = pd.read_csv(\"sequences_openpifpaf_\"+self.dtype+\".csv\")\n",
    "#         sequence_centric = pd.read_csv(\"sequences_4_\"+self.dtype+\".csv\")\n",
    "\n",
    "        df = sequence_centric.copy()      \n",
    "        for v in list(df.columns.values):\n",
    "            print(v+' loaded')\n",
    "            try:\n",
    "                df.loc[:,v] = df.loc[:, v].apply(lambda x: literal_eval(x))\n",
    "            except:\n",
    "                continue\n",
    "        sequence_centric[df.columns] = df[df.columns]\n",
    "        self.data = sequence_centric.copy().reset_index(drop=True)\n",
    "    \n",
    "        print('*'*30)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        seq = self.data.iloc[index]\n",
    "        outputs = []\n",
    "\n",
    "        obs = torch.tensor([seq.Pose[i] for i in range(0,self.args.input,self.args.skip)])        \n",
    "        obs_speed = (obs[1:] - obs[:-1])\n",
    "        \n",
    "        true = torch.tensor([seq.Future_Pose[i] for i in range(0,self.args.output,self.args.skip)])\n",
    "        true_speed = torch.cat(((true[0]-obs[-1]).unsqueeze(0), true[1:]-true[:-1]))\n",
    "        \n",
    "        outputs.append(obs_speed)\n",
    "        outputs.append(true_speed)\n",
    "        #obs and future global avg of joint 5 & 6 (0 indexed) for openpifpaf\n",
    "        #local = obs pose-global  for openpifpaf\n",
    "        #do serparately for x and y\n",
    "        obs_resh = torch.reshape(obs, (obs.size()[0],17,2))\n",
    "        \n",
    "        obs_global=(obs_resh[:,5]+obs_resh[:,6])/2.0\n",
    "        obs_global=obs_global.unsqueeze(1)\n",
    "\n",
    "        obs_resh=obs_resh-obs_global\n",
    "    \n",
    "#         print(\"-\"*100,\"\\n\",obs)\n",
    "#         print(\"-\"*100,\"\\n\",obs_resh)\n",
    "#         print(\"-\"*100,\"\\n\",obs_global)\n",
    "#         print(\"-\"*100,\"\\n\",obs_resh+obs_global)\n",
    "    \n",
    "        obs_resh=obs_resh.reshape(obs.size())\n",
    "        \n",
    "        true_resh = torch.reshape(true, (true.size()[0],17,2))\n",
    "        true_global=(true_resh[:,5]+true_resh[:,6])/2.0\n",
    "        true_global=true_global.unsqueeze(1)\n",
    "#         print(obs_resh.size(),obs_global.size())\n",
    "        true_resh=true_resh-true_global\n",
    "        true_resh=true_resh.reshape(true.size())\n",
    "        \n",
    "        obs_global=torch.reshape(obs_global, (obs.size()[0],2))\n",
    "        true_global=torch.reshape(true_global, (true.size()[0],2))\n",
    "        \n",
    "        obs_global_speed = (obs_global[1:] - obs_global[:-1])\n",
    "        true_global_speed = torch.cat(((true_global[0]-obs_global[-1]).unsqueeze(0), true_global[1:]-true_global[:-1]))\n",
    "        \n",
    "#         print(obs_resh.size(),true_resh.size(),obs_global.size(),true_global.size())\n",
    "        \n",
    "        outputs.append(obs_resh)\n",
    "        outputs.append(true_resh)\n",
    "        outputs.append(obs_global)\n",
    "        outputs.append(true_global)\n",
    "        outputs.append(obs_global_speed)\n",
    "        outputs.append(true_global_speed)\n",
    "        outputs.append(true)\n",
    "#         print(obs_resh.size())\n",
    "#         obs_y_global=(obs_resh[:,5,1]+obs_resh[:,6,1])/2.0\n",
    "#         print(obs_global.size())\n",
    "#         print(obs_global)\n",
    "#         print(obs_y_global)\n",
    "        \n",
    "#         print(obs.size(),obs_resh.size())\n",
    "#         obs_global= torch.mean(obs[:,5:7],dim=1).unsqueeze(1)\n",
    "#         print(obs_global)\n",
    "#         obs=obs-obs_global\n",
    "#         print(obs)\n",
    "#         true_global=torch.mean(true[:,5:7],dim=1).unsqueeze(1)\n",
    "#         true=true-true_global\n",
    "       \n",
    "        \n",
    "        \n",
    "        return tuple(outputs)    \n",
    "    \n",
    "    \n",
    "def data_loader_DE(args,data):\n",
    "    dataset = myDataset_DE(args,data)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=args.batch_size, shuffle=args.loader_shuffle,\n",
    "        pin_memory=args.pin_memory)\n",
    "\n",
    "    return dataloader\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-intelligence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class PV_LSTM_DE_op(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        '''\n",
    "           input: observed body poses and velocites global and local\n",
    "           output: global and local velocities\n",
    "        '''\n",
    "        super(PV_LSTM_DE_op, self).__init__()\n",
    "         \n",
    "        self.pose_encoder = nn.LSTM(input_size=34, hidden_size=args.hidden_size)\n",
    "        self.vel_encoder = nn.LSTM(input_size=34, hidden_size=args.hidden_size)\n",
    "        self.vel_decoder = nn.LSTMCell(input_size=34, hidden_size=args.hidden_size)\n",
    "        self.fc_vel    = nn.Linear(in_features=args.hidden_size, out_features=34)\n",
    "        \n",
    "        self.pose_glob_encoder = nn.LSTM(input_size=2, hidden_size=100)\n",
    "        self.vel_glob_encoder = nn.LSTM(input_size=2, hidden_size=100)\n",
    "        self.vel_glob_decoder = nn.LSTMCell(input_size=2, hidden_size=100)\n",
    "        self.fc_global    = nn.Linear(in_features=100, out_features=2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.hardtanh = nn.Hardtanh(min_val=-1*args.hardtanh_limit,max_val=args.hardtanh_limit)\n",
    "        self.relu = nn.ReLU() \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "    def forward(self, pose=None, vel=None, pose_glob=None, vel_glob=None):\n",
    "\n",
    "\n",
    "        _, (hidden_vel, cell_vel) = self.vel_encoder(vel.permute(1,0,2))\n",
    "        hidden_vel = hidden_vel.squeeze(0)\n",
    "        cell_vel = cell_vel.squeeze(0)\n",
    "\n",
    "\n",
    "        _, (hidden_pose, cell_pose) = self.pose_encoder(pose.permute(1,0,2))\n",
    "        hidden_pose = hidden_pose.squeeze(0)\n",
    "        cell_pose = cell_pose.squeeze(0)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "      \n",
    "        #predicting the velocity of poses and poses\n",
    "        vel_outputs    = torch.tensor([], device=self.args.device)\n",
    "        #pose_outputs   = torch.tensor([], device=self.args.device)\n",
    "        glob_outputs    = torch.tensor([], device=self.args.device)\n",
    "\n",
    "        VelDec_inp = vel[:,-1,:]\n",
    "        #PoseDec_inp = pose[:,-1,:]\n",
    "        \n",
    "        hidden_dec = hidden_pose + hidden_vel\n",
    "        cell_dec = cell_pose + cell_vel\n",
    "        for i in range(self.args.output//self.args.skip):\n",
    "            hidden_dec, cell_dec = self.vel_decoder(VelDec_inp, (hidden_dec, cell_dec))\n",
    "            vel_output  = self.hardtanh(self.fc_vel(hidden_dec))\n",
    "            vel_outputs = torch.cat((vel_outputs, vel_output.unsqueeze(1)), dim = 1)\n",
    "            VelDec_inp  = vel_output.detach()\n",
    "          \n",
    "        outputs.append(vel_outputs)\n",
    "        \n",
    "        _, (hidden_vel_glob, cell_vel_glob) = self.vel_glob_encoder(vel_glob.permute(1,0,2))\n",
    "        hidden_vel_glob = hidden_vel_glob.squeeze(0)\n",
    "        cell_vel_glob = cell_vel_glob.squeeze(0)\n",
    "        \n",
    "        _, (hidden_pose_glob, cell_pose_glob) = self.pose_glob_encoder(pose_glob.permute(1,0,2))\n",
    "        hidden_pose_glob = hidden_pose_glob.squeeze(0)\n",
    "        cell_pose_glob = cell_pose_glob.squeeze(0)\n",
    "        \n",
    "        glob_inp = pose_glob[:,-1,:]\n",
    "        \n",
    "        hidden_glob = hidden_pose_glob + hidden_vel_glob\n",
    "        cell_glob = cell_pose_glob + cell_vel_glob\n",
    "        for i in range(self.args.output//self.args.skip):\n",
    "            hidden_glob, cell_glob = self.vel_glob_decoder(glob_inp, (hidden_glob, cell_glob))\n",
    "            glob_output  = self.fc_global(hidden_glob)\n",
    "            glob_outputs = torch.cat((glob_outputs, glob_output.unsqueeze(1)), dim = 1)\n",
    "            glob_inp  = glob_output.detach()\n",
    "       \n",
    "        outputs.append(glob_outputs)\n",
    "        \n",
    "       \n",
    "        return tuple(outputs)\n",
    "\n",
    "\n",
    "class myDataset_DE_op(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, dtype):\n",
    "        \n",
    "        self.args = args\n",
    "        self.dtype = dtype\n",
    "        print(\"Loading\",self.dtype)\n",
    "        \n",
    "        sequence_centric = pd.read_csv(\"sequences_openpifpaf_thres4_wimage_\"+self.dtype+\".csv\")\n",
    "#         sequences_openpifpaf_thres4_wconfsc_wimage_train\n",
    "#         sequence_centric = pd.read_csv(\"sequences_16_overlap_4_thres4_\"+self.dtype+\".csv\")\n",
    "\n",
    "        df = sequence_centric.copy()      \n",
    "        for v in list(df.columns.values):\n",
    "            print(v+' loaded')\n",
    "            try:\n",
    "                df.loc[:,v] = df.loc[:, v].apply(lambda x: literal_eval(x))\n",
    "            except:\n",
    "                continue\n",
    "        sequence_centric[df.columns] = df[df.columns]\n",
    "        self.data = sequence_centric.copy().reset_index(drop=True)\n",
    "    \n",
    "        print('*'*30)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        seq = self.data.iloc[index]\n",
    "        outputs = []\n",
    "\n",
    "        obs = torch.tensor([seq.Pose[i] for i in range(0,self.args.input,self.args.skip)])        \n",
    "        obs_speed = (obs[1:] - obs[:-1])\n",
    "        \n",
    "        true = torch.tensor([seq.Future_Pose[i] for i in range(0,self.args.output,self.args.skip)])\n",
    "        true_speed = torch.cat(((true[0]-obs[-1]).unsqueeze(0), true[1:]-true[:-1]))\n",
    "        \n",
    "        outputs.append(obs_speed)\n",
    "        outputs.append(true_speed)\n",
    "        #obs and future global avg of joint 5 & 6 (0 indexed) for openpifpaf\n",
    "        #local = obs pose-global  for openpifpaf\n",
    "        #do serparately for x and y\n",
    "        obs_resh = torch.reshape(obs, (obs.size()[0],17,2))\n",
    "        \n",
    "        obs_global=obs_resh[:,0]\n",
    "        obs_global=obs_global.unsqueeze(1)\n",
    "\n",
    "        obs_resh=obs_resh-obs_global\n",
    "    \n",
    "\n",
    "        obs_resh=obs_resh.reshape(obs.size())\n",
    "        \n",
    "        true_resh = torch.reshape(true, (true.size()[0],17,2))\n",
    "        true_global=true_resh[:,0]\n",
    "        true_global=true_global.unsqueeze(1)\n",
    "#         print(obs_resh.size(),obs_global.size())\n",
    "        true_resh=true_resh-true_global\n",
    "        true_resh=true_resh.reshape(true.size())\n",
    "        \n",
    "        obs_global=torch.reshape(obs_global, (obs.size()[0],2))\n",
    "        true_global=torch.reshape(true_global, (true.size()[0],2))\n",
    "        \n",
    "        obs_global_speed = (obs_global[1:] - obs_global[:-1])\n",
    "        true_global_speed = torch.cat(((true_global[0]-obs_global[-1]).unsqueeze(0), true_global[1:]-true_global[:-1]))\n",
    "\n",
    "        outputs.append(obs_resh)\n",
    "        outputs.append(true_resh)\n",
    "        outputs.append(obs_global)\n",
    "        outputs.append(true_global)\n",
    "        outputs.append(obs_global_speed)\n",
    "        outputs.append(true_global_speed)\n",
    "        outputs.append(true)\n",
    "\n",
    "        return tuple(outputs)    \n",
    "    \n",
    "    \n",
    "def data_loader_DE_op(args,data):\n",
    "    dataset = myDataset_DE_op(args,data)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=args.batch_size, shuffle=args.loader_shuffle,\n",
    "        pin_memory=args.pin_memory)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "#Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "special-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "class glob_LSTM_vel(nn.Module):    \n",
    "    def __init__(self, args):\n",
    "        '''\n",
    "           input: observed body poses and velocites \n",
    "           output: predicted poses(not trained), velocities of poses, and intentions\n",
    "        '''\n",
    "        super(glob_LSTM_vel, self).__init__()\n",
    "         \n",
    "        self.pose_encoder_g = nn.LSTM(input_size=2, hidden_size=50)\n",
    "        self.vel_encoder_g = nn.LSTM(input_size=2, hidden_size=50)\n",
    "        self.vel_decoder_g = nn.LSTMCell(input_size=2, hidden_size=50)\n",
    "        self.fc_vel_g    = nn.Linear(in_features=50, out_features=2)\n",
    "        \n",
    "        self.pose_encoder = nn.LSTM(input_size=34, hidden_size=1000,num_layers=3)\n",
    "        self.vel_encoder = nn.LSTM(input_size=34, hidden_size=1000,num_layers=3)\n",
    "        self.vel_decoder = nn.LSTM(input_size=34, hidden_size=1000,num_layers=3)\n",
    "        self.fc_vel    = nn.Linear(in_features=1000, out_features=34)\n",
    "        \n",
    "        self.hardtanh = nn.Hardtanh(min_val=-1*args.hardtanh_limit,max_val=args.hardtanh_limit)\n",
    "        self.relu = nn.ReLU() \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "    def forward(self, pose=None, vel=None, pose_g=None,vel_g=None):\n",
    "        \n",
    "        outputs = []        \n",
    "#         _, (hidden_s,cell_s) = self.vel_encoder(vel.permute(1,0,2))             \n",
    "#         _, (hidden_p,cell_p) = self.pose_encoder(pose.permute(1,0,2))\n",
    "#         hidden_dec = hidden_s + hidden_p\n",
    "#         cell_dec = cell_s + cell_p\n",
    "\n",
    "#         VelDec_inp = vel[:,-1,:].unsqueeze(0)\n",
    "       \n",
    "        vel_outputs = torch.tensor([], device=self.args.device)\n",
    "\n",
    "#         print(VelDec_inp.shape)\n",
    "#         for i in range(self.args.output//self.args.skip):\n",
    "# #             print(hidden_dec[0].shape,hidden_dec[1].shape)\n",
    "#             decoder_output, (hidden_dec,cell_dec) = self.vel_decoder(VelDec_inp, (hidden_dec,cell_dec))\n",
    "#             vel_output  = self.fc_vel(decoder_output)\n",
    "# #             print(vel_output.shape)\n",
    "#             vel_outputs = torch.cat((vel_outputs, vel_output), dim = 0)\n",
    "#             VelDec_inp  = vel_output.detach()\n",
    "            \n",
    "            \n",
    "        outputs.append(vel_outputs)#.permute(1,0,2)\n",
    "   \n",
    "        \n",
    "        _, (hidden_vel_g, cell_vel_g) = self.vel_encoder_g(vel_g.permute(1,0,2))\n",
    "        hidden_vel_g = hidden_vel_g.squeeze(0)\n",
    "        cell_vel_g = cell_vel_g.squeeze(0)\n",
    "\n",
    "        _, (hidden_pose_g, cell_pose_g) = self.pose_encoder_g(pose_g.permute(1,0,2))\n",
    "        hidden_pose_g = hidden_pose_g.squeeze(0)\n",
    "        cell_pose_g = cell_pose_g.squeeze(0)\n",
    "\n",
    "                \n",
    "        vel_outputs_g    = torch.tensor([], device=self.args.device)\n",
    "\n",
    "        VelDec_inp_g = vel_g[:,-1,:]\n",
    "        hidden_dec_g = hidden_pose_g + hidden_vel_g\n",
    "        cell_dec_g = cell_pose_g + cell_vel_g\n",
    "#         print(VelDec_inp_g.shape)\n",
    "        \n",
    "        for i in range(self.args.output//self.args.skip):\n",
    "            hidden_dec_g, cell_dec_g = self.vel_decoder_g(VelDec_inp_g, (hidden_dec_g, cell_dec_g))\n",
    "            vel_output_g  = self.fc_vel_g(hidden_dec_g)\n",
    "            vel_outputs_g = torch.cat((vel_outputs_g, vel_output_g.unsqueeze(1)), dim = 1)\n",
    "            VelDec_inp_g  = vel_output_g.detach()\n",
    "            \n",
    "        outputs.append(vel_outputs_g)\n",
    "\n",
    "\n",
    "            \n",
    "        return tuple(outputs)\n",
    "\n",
    "def speed2pos_glob(preds, glob_pred, obs_p, obs_global) :\n",
    "    pred_pos = torch.zeros(preds.shape[0], preds.shape[1], 34).to('cuda')\n",
    "\n",
    "    current = (obs_p[:,-1,:].reshape(-1,17,2)+ obs_global[:,-1,:].reshape(-1,1,2)).reshape(-1,34)\n",
    "    \n",
    "    for i in range(preds.shape[1]):\n",
    "        pred_pos[:,i,:] = ((current + preds[:,i,:]).reshape(current.size()[0],17,2)+glob_pred[:,i,:].reshape(-1,1,2)).reshape(-1,34)\n",
    "        current = pred_pos[:,i,:]\n",
    "        \n",
    "    for i in range(34):\n",
    "        pred_pos[:,:,i] = torch.min(pred_pos[:,:,i], 1920*torch.ones(pred_pos.shape[0], pred_pos.shape[1], device='cuda'))\n",
    "        pred_pos[:,:,i] = torch.max(pred_pos[:,:,i], torch.zeros(pred_pos.shape[0], pred_pos.shape[1], device='cuda'))\n",
    "        \n",
    "    return pred_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class args():\n",
    "    def __init__(self):\n",
    "        self.jaad_dataset = '/data/smailait-data/JAAD/processed_annotations' #folder containing parsed jaad annotations (used when first time loading data)\n",
    "        self.dtype        = 'train'\n",
    "        self.from_file    = False #read dataset from csv file or reprocess data\n",
    "        self.save         = True\n",
    "        self.file         = '/data/smailait-data/jaad_train_16_16.csv'\n",
    "        self.save_path    = '/data/smailait-data/jaad_train_16_16.csv'\n",
    "        self.model_path    = '/data/smailait-data/models/multitask_pv_lstm_trained.pkl'\n",
    "        self.loader_workers = 1\n",
    "        self.loader_shuffle = True\n",
    "        self.pin_memory     = False\n",
    "        self.image_resize   = [240, 426]\n",
    "        self.device         = 'cuda'\n",
    "        self.batch_size     = 50\n",
    "        self.n_epochs       = 250\n",
    "        self.hidden_size    = 1000\n",
    "        self.hardtanh_limit = 100\n",
    "        self.input  = 16\n",
    "        self.output = 16\n",
    "        self.stride = 16\n",
    "        self.skip   = 1\n",
    "        # self.task   = 'bounding_box-intention'\n",
    "        self.task   = 'pose'\n",
    "        self.use_scenes = False       \n",
    "        self.lr = 0.01\n",
    "        \n",
    "args = args()\n",
    "\n",
    "# net = PV_LSTM(args).to(args.device)\n",
    "net = glob_LSTM_vel(args).to(args.device)#PV_LSTM_DE_op(args).to(args.device)\n",
    "# train = DataLoader.data_loader(args)\n",
    "# args.dtype = 'val'\n",
    "# args.save_path = args.save_path.replace('train', 'val')\n",
    "# args.file = args.file.replace('train', 'val')\n",
    "# val = DataLoader.data_loader(args)\n",
    "\n",
    "# optimizer = optim.Adam(net.parameters(), lr=args.lr)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=15, \n",
    "#                                                  threshold = 1e-8, verbose=True)\n",
    "mse = nn.MSELoss()\n",
    "l1e = nn.L1Loss()\n",
    "bce = nn.BCELoss()\n",
    "train_s_scores = []\n",
    "train_pose_scores=[]\n",
    "val_pose_scores=[]\n",
    "train_c_scores = []\n",
    "val_s_scores   = []\n",
    "val_c_scores   = []\n",
    "train_g_scores = []\n",
    "val_g_scores   = []\n",
    "\n",
    "train_loader=data_loader_DE_op(args,\"train\")\n",
    "val_loader=data_loader_DE_op(args,\"val\" )\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr= 0.1)\n",
    "# optimizer = optim.Adadelta(net.parameters(),lr= 0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=8, \n",
    "                                                 threshold = 1e-12, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "indie-occasions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Training ...\n",
      "e: 0 | loss: 2.59 | val_loss: 4.32 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 33.28 | ade_val_g: 28.59 | fde_train_g: 65.03 | fde_val_g: 54.31\n",
      "e: 1 | loss: 2.16 | val_loss: 4.54 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 25.62 | ade_val_g: 24.37 | fde_train_g: 50.81 | fde_val_g: 44.78\n",
      "e: 2 | loss: 2.03 | val_loss: 3.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 23.54 | ade_val_g: 21.64 | fde_train_g: 46.90 | fde_val_g: 40.32\n",
      "e: 3 | loss: 1.75 | val_loss: 3.24 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 19.10 | ade_val_g: 18.23 | fde_train_g: 37.97 | fde_val_g: 35.17\n",
      "e: 4 | loss: 1.71 | val_loss: 3.66 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 18.46 | ade_val_g: 17.35 | fde_train_g: 37.04 | fde_val_g: 32.71\n",
      "e: 5 | loss: 1.77 | val_loss: 3.28 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 17.72 | ade_val_g: 18.79 | fde_train_g: 37.50 | fde_val_g: 36.54\n",
      "e: 6 | loss: 1.65 | val_loss: 3.65 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 17.21 | ade_val_g: 17.05 | fde_train_g: 34.66 | fde_val_g: 31.78\n",
      "e: 7 | loss: 1.63 | val_loss: 3.16 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 16.89 | ade_val_g: 16.56 | fde_train_g: 33.69 | fde_val_g: 32.47\n",
      "e: 8 | loss: 1.60 | val_loss: 3.57 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 16.53 | ade_val_g: 15.64 | fde_train_g: 33.12 | fde_val_g: 29.43\n",
      "e: 9 | loss: 1.61 | val_loss: 3.61 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 16.82 | ade_val_g: 16.25 | fde_train_g: 33.30 | fde_val_g: 30.50\n",
      "e: 10 | loss: 1.69 | val_loss: 3.16 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 17.77 | ade_val_g: 17.01 | fde_train_g: 35.77 | fde_val_g: 32.75\n",
      "e: 11 | loss: 1.64 | val_loss: 3.06 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 17.23 | ade_val_g: 15.27 | fde_train_g: 34.73 | fde_val_g: 29.90\n",
      "e: 12 | loss: 1.54 | val_loss: 3.53 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.48 | ade_val_g: 15.16 | fde_train_g: 31.15 | fde_val_g: 28.68\n",
      "e: 13 | loss: 1.55 | val_loss: 3.05 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.52 | ade_val_g: 15.06 | fde_train_g: 31.55 | fde_val_g: 29.06\n",
      "e: 14 | loss: 1.54 | val_loss: 3.56 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.34 | ade_val_g: 15.73 | fde_train_g: 31.11 | fde_val_g: 29.41\n",
      "e: 15 | loss: 1.54 | val_loss: 3.10 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.59 | ade_val_g: 15.89 | fde_train_g: 31.20 | fde_val_g: 30.46\n",
      "e: 16 | loss: 1.56 | val_loss: 3.51 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.75 | ade_val_g: 14.92 | fde_train_g: 31.87 | fde_val_g: 28.08\n",
      "e: 17 | loss: 1.52 | val_loss: 3.02 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.05 | ade_val_g: 14.89 | fde_train_g: 30.55 | fde_val_g: 27.88\n",
      "e: 18 | loss: 1.55 | val_loss: 3.53 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.66 | ade_val_g: 14.89 | fde_train_g: 31.59 | fde_val_g: 28.21\n",
      "e: 19 | loss: 1.52 | val_loss: 3.12 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.04 | ade_val_g: 15.92 | fde_train_g: 30.58 | fde_val_g: 31.09\n",
      "e: 20 | loss: 1.53 | val_loss: 3.00 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.37 | ade_val_g: 14.29 | fde_train_g: 31.03 | fde_val_g: 27.44\n",
      "e: 21 | loss: 1.53 | val_loss: 3.01 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.17 | ade_val_g: 14.59 | fde_train_g: 30.94 | fde_val_g: 28.37\n",
      "e: 22 | loss: 1.50 | val_loss: 3.00 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 14.84 | ade_val_g: 14.13 | fde_train_g: 30.28 | fde_val_g: 27.30\n",
      "e: 23 | loss: 1.55 | val_loss: 3.04 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.60 | ade_val_g: 15.00 | fde_train_g: 31.50 | fde_val_g: 28.60\n",
      "e: 24 | loss: 1.50 | val_loss: 3.92 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 14.92 | ade_val_g: 13.87 | fde_train_g: 30.22 | fde_val_g: 25.18\n",
      "e: 25 | loss: 1.58 | val_loss: 3.05 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 14.71 | ade_val_g: 14.86 | fde_train_g: 31.56 | fde_val_g: 28.87\n",
      "e: 26 | loss: 1.56 | val_loss: 3.11 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.84 | ade_val_g: 16.04 | fde_train_g: 31.60 | fde_val_g: 29.97\n",
      "e: 27 | loss: 1.59 | val_loss: 3.10 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 16.20 | ade_val_g: 16.02 | fde_train_g: 32.26 | fde_val_g: 30.80\n",
      "e: 28 | loss: 1.57 | val_loss: 3.02 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.88 | ade_val_g: 14.60 | fde_train_g: 31.55 | fde_val_g: 27.83\n",
      "e: 29 | loss: 1.57 | val_loss: 3.50 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 16.12 | ade_val_g: 14.90 | fde_train_g: 32.14 | fde_val_g: 27.65\n",
      "e: 30 | loss: 1.54 | val_loss: 3.46 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.38 | ade_val_g: 13.85 | fde_train_g: 31.13 | fde_val_g: 25.54\n",
      "Epoch    32: reducing learning rate of group 0 to 5.0000e-02.\n",
      "e: 31 | loss: 1.51 | val_loss: 3.00 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.02 | ade_val_g: 14.25 | fde_train_g: 30.12 | fde_val_g: 27.34\n",
      "e: 32 | loss: 1.54 | val_loss: 3.57 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.38 | ade_val_g: 15.73 | fde_train_g: 30.95 | fde_val_g: 29.48\n",
      "e: 33 | loss: 1.52 | val_loss: 3.01 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 14.93 | ade_val_g: 13.95 | fde_train_g: 30.11 | fde_val_g: 27.19\n",
      "e: 34 | loss: 1.54 | val_loss: 2.97 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.53 | ade_val_g: 13.65 | fde_train_g: 30.45 | fde_val_g: 25.61\n",
      "e: 35 | loss: 1.50 | val_loss: 2.93 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 14.68 | ade_val_g: 13.21 | fde_train_g: 29.50 | fde_val_g: 25.07\n",
      "e: 36 | loss: 1.44 | val_loss: 3.40 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 14.01 | ade_val_g: 13.23 | fde_train_g: 28.04 | fde_val_g: 24.32\n",
      "e: 37 | loss: 1.43 | val_loss: 3.47 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.85 | ade_val_g: 14.40 | fde_train_g: 27.74 | fde_val_g: 26.61\n",
      "e: 38 | loss: 1.49 | val_loss: 2.92 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 14.99 | ade_val_g: 13.10 | fde_train_g: 29.83 | fde_val_g: 24.86\n",
      "e: 39 | loss: 1.43 | val_loss: 3.45 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 14.06 | ade_val_g: 13.85 | fde_train_g: 28.18 | fde_val_g: 25.97\n",
      "e: 40 | loss: 1.43 | val_loss: 2.95 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.79 | ade_val_g: 13.30 | fde_train_g: 27.80 | fde_val_g: 25.72\n",
      "e: 41 | loss: 1.43 | val_loss: 3.92 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.82 | ade_val_g: 13.81 | fde_train_g: 27.80 | fde_val_g: 24.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 42 | loss: 1.41 | val_loss: 2.93 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.35 | ade_val_g: 12.89 | fde_train_g: 27.01 | fde_val_g: 24.91\n",
      "e: 43 | loss: 1.41 | val_loss: 2.98 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.39 | ade_val_g: 13.55 | fde_train_g: 26.83 | fde_val_g: 26.25\n",
      "e: 44 | loss: 1.42 | val_loss: 3.43 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.48 | ade_val_g: 13.49 | fde_train_g: 27.26 | fde_val_g: 25.07\n",
      "e: 45 | loss: 1.39 | val_loss: 2.92 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.24 | ade_val_g: 13.20 | fde_train_g: 26.74 | fde_val_g: 24.98\n",
      "e: 46 | loss: 1.40 | val_loss: 2.89 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.39 | ade_val_g: 12.57 | fde_train_g: 26.99 | fde_val_g: 23.91\n",
      "e: 47 | loss: 1.38 | val_loss: 3.35 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.96 | ade_val_g: 12.55 | fde_train_g: 26.03 | fde_val_g: 22.71\n",
      "e: 48 | loss: 1.43 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.74 | ade_val_g: 12.52 | fde_train_g: 27.59 | fde_val_g: 23.76\n",
      "e: 49 | loss: 1.40 | val_loss: 2.99 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.35 | ade_val_g: 13.68 | fde_train_g: 26.71 | fde_val_g: 26.75\n",
      "e: 50 | loss: 1.39 | val_loss: 3.37 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.11 | ade_val_g: 12.92 | fde_train_g: 26.26 | fde_val_g: 23.16\n",
      "e: 51 | loss: 1.38 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.08 | ade_val_g: 12.78 | fde_train_g: 26.23 | fde_val_g: 23.14\n",
      "e: 52 | loss: 1.39 | val_loss: 2.95 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.05 | ade_val_g: 13.27 | fde_train_g: 26.21 | fde_val_g: 25.80\n",
      "e: 53 | loss: 1.39 | val_loss: 3.40 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.20 | ade_val_g: 13.13 | fde_train_g: 26.41 | fde_val_g: 24.23\n",
      "e: 54 | loss: 1.37 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.88 | ade_val_g: 12.44 | fde_train_g: 25.63 | fde_val_g: 23.34\n",
      "e: 55 | loss: 1.36 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.70 | ade_val_g: 12.26 | fde_train_g: 25.32 | fde_val_g: 22.68\n",
      "e: 56 | loss: 1.37 | val_loss: 3.37 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.79 | ade_val_g: 12.84 | fde_train_g: 25.64 | fde_val_g: 23.54\n",
      "e: 57 | loss: 1.48 | val_loss: 2.95 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 15.19 | ade_val_g: 13.21 | fde_train_g: 28.41 | fde_val_g: 25.79\n",
      "e: 58 | loss: 1.38 | val_loss: 3.38 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.91 | ade_val_g: 12.83 | fde_train_g: 26.00 | fde_val_g: 23.76\n",
      "e: 59 | loss: 1.39 | val_loss: 3.41 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.12 | ade_val_g: 13.18 | fde_train_g: 26.37 | fde_val_g: 24.53\n",
      "e: 60 | loss: 1.38 | val_loss: 3.41 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.15 | ade_val_g: 13.68 | fde_train_g: 26.30 | fde_val_g: 25.16\n",
      "e: 61 | loss: 1.40 | val_loss: 2.91 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.31 | ade_val_g: 13.11 | fde_train_g: 26.68 | fde_val_g: 24.69\n",
      "e: 62 | loss: 1.40 | val_loss: 3.90 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.31 | ade_val_g: 13.34 | fde_train_g: 26.79 | fde_val_g: 23.94\n",
      "e: 63 | loss: 1.36 | val_loss: 3.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.66 | ade_val_g: 12.96 | fde_train_g: 25.25 | fde_val_g: 22.59\n",
      "e: 64 | loss: 1.35 | val_loss: 3.40 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.64 | ade_val_g: 13.14 | fde_train_g: 25.18 | fde_val_g: 24.16\n",
      "e: 65 | loss: 1.37 | val_loss: 3.34 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.78 | ade_val_g: 12.53 | fde_train_g: 25.64 | fde_val_g: 22.35\n",
      "e: 66 | loss: 1.34 | val_loss: 3.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.44 | ade_val_g: 12.80 | fde_train_g: 24.72 | fde_val_g: 22.84\n",
      "e: 67 | loss: 1.35 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.54 | ade_val_g: 12.61 | fde_train_g: 25.18 | fde_val_g: 23.06\n",
      "e: 68 | loss: 1.35 | val_loss: 3.39 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.58 | ade_val_g: 12.99 | fde_train_g: 24.99 | fde_val_g: 24.00\n",
      "e: 69 | loss: 1.35 | val_loss: 3.35 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.55 | ade_val_g: 12.67 | fde_train_g: 25.07 | fde_val_g: 22.62\n",
      "e: 70 | loss: 1.36 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.71 | ade_val_g: 12.74 | fde_train_g: 25.52 | fde_val_g: 22.85\n",
      "e: 71 | loss: 1.35 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.53 | ade_val_g: 12.82 | fde_train_g: 25.05 | fde_val_g: 23.77\n",
      "e: 72 | loss: 1.46 | val_loss: 2.84 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.90 | ade_val_g: 12.02 | fde_train_g: 27.92 | fde_val_g: 22.66\n",
      "e: 73 | loss: 1.45 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.76 | ade_val_g: 12.52 | fde_train_g: 27.31 | fde_val_g: 22.76\n",
      "e: 74 | loss: 1.34 | val_loss: 2.83 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.50 | ade_val_g: 11.83 | fde_train_g: 25.25 | fde_val_g: 22.36\n",
      "Epoch    76: reducing learning rate of group 0 to 2.5000e-02.\n",
      "e: 75 | loss: 1.35 | val_loss: 2.83 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.49 | ade_val_g: 11.86 | fde_train_g: 25.05 | fde_val_g: 22.38\n",
      "e: 76 | loss: 1.34 | val_loss: 3.35 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.32 | ade_val_g: 12.47 | fde_train_g: 24.78 | fde_val_g: 22.91\n",
      "e: 77 | loss: 1.31 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.94 | ade_val_g: 12.35 | fde_train_g: 23.90 | fde_val_g: 23.72\n",
      "e: 78 | loss: 1.32 | val_loss: 3.33 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.09 | ade_val_g: 12.29 | fde_train_g: 24.07 | fde_val_g: 22.28\n",
      "e: 79 | loss: 1.31 | val_loss: 3.33 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.91 | ade_val_g: 12.30 | fde_train_g: 23.81 | fde_val_g: 22.31\n",
      "e: 80 | loss: 1.31 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.79 | ade_val_g: 12.49 | fde_train_g: 23.65 | fde_val_g: 22.94\n",
      "e: 81 | loss: 1.30 | val_loss: 3.31 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.72 | ade_val_g: 12.26 | fde_train_g: 23.38 | fde_val_g: 22.16\n",
      "e: 82 | loss: 1.40 | val_loss: 3.39 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.94 | ade_val_g: 12.86 | fde_train_g: 25.86 | fde_val_g: 23.98\n",
      "e: 83 | loss: 1.30 | val_loss: 2.84 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.70 | ade_val_g: 12.19 | fde_train_g: 23.41 | fde_val_g: 23.04\n",
      "e: 84 | loss: 1.31 | val_loss: 2.89 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.95 | ade_val_g: 12.47 | fde_train_g: 23.84 | fde_val_g: 23.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 85 | loss: 1.31 | val_loss: 2.83 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.80 | ade_val_g: 11.81 | fde_train_g: 23.90 | fde_val_g: 22.04\n",
      "e: 86 | loss: 1.31 | val_loss: 2.83 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.01 | ade_val_g: 11.93 | fde_train_g: 23.90 | fde_val_g: 22.32\n",
      "e: 87 | loss: 1.31 | val_loss: 3.34 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.89 | ade_val_g: 12.51 | fde_train_g: 23.78 | fde_val_g: 22.95\n",
      "e: 88 | loss: 1.29 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.60 | ade_val_g: 12.03 | fde_train_g: 23.16 | fde_val_g: 22.32\n",
      "e: 89 | loss: 1.29 | val_loss: 3.39 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.67 | ade_val_g: 12.90 | fde_train_g: 23.16 | fde_val_g: 24.16\n",
      "e: 90 | loss: 1.29 | val_loss: 2.82 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.63 | ade_val_g: 11.79 | fde_train_g: 23.08 | fde_val_g: 22.26\n",
      "e: 91 | loss: 1.29 | val_loss: 2.82 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.71 | ade_val_g: 11.80 | fde_train_g: 23.30 | fde_val_g: 22.08\n",
      "e: 92 | loss: 1.30 | val_loss: 3.32 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.84 | ade_val_g: 12.27 | fde_train_g: 23.43 | fde_val_g: 22.18\n",
      "e: 93 | loss: 1.39 | val_loss: 2.82 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.89 | ade_val_g: 11.88 | fde_train_g: 25.75 | fde_val_g: 22.35\n",
      "e: 94 | loss: 1.28 | val_loss: 2.83 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.50 | ade_val_g: 11.97 | fde_train_g: 22.81 | fde_val_g: 22.44\n",
      "e: 95 | loss: 1.29 | val_loss: 2.84 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.61 | ade_val_g: 12.08 | fde_train_g: 23.29 | fde_val_g: 22.72\n",
      "e: 96 | loss: 1.39 | val_loss: 2.81 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.79 | ade_val_g: 11.64 | fde_train_g: 25.29 | fde_val_g: 21.78\n",
      "e: 97 | loss: 1.30 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.76 | ade_val_g: 12.10 | fde_train_g: 23.45 | fde_val_g: 23.00\n",
      "e: 98 | loss: 1.29 | val_loss: 2.83 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.61 | ade_val_g: 11.93 | fde_train_g: 23.16 | fde_val_g: 22.37\n",
      "e: 99 | loss: 1.29 | val_loss: 3.35 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.56 | ade_val_g: 12.23 | fde_train_g: 23.19 | fde_val_g: 22.47\n",
      "e: 100 | loss: 1.29 | val_loss: 3.39 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.70 | ade_val_g: 12.95 | fde_train_g: 23.21 | fde_val_g: 24.21\n",
      "e: 101 | loss: 1.29 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.63 | ade_val_g: 12.12 | fde_train_g: 23.18 | fde_val_g: 23.13\n",
      "e: 102 | loss: 1.28 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.55 | ade_val_g: 12.19 | fde_train_g: 22.81 | fde_val_g: 23.52\n",
      "Epoch   104: reducing learning rate of group 0 to 1.2500e-02.\n",
      "e: 103 | loss: 1.37 | val_loss: 3.33 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.62 | ade_val_g: 12.29 | fde_train_g: 25.09 | fde_val_g: 22.23\n",
      "e: 104 | loss: 1.27 | val_loss: 3.33 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.34 | ade_val_g: 12.40 | fde_train_g: 22.52 | fde_val_g: 22.49\n",
      "e: 105 | loss: 1.27 | val_loss: 3.81 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.33 | ade_val_g: 12.26 | fde_train_g: 22.36 | fde_val_g: 21.64\n",
      "e: 106 | loss: 1.26 | val_loss: 2.82 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.14 | ade_val_g: 11.70 | fde_train_g: 22.06 | fde_val_g: 22.03\n",
      "e: 107 | loss: 1.27 | val_loss: 2.81 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.33 | ade_val_g: 11.60 | fde_train_g: 22.49 | fde_val_g: 21.79\n",
      "e: 108 | loss: 1.27 | val_loss: 3.32 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.26 | ade_val_g: 12.11 | fde_train_g: 22.35 | fde_val_g: 21.90\n",
      "e: 109 | loss: 1.27 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.35 | ade_val_g: 12.05 | fde_train_g: 22.41 | fde_val_g: 22.79\n",
      "e: 110 | loss: 1.26 | val_loss: 3.32 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.16 | ade_val_g: 12.13 | fde_train_g: 22.14 | fde_val_g: 22.23\n",
      "e: 111 | loss: 1.26 | val_loss: 2.83 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.22 | ade_val_g: 11.83 | fde_train_g: 22.14 | fde_val_g: 22.20\n",
      "e: 112 | loss: 1.25 | val_loss: 2.82 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.12 | ade_val_g: 11.70 | fde_train_g: 21.98 | fde_val_g: 21.83\n",
      "e: 113 | loss: 1.35 | val_loss: 2.82 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.18 | ade_val_g: 11.82 | fde_train_g: 24.10 | fde_val_g: 22.27\n",
      "e: 114 | loss: 1.26 | val_loss: 2.84 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.20 | ade_val_g: 11.89 | fde_train_g: 22.15 | fde_val_g: 22.49\n",
      "e: 115 | loss: 1.25 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.05 | ade_val_g: 12.56 | fde_train_g: 21.92 | fde_val_g: 23.26\n",
      "e: 116 | loss: 1.25 | val_loss: 3.32 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.04 | ade_val_g: 12.15 | fde_train_g: 21.82 | fde_val_g: 21.89\n",
      "e: 117 | loss: 1.26 | val_loss: 2.82 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.14 | ade_val_g: 11.77 | fde_train_g: 21.97 | fde_val_g: 22.12\n",
      "e: 118 | loss: 1.26 | val_loss: 2.84 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.21 | ade_val_g: 12.12 | fde_train_g: 22.19 | fde_val_g: 22.80\n",
      "e: 119 | loss: 1.25 | val_loss: 2.91 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.01 | ade_val_g: 12.94 | fde_train_g: 21.76 | fde_val_g: 25.28\n",
      "e: 120 | loss: 1.25 | val_loss: 3.34 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.14 | ade_val_g: 12.17 | fde_train_g: 21.81 | fde_val_g: 22.59\n",
      "e: 121 | loss: 1.25 | val_loss: 2.89 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.06 | ade_val_g: 12.50 | fde_train_g: 21.79 | fde_val_g: 23.89\n",
      "e: 122 | loss: 1.25 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.95 | ade_val_g: 12.25 | fde_train_g: 21.66 | fde_val_g: 22.87\n",
      "e: 123 | loss: 1.27 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.37 | ade_val_g: 12.31 | fde_train_g: 22.50 | fde_val_g: 23.06\n",
      "e: 124 | loss: 1.26 | val_loss: 2.90 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.28 | ade_val_g: 12.61 | fde_train_g: 22.27 | fde_val_g: 24.33\n",
      "e: 125 | loss: 1.25 | val_loss: 2.83 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.07 | ade_val_g: 12.00 | fde_train_g: 21.85 | fde_val_g: 22.44\n",
      "e: 126 | loss: 1.24 | val_loss: 2.90 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.96 | ade_val_g: 12.76 | fde_train_g: 21.62 | fde_val_g: 24.39\n",
      "e: 127 | loss: 1.35 | val_loss: 2.84 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.20 | ade_val_g: 12.23 | fde_train_g: 24.05 | fde_val_g: 23.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 128 | loss: 1.25 | val_loss: 2.91 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.04 | ade_val_g: 12.72 | fde_train_g: 21.80 | fde_val_g: 24.76\n",
      "e: 129 | loss: 1.25 | val_loss: 3.35 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.00 | ade_val_g: 12.56 | fde_train_g: 21.82 | fde_val_g: 23.12\n",
      "e: 130 | loss: 1.25 | val_loss: 3.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.01 | ade_val_g: 12.88 | fde_train_g: 21.77 | fde_val_g: 22.99\n",
      "e: 131 | loss: 1.25 | val_loss: 3.39 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.02 | ade_val_g: 12.95 | fde_train_g: 21.85 | fde_val_g: 24.28\n",
      "e: 132 | loss: 1.34 | val_loss: 3.40 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 13.10 | ade_val_g: 13.14 | fde_train_g: 23.88 | fde_val_g: 24.54\n",
      "e: 133 | loss: 1.36 | val_loss: 3.41 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.36 | ade_val_g: 13.16 | fde_train_g: 24.39 | fde_val_g: 24.88\n",
      "e: 134 | loss: 1.36 | val_loss: 3.39 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.35 | ade_val_g: 12.91 | fde_train_g: 24.40 | fde_val_g: 24.12\n",
      "Epoch   136: reducing learning rate of group 0 to 6.2500e-03.\n",
      "e: 135 | loss: 1.25 | val_loss: 2.90 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 11.04 | ade_val_g: 12.83 | fde_train_g: 21.82 | fde_val_g: 24.77\n",
      "e: 136 | loss: 1.25 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.98 | ade_val_g: 12.44 | fde_train_g: 21.74 | fde_val_g: 23.96\n",
      "e: 137 | loss: 1.24 | val_loss: 2.91 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.85 | ade_val_g: 12.60 | fde_train_g: 21.47 | fde_val_g: 24.89\n",
      "e: 138 | loss: 1.25 | val_loss: 3.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.95 | ade_val_g: 12.94 | fde_train_g: 21.66 | fde_val_g: 23.48\n",
      "e: 139 | loss: 1.24 | val_loss: 3.37 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.89 | ade_val_g: 12.73 | fde_train_g: 21.49 | fde_val_g: 23.56\n",
      "e: 140 | loss: 1.24 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.83 | ade_val_g: 12.23 | fde_train_g: 21.45 | fde_val_g: 23.54\n",
      "e: 141 | loss: 1.23 | val_loss: 2.84 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.72 | ade_val_g: 11.93 | fde_train_g: 21.15 | fde_val_g: 22.64\n",
      "e: 142 | loss: 1.24 | val_loss: 2.90 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.82 | ade_val_g: 12.63 | fde_train_g: 21.31 | fde_val_g: 24.60\n",
      "e: 143 | loss: 1.23 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.77 | ade_val_g: 12.59 | fde_train_g: 21.25 | fde_val_g: 23.57\n",
      "e: 144 | loss: 1.25 | val_loss: 2.84 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.93 | ade_val_g: 12.06 | fde_train_g: 21.71 | fde_val_g: 22.85\n",
      "e: 145 | loss: 1.24 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.78 | ade_val_g: 12.21 | fde_train_g: 21.37 | fde_val_g: 23.23\n",
      "e: 146 | loss: 1.24 | val_loss: 3.37 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.73 | ade_val_g: 12.68 | fde_train_g: 21.13 | fde_val_g: 23.34\n",
      "e: 147 | loss: 1.24 | val_loss: 2.92 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.83 | ade_val_g: 13.01 | fde_train_g: 21.34 | fde_val_g: 25.49\n",
      "e: 148 | loss: 1.23 | val_loss: 2.91 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.68 | ade_val_g: 12.68 | fde_train_g: 20.89 | fde_val_g: 24.68\n",
      "e: 149 | loss: 1.23 | val_loss: 2.84 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.74 | ade_val_g: 12.05 | fde_train_g: 21.09 | fde_val_g: 22.74\n",
      "e: 150 | loss: 1.23 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.72 | ade_val_g: 12.21 | fde_train_g: 21.16 | fde_val_g: 23.30\n",
      "e: 151 | loss: 1.23 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.65 | ade_val_g: 12.72 | fde_train_g: 20.93 | fde_val_g: 23.87\n",
      "e: 152 | loss: 1.23 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.68 | ade_val_g: 12.18 | fde_train_g: 21.08 | fde_val_g: 23.16\n",
      "e: 153 | loss: 1.23 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.73 | ade_val_g: 12.32 | fde_train_g: 21.13 | fde_val_g: 23.56\n",
      "e: 154 | loss: 1.23 | val_loss: 2.89 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.74 | ade_val_g: 12.62 | fde_train_g: 21.12 | fde_val_g: 24.50\n",
      "e: 155 | loss: 1.23 | val_loss: 3.90 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.82 | ade_val_g: 13.23 | fde_train_g: 21.27 | fde_val_g: 24.28\n",
      "e: 156 | loss: 1.24 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.84 | ade_val_g: 12.29 | fde_train_g: 21.43 | fde_val_g: 23.38\n",
      "Epoch   158: reducing learning rate of group 0 to 3.1250e-03.\n",
      "e: 157 | loss: 1.23 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.70 | ade_val_g: 12.15 | fde_train_g: 21.08 | fde_val_g: 22.92\n",
      "e: 158 | loss: 1.23 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.82 | ade_val_g: 12.37 | fde_train_g: 21.17 | fde_val_g: 23.63\n",
      "e: 159 | loss: 1.23 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.67 | ade_val_g: 12.51 | fde_train_g: 20.99 | fde_val_g: 24.10\n",
      "e: 160 | loss: 1.23 | val_loss: 2.84 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.68 | ade_val_g: 12.12 | fde_train_g: 21.03 | fde_val_g: 22.82\n",
      "e: 161 | loss: 1.21 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.53 | ade_val_g: 12.18 | fde_train_g: 20.62 | fde_val_g: 23.24\n",
      "e: 162 | loss: 1.22 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.61 | ade_val_g: 12.48 | fde_train_g: 20.83 | fde_val_g: 24.14\n",
      "e: 163 | loss: 1.22 | val_loss: 2.89 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.56 | ade_val_g: 12.71 | fde_train_g: 20.77 | fde_val_g: 24.48\n",
      "e: 164 | loss: 1.31 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.73 | ade_val_g: 12.47 | fde_train_g: 23.07 | fde_val_g: 24.00\n",
      "e: 165 | loss: 1.24 | val_loss: 3.34 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.82 | ade_val_g: 12.42 | fde_train_g: 21.31 | fde_val_g: 22.94\n",
      "e: 166 | loss: 1.21 | val_loss: 3.40 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.53 | ade_val_g: 13.11 | fde_train_g: 20.64 | fde_val_g: 24.60\n",
      "e: 167 | loss: 1.23 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.65 | ade_val_g: 12.36 | fde_train_g: 20.93 | fde_val_g: 23.88\n",
      "e: 168 | loss: 1.22 | val_loss: 3.93 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.60 | ade_val_g: 13.71 | fde_train_g: 20.76 | fde_val_g: 25.61\n",
      "e: 169 | loss: 1.32 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.81 | ade_val_g: 12.47 | fde_train_g: 23.06 | fde_val_g: 23.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   171: reducing learning rate of group 0 to 1.5625e-03.\n",
      "e: 170 | loss: 1.22 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.57 | ade_val_g: 12.67 | fde_train_g: 20.81 | fde_val_g: 24.36\n",
      "e: 171 | loss: 1.22 | val_loss: 3.35 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.58 | ade_val_g: 12.61 | fde_train_g: 20.72 | fde_val_g: 23.20\n",
      "e: 172 | loss: 1.23 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.66 | ade_val_g: 12.45 | fde_train_g: 20.94 | fde_val_g: 23.72\n",
      "e: 173 | loss: 1.22 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.62 | ade_val_g: 12.61 | fde_train_g: 20.83 | fde_val_g: 23.35\n",
      "e: 174 | loss: 1.22 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.61 | ade_val_g: 12.24 | fde_train_g: 20.86 | fde_val_g: 23.46\n",
      "e: 175 | loss: 1.22 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.50 | ade_val_g: 12.75 | fde_train_g: 20.66 | fde_val_g: 23.66\n",
      "e: 176 | loss: 1.22 | val_loss: 3.35 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.55 | ade_val_g: 12.52 | fde_train_g: 20.72 | fde_val_g: 23.35\n",
      "e: 177 | loss: 1.22 | val_loss: 2.90 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.55 | ade_val_g: 12.69 | fde_train_g: 20.79 | fde_val_g: 24.43\n",
      "e: 178 | loss: 1.22 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.60 | ade_val_g: 12.35 | fde_train_g: 20.83 | fde_val_g: 23.49\n",
      "Epoch   180: reducing learning rate of group 0 to 7.8125e-04.\n",
      "e: 179 | loss: 1.22 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.57 | ade_val_g: 12.72 | fde_train_g: 20.82 | fde_val_g: 23.50\n",
      "e: 180 | loss: 1.22 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.61 | ade_val_g: 12.36 | fde_train_g: 20.80 | fde_val_g: 23.89\n",
      "e: 181 | loss: 1.22 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.60 | ade_val_g: 12.05 | fde_train_g: 20.77 | fde_val_g: 23.28\n",
      "e: 182 | loss: 1.22 | val_loss: 3.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.53 | ade_val_g: 13.06 | fde_train_g: 20.64 | fde_val_g: 23.50\n",
      "e: 183 | loss: 1.22 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.57 | ade_val_g: 12.21 | fde_train_g: 20.72 | fde_val_g: 23.55\n",
      "e: 184 | loss: 1.22 | val_loss: 2.93 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.58 | ade_val_g: 13.07 | fde_train_g: 20.73 | fde_val_g: 25.52\n",
      "e: 185 | loss: 1.23 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.68 | ade_val_g: 12.43 | fde_train_g: 21.03 | fde_val_g: 23.81\n",
      "e: 186 | loss: 1.22 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.61 | ade_val_g: 12.35 | fde_train_g: 20.90 | fde_val_g: 23.61\n",
      "e: 187 | loss: 1.22 | val_loss: 3.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.56 | ade_val_g: 12.87 | fde_train_g: 20.71 | fde_val_g: 23.09\n",
      "Epoch   189: reducing learning rate of group 0 to 3.9063e-04.\n",
      "e: 188 | loss: 1.22 | val_loss: 3.42 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.55 | ade_val_g: 13.31 | fde_train_g: 20.59 | fde_val_g: 25.22\n",
      "e: 189 | loss: 1.22 | val_loss: 3.37 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.59 | ade_val_g: 12.68 | fde_train_g: 20.80 | fde_val_g: 23.72\n",
      "e: 190 | loss: 1.21 | val_loss: 3.38 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.51 | ade_val_g: 12.90 | fde_train_g: 20.61 | fde_val_g: 24.06\n",
      "e: 191 | loss: 1.22 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.55 | ade_val_g: 12.46 | fde_train_g: 20.69 | fde_val_g: 23.75\n",
      "e: 192 | loss: 1.22 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.53 | ade_val_g: 12.57 | fde_train_g: 20.70 | fde_val_g: 24.00\n",
      "e: 193 | loss: 1.23 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.64 | ade_val_g: 12.09 | fde_train_g: 21.05 | fde_val_g: 23.38\n",
      "e: 194 | loss: 1.21 | val_loss: 3.37 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.53 | ade_val_g: 12.75 | fde_train_g: 20.60 | fde_val_g: 23.56\n",
      "e: 195 | loss: 1.21 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.55 | ade_val_g: 12.56 | fde_train_g: 20.71 | fde_val_g: 24.17\n",
      "e: 196 | loss: 1.21 | val_loss: 2.89 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.49 | ade_val_g: 12.45 | fde_train_g: 20.62 | fde_val_g: 23.95\n",
      "e: 197 | loss: 1.22 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.62 | ade_val_g: 12.61 | fde_train_g: 20.89 | fde_val_g: 23.59\n",
      "e: 198 | loss: 1.22 | val_loss: 2.94 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.54 | ade_val_g: 13.33 | fde_train_g: 20.59 | fde_val_g: 26.19\n",
      "e: 199 | loss: 1.31 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.71 | ade_val_g: 12.20 | fde_train_g: 22.95 | fde_val_g: 23.26\n",
      "e: 200 | loss: 1.22 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.66 | ade_val_g: 12.12 | fde_train_g: 21.01 | fde_val_g: 23.21\n",
      "e: 201 | loss: 1.21 | val_loss: 3.41 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.54 | ade_val_g: 13.13 | fde_train_g: 20.68 | fde_val_g: 24.89\n",
      "e: 202 | loss: 1.30 | val_loss: 3.90 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.57 | ade_val_g: 13.21 | fde_train_g: 22.54 | fde_val_g: 24.31\n",
      "e: 203 | loss: 1.21 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.47 | ade_val_g: 12.36 | fde_train_g: 20.50 | fde_val_g: 23.72\n",
      "e: 204 | loss: 1.22 | val_loss: 3.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.57 | ade_val_g: 13.04 | fde_train_g: 20.78 | fde_val_g: 23.35\n",
      "Epoch   206: reducing learning rate of group 0 to 1.9531e-04.\n",
      "e: 205 | loss: 1.22 | val_loss: 2.89 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.55 | ade_val_g: 12.54 | fde_train_g: 20.72 | fde_val_g: 24.35\n",
      "e: 206 | loss: 1.21 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.52 | ade_val_g: 12.55 | fde_train_g: 20.57 | fde_val_g: 24.02\n",
      "e: 207 | loss: 1.22 | val_loss: 2.90 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.61 | ade_val_g: 12.70 | fde_train_g: 20.86 | fde_val_g: 24.75\n",
      "e: 208 | loss: 1.21 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.47 | ade_val_g: 12.31 | fde_train_g: 20.50 | fde_val_g: 23.62\n",
      "e: 209 | loss: 1.31 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.64 | ade_val_g: 12.48 | fde_train_g: 22.81 | fde_val_g: 24.06\n",
      "e: 210 | loss: 1.22 | val_loss: 2.94 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.53 | ade_val_g: 13.11 | fde_train_g: 20.66 | fde_val_g: 25.72\n",
      "e: 211 | loss: 1.21 | val_loss: 2.94 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.47 | ade_val_g: 13.12 | fde_train_g: 20.44 | fde_val_g: 25.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 212 | loss: 1.30 | val_loss: 3.35 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.58 | ade_val_g: 12.66 | fde_train_g: 22.60 | fde_val_g: 23.37\n",
      "e: 213 | loss: 1.21 | val_loss: 3.37 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.48 | ade_val_g: 12.78 | fde_train_g: 20.51 | fde_val_g: 23.58\n",
      "e: 214 | loss: 1.22 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.51 | ade_val_g: 12.26 | fde_train_g: 20.67 | fde_val_g: 23.55\n",
      "e: 215 | loss: 1.21 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.51 | ade_val_g: 12.27 | fde_train_g: 20.57 | fde_val_g: 23.42\n",
      "e: 216 | loss: 1.21 | val_loss: 2.96 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.46 | ade_val_g: 13.44 | fde_train_g: 20.48 | fde_val_g: 26.53\n",
      "e: 217 | loss: 1.21 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.52 | ade_val_g: 12.30 | fde_train_g: 20.57 | fde_val_g: 23.65\n",
      "e: 218 | loss: 1.21 | val_loss: 2.91 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.51 | ade_val_g: 12.68 | fde_train_g: 20.53 | fde_val_g: 24.90\n",
      "e: 219 | loss: 1.21 | val_loss: 2.89 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.54 | ade_val_g: 12.50 | fde_train_g: 20.71 | fde_val_g: 24.18\n",
      "Epoch   221: reducing learning rate of group 0 to 9.7656e-05.\n",
      "e: 220 | loss: 1.30 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.62 | ade_val_g: 12.46 | fde_train_g: 22.67 | fde_val_g: 23.90\n",
      "e: 221 | loss: 1.31 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.74 | ade_val_g: 12.70 | fde_train_g: 22.85 | fde_val_g: 23.59\n",
      "e: 222 | loss: 1.21 | val_loss: 2.91 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.52 | ade_val_g: 12.77 | fde_train_g: 20.65 | fde_val_g: 24.81\n",
      "e: 223 | loss: 1.30 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.57 | ade_val_g: 12.59 | fde_train_g: 22.59 | fde_val_g: 23.14\n",
      "e: 224 | loss: 1.21 | val_loss: 2.90 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.52 | ade_val_g: 12.64 | fde_train_g: 20.68 | fde_val_g: 24.54\n",
      "e: 225 | loss: 1.23 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.66 | ade_val_g: 12.36 | fde_train_g: 21.00 | fde_val_g: 23.59\n",
      "e: 226 | loss: 1.23 | val_loss: 3.35 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.72 | ade_val_g: 12.56 | fde_train_g: 21.20 | fde_val_g: 23.48\n",
      "e: 227 | loss: 1.21 | val_loss: 3.35 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.53 | ade_val_g: 12.49 | fde_train_g: 20.61 | fde_val_g: 23.20\n",
      "e: 228 | loss: 1.21 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.45 | ade_val_g: 12.74 | fde_train_g: 20.49 | fde_val_g: 23.70\n",
      "Epoch   230: reducing learning rate of group 0 to 4.8828e-05.\n",
      "e: 229 | loss: 1.30 | val_loss: 3.37 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.57 | ade_val_g: 12.71 | fde_train_g: 22.61 | fde_val_g: 23.77\n",
      "e: 230 | loss: 1.30 | val_loss: 3.89 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.61 | ade_val_g: 13.27 | fde_train_g: 22.62 | fde_val_g: 24.35\n",
      "e: 231 | loss: 1.21 | val_loss: 3.37 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.49 | ade_val_g: 12.60 | fde_train_g: 20.51 | fde_val_g: 23.25\n",
      "e: 232 | loss: 1.30 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.60 | ade_val_g: 12.24 | fde_train_g: 22.61 | fde_val_g: 23.66\n",
      "e: 233 | loss: 1.21 | val_loss: 3.35 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.48 | ade_val_g: 12.49 | fde_train_g: 20.56 | fde_val_g: 23.21\n",
      "e: 234 | loss: 1.21 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.55 | ade_val_g: 12.67 | fde_train_g: 20.69 | fde_val_g: 23.51\n",
      "e: 235 | loss: 1.22 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.58 | ade_val_g: 12.36 | fde_train_g: 20.88 | fde_val_g: 23.69\n",
      "e: 236 | loss: 1.22 | val_loss: 3.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.54 | ade_val_g: 12.86 | fde_train_g: 20.69 | fde_val_g: 23.16\n",
      "e: 237 | loss: 1.21 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.54 | ade_val_g: 12.30 | fde_train_g: 20.61 | fde_val_g: 23.66\n",
      "Epoch   239: reducing learning rate of group 0 to 2.4414e-05.\n",
      "e: 238 | loss: 1.22 | val_loss: 3.89 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.51 | ade_val_g: 13.32 | fde_train_g: 20.63 | fde_val_g: 24.45\n",
      "e: 239 | loss: 1.21 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.49 | ade_val_g: 12.41 | fde_train_g: 20.52 | fde_val_g: 23.88\n",
      "e: 240 | loss: 1.22 | val_loss: 3.37 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.57 | ade_val_g: 12.72 | fde_train_g: 20.75 | fde_val_g: 23.69\n",
      "e: 241 | loss: 1.21 | val_loss: 2.90 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.45 | ade_val_g: 12.65 | fde_train_g: 20.49 | fde_val_g: 24.34\n",
      "e: 242 | loss: 1.30 | val_loss: 2.83 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.67 | ade_val_g: 12.04 | fde_train_g: 22.66 | fde_val_g: 22.99\n",
      "e: 243 | loss: 1.21 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.52 | ade_val_g: 12.33 | fde_train_g: 20.57 | fde_val_g: 23.53\n",
      "e: 244 | loss: 1.20 | val_loss: 4.40 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.41 | ade_val_g: 13.88 | fde_train_g: 20.34 | fde_val_g: 24.72\n",
      "e: 245 | loss: 1.22 | val_loss: 2.89 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.54 | ade_val_g: 12.65 | fde_train_g: 20.66 | fde_val_g: 24.48\n",
      "e: 246 | loss: 1.22 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.53 | ade_val_g: 12.48 | fde_train_g: 20.67 | fde_val_g: 24.01\n",
      "e: 247 | loss: 1.22 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.60 | ade_val_g: 12.71 | fde_train_g: 20.79 | fde_val_g: 23.72\n",
      "e: 248 | loss: 1.22 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.58 | ade_val_g: 12.36 | fde_train_g: 20.93 | fde_val_g: 23.81\n",
      "e: 249 | loss: 1.23 | val_loss: 3.40 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.66 | ade_val_g: 12.97 | fde_train_g: 20.95 | fde_val_g: 24.55\n",
      "e: 250 | loss: 1.22 | val_loss: 3.37 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.55 | ade_val_g: 12.76 | fde_train_g: 20.66 | fde_val_g: 23.91\n",
      "e: 251 | loss: 1.31 | val_loss: 2.90 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.57 | ade_val_g: 12.78 | fde_train_g: 22.65 | fde_val_g: 24.77\n",
      "e: 252 | loss: 1.22 | val_loss: 2.91 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.51 | ade_val_g: 12.82 | fde_train_g: 20.59 | fde_val_g: 24.81\n",
      "Epoch   254: reducing learning rate of group 0 to 1.2207e-05.\n",
      "e: 253 | loss: 1.22 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.56 | ade_val_g: 12.27 | fde_train_g: 20.71 | fde_val_g: 23.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 254 | loss: 1.22 | val_loss: 3.37 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.59 | ade_val_g: 12.70 | fde_train_g: 20.73 | fde_val_g: 23.61\n",
      "e: 255 | loss: 1.22 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.58 | ade_val_g: 12.17 | fde_train_g: 20.66 | fde_val_g: 23.25\n",
      "e: 256 | loss: 1.22 | val_loss: 3.38 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.51 | ade_val_g: 12.82 | fde_train_g: 20.57 | fde_val_g: 23.94\n",
      "e: 257 | loss: 1.31 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.64 | ade_val_g: 12.37 | fde_train_g: 22.87 | fde_val_g: 23.62\n",
      "e: 258 | loss: 1.21 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.45 | ade_val_g: 12.44 | fde_train_g: 20.45 | fde_val_g: 23.44\n",
      "e: 259 | loss: 1.20 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.43 | ade_val_g: 12.58 | fde_train_g: 20.43 | fde_val_g: 23.25\n",
      "e: 260 | loss: 1.21 | val_loss: 2.92 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.52 | ade_val_g: 12.78 | fde_train_g: 20.58 | fde_val_g: 25.03\n",
      "e: 261 | loss: 1.21 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.46 | ade_val_g: 12.49 | fde_train_g: 20.52 | fde_val_g: 24.08\n",
      "Epoch   263: reducing learning rate of group 0 to 6.1035e-06.\n",
      "e: 262 | loss: 1.21 | val_loss: 2.84 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.51 | ade_val_g: 11.98 | fde_train_g: 20.58 | fde_val_g: 23.05\n",
      "e: 263 | loss: 1.22 | val_loss: 3.89 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.58 | ade_val_g: 13.39 | fde_train_g: 20.73 | fde_val_g: 24.45\n",
      "e: 264 | loss: 1.22 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.55 | ade_val_g: 12.48 | fde_train_g: 20.69 | fde_val_g: 23.99\n",
      "e: 265 | loss: 1.21 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.51 | ade_val_g: 12.33 | fde_train_g: 20.56 | fde_val_g: 23.83\n",
      "e: 266 | loss: 1.22 | val_loss: 2.90 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.61 | ade_val_g: 12.63 | fde_train_g: 20.94 | fde_val_g: 24.59\n",
      "e: 267 | loss: 1.22 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.52 | ade_val_g: 12.50 | fde_train_g: 20.72 | fde_val_g: 23.77\n",
      "e: 268 | loss: 1.22 | val_loss: 3.35 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.60 | ade_val_g: 12.60 | fde_train_g: 20.82 | fde_val_g: 23.39\n",
      "e: 269 | loss: 1.21 | val_loss: 3.39 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.51 | ade_val_g: 12.87 | fde_train_g: 20.56 | fde_val_g: 24.26\n",
      "e: 270 | loss: 1.21 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.48 | ade_val_g: 12.30 | fde_train_g: 20.49 | fde_val_g: 23.85\n",
      "Epoch   272: reducing learning rate of group 0 to 3.0518e-06.\n",
      "e: 271 | loss: 1.21 | val_loss: 2.89 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.50 | ade_val_g: 12.57 | fde_train_g: 20.56 | fde_val_g: 24.13\n",
      "e: 272 | loss: 1.22 | val_loss: 3.43 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.61 | ade_val_g: 13.22 | fde_train_g: 20.85 | fde_val_g: 25.16\n",
      "e: 273 | loss: 1.22 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.57 | ade_val_g: 12.42 | fde_train_g: 20.75 | fde_val_g: 23.89\n",
      "e: 274 | loss: 1.22 | val_loss: 2.89 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.56 | ade_val_g: 12.51 | fde_train_g: 20.72 | fde_val_g: 24.26\n",
      "e: 275 | loss: 1.21 | val_loss: 3.41 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.52 | ade_val_g: 13.10 | fde_train_g: 20.65 | fde_val_g: 24.76\n",
      "e: 276 | loss: 1.22 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.54 | ade_val_g: 12.48 | fde_train_g: 20.74 | fde_val_g: 23.83\n",
      "e: 277 | loss: 1.22 | val_loss: 2.90 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.64 | ade_val_g: 12.69 | fde_train_g: 20.86 | fde_val_g: 24.62\n",
      "e: 278 | loss: 1.22 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.60 | ade_val_g: 12.60 | fde_train_g: 20.84 | fde_val_g: 23.43\n",
      "e: 279 | loss: 1.21 | val_loss: 2.90 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.47 | ade_val_g: 12.61 | fde_train_g: 20.56 | fde_val_g: 24.69\n",
      "Epoch   281: reducing learning rate of group 0 to 1.5259e-06.\n",
      "e: 280 | loss: 1.21 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.47 | ade_val_g: 12.39 | fde_train_g: 20.54 | fde_val_g: 24.05\n",
      "e: 281 | loss: 1.21 | val_loss: 3.34 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.49 | ade_val_g: 12.50 | fde_train_g: 20.60 | fde_val_g: 22.99\n",
      "e: 282 | loss: 1.21 | val_loss: 2.87 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.51 | ade_val_g: 12.39 | fde_train_g: 20.65 | fde_val_g: 23.79\n",
      "e: 283 | loss: 1.21 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.46 | ade_val_g: 12.14 | fde_train_g: 20.49 | fde_val_g: 23.07\n",
      "e: 284 | loss: 1.31 | val_loss: 3.37 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 12.65 | ade_val_g: 12.75 | fde_train_g: 22.72 | fde_val_g: 23.82\n",
      "e: 285 | loss: 1.22 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.57 | ade_val_g: 12.46 | fde_train_g: 20.68 | fde_val_g: 24.06\n",
      "e: 286 | loss: 1.22 | val_loss: 2.93 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.60 | ade_val_g: 13.02 | fde_train_g: 20.80 | fde_val_g: 25.54\n",
      "e: 287 | loss: 1.21 | val_loss: 3.36 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.50 | ade_val_g: 12.59 | fde_train_g: 20.53 | fde_val_g: 23.47\n",
      "e: 288 | loss: 1.21 | val_loss: 2.93 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.54 | ade_val_g: 12.83 | fde_train_g: 20.54 | fde_val_g: 25.22\n",
      "Epoch   290: reducing learning rate of group 0 to 7.6294e-07.\n",
      "e: 289 | loss: 1.21 | val_loss: 2.92 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.55 | ade_val_g: 12.85 | fde_train_g: 20.63 | fde_val_g: 25.16\n",
      "e: 290 | loss: 1.21 | val_loss: 3.35 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.51 | ade_val_g: 12.52 | fde_train_g: 20.59 | fde_val_g: 23.24\n",
      "e: 291 | loss: 1.22 | val_loss: 3.41 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.57 | ade_val_g: 13.08 | fde_train_g: 20.70 | fde_val_g: 24.78\n",
      "e: 292 | loss: 1.21 | val_loss: 2.88 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.52 | ade_val_g: 12.49 | fde_train_g: 20.59 | fde_val_g: 24.06\n",
      "e: 293 | loss: 1.21 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.55 | ade_val_g: 12.15 | fde_train_g: 20.57 | fde_val_g: 23.10\n",
      "e: 294 | loss: 1.22 | val_loss: 2.91 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.57 | ade_val_g: 12.69 | fde_train_g: 20.79 | fde_val_g: 24.42\n",
      "e: 295 | loss: 1.22 | val_loss: 2.85 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.56 | ade_val_g: 12.15 | fde_train_g: 20.83 | fde_val_g: 23.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 296 | loss: 1.21 | val_loss: 2.89 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.43 | ade_val_g: 12.57 | fde_train_g: 20.42 | fde_val_g: 24.51\n",
      "e: 297 | loss: 1.22 | val_loss: 2.86 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.60 | ade_val_g: 12.39 | fde_train_g: 20.83 | fde_val_g: 23.80\n",
      "Epoch   299: reducing learning rate of group 0 to 3.8147e-07.\n",
      "e: 298 | loss: 1.22 | val_loss: 3.39 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.63 | ade_val_g: 12.89 | fde_train_g: 20.89 | fde_val_g: 24.30\n",
      "e: 299 | loss: 1.31 | val_loss: 3.40 | ade_train_l: 0.00 | ade_val_l: 0.00 | fde_train_l: 0.00 | fde_val_l: 0.00 | ade_train_g: 10.69 | ade_val_g: 13.17 | fde_train_g: 22.77 | fde_val_g: 24.53\n",
      "====================================================================================================\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "print('='*100)\n",
    "print('Training ...')\n",
    "\n",
    "train_p_scores=[]\n",
    "val_p_scores=[]\n",
    "alpha=1#0.4\n",
    "\n",
    "for epoch in range(300):\n",
    "    start = time.time()\n",
    "    \n",
    "    avg_epoch_train_p_loss   = 0\n",
    "    avg_epoch_val_p_loss     = 0 \n",
    "    ade  = 0\n",
    "    fde  = 0\n",
    "    ade_train  = 0\n",
    "    fde_train  = 0\n",
    "    ade_g  = 0\n",
    "    fde_g  = 0\n",
    "    ade_train_g  = 0\n",
    "    fde_train_g  = 0\n",
    "    counter = 0\n",
    "    net.train()\n",
    "    \n",
    "    for idx, (obs_s, target_s, obs_pose, target_pose, obs_pose_global, target_pose_global, obs_s_g, target_s_g, target) in enumerate(train_loader):\n",
    "#         print(\"SAMPLE\",idx,\".\"*50)\n",
    "#         obs_s = obs_s.to(device='cuda')\n",
    "#         target_s = target_s.to(device='cuda')\n",
    "#         obs_pose    = obs_pose.to(device='cuda')\n",
    "#         target_pose = target_pose.to(device='cuda')\n",
    "        obs_pose_global = obs_pose_global.to(device='cuda')\n",
    "        target_pose_global = target_pose_global.to(device='cuda')\n",
    "        obs_s_g  = obs_s_g.to(device='cuda')\n",
    "        target_s_g = target_s_g.to(device='cuda')\n",
    "#         target = target.to(device='cuda')   \n",
    "#         print(obs_pose_global.shape)\n",
    "#         print(obs_s_g.shape)\n",
    "        counter += 1        \n",
    "#         break\n",
    "        \n",
    "        \n",
    "        net.zero_grad()\n",
    "    \n",
    "        (preds_s,preds_s_g) = net(pose_g=obs_pose_global, vel_g=obs_s_g,pose=obs_pose, vel=obs_s)\n",
    "        \n",
    "#         print(preds_s.shape,preds_s_g.shape)\n",
    "#         pose_preds=speed2pos(preds_s,obs_pose)\n",
    "        pose_preds_g=speed2pos(preds_s_g,obs_pose_global)\n",
    "    \n",
    "#         ade_train += float(ADE_c(pose_preds, target_pose))\n",
    "#         fde_train += float(FDE_c(pose_preds, target_pose))\n",
    "        ade_train_g += float(ADE_c(pose_preds_g, target_pose_global))\n",
    "        fde_train_g += float(FDE_c(pose_preds_g, target_pose_global))\n",
    "        \n",
    "        #speed2pos_glob(preds, glob_pred, obs_p, obs_global)\n",
    "        \n",
    "#         loss_l  = l1e(preds_s, target_s)\n",
    "        loss_g  = l1e(preds_s_g, target_s_g)\n",
    "        \n",
    "        loss=alpha*loss_g# + (1-alpha)*loss_l\n",
    "        \n",
    "\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "        avg_epoch_train_p_loss += float(loss)\n",
    "\n",
    "    avg_epoch_train_p_loss /= counter\n",
    "    train_p_scores.append(avg_epoch_train_p_loss)\n",
    "    ade_train  /= counter\n",
    "    fde_train  /= counter   \n",
    "    ade_train_g  /= counter\n",
    "    fde_train_g  /= counter   \n",
    "    \n",
    "#     break\n",
    "  \n",
    "    counter=0\n",
    "    net.eval()\n",
    "    for idx, (obs_s, target_s, obs_pose, target_pose, obs_pose_global, target_pose_global, obs_s_g, target_s_g, target) in enumerate(val_loader):\n",
    "#         print(\"SAMPLE\",idx,\".\"*50)\n",
    "#         obs_s = obs_s.to(device='cuda')\n",
    "#         target_s = target_s.to(device='cuda')\n",
    "#         obs_pose    = obs_pose.to(device='cuda')\n",
    "#         target_pose = target_pose.to(device='cuda')\n",
    "        obs_pose_global = obs_pose_global.to(device='cuda')\n",
    "        target_pose_global = target_pose_global.to(device='cuda')\n",
    "        obs_s_g  = obs_s_g.to(device='cuda')\n",
    "        target_s_g = target_s_g.to(device='cuda')\n",
    "#         target = target.to(device='cuda')   \n",
    "        \n",
    "        counter += 1        \n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            (preds_s,preds_s_g) = net(pose_g=obs_pose_global, vel_g=obs_s_g,pose=obs_pose, vel=obs_s)\n",
    "\n",
    "#             loss_l  = l1e(preds_s, target_s)\n",
    "            loss_g  = l1e(preds_s_g, target_s_g)\n",
    "        \n",
    "            val_loss=alpha*loss_g #+ (1-alpha)*loss_l\n",
    "            \n",
    "            avg_epoch_val_p_loss += float(val_loss)\n",
    "            \n",
    "            \n",
    "#             pose_preds=speed2pos(preds_s,obs_pose)\n",
    "            pose_preds_g=speed2pos(preds_s_g,obs_pose_global)\n",
    "            \n",
    "#             ade += float(ADE_c(pose_preds, target_pose))\n",
    "#             fde += float(FDE_c(pose_preds, target_pose))\n",
    "            ade_g += float(ADE_c(pose_preds_g, target_pose_global))\n",
    "            fde_g += float(FDE_c(pose_preds_g, target_pose_global))\n",
    "        \n",
    "        \n",
    "    avg_epoch_val_p_loss /= counter\n",
    "    val_p_scores.append(avg_epoch_val_p_loss)\n",
    "    \n",
    "    ade  /= counter\n",
    "    fde  /= counter    \n",
    "    ade_g  /= counter\n",
    "    fde_g  /= counter    \n",
    "   \n",
    "    \n",
    "    \n",
    "    scheduler.step(avg_epoch_train_p_loss)\n",
    "    \n",
    "    print('e:', epoch,'| loss: %.2f'%avg_epoch_train_p_loss,'| val_loss: %.2f'% avg_epoch_val_p_loss, '| ade_train_l: %.2f'% ade_train, '| ade_val_l: %.2f'% ade, '| fde_train_l: %.2f'% fde_train,'| fde_val_l: %.2f'% fde, '| ade_train_g: %.2f'% ade_train_g, '| ade_val_g: %.2f'% ade_g, '| fde_train_g: %.2f'% fde_train_g,'| fde_val_g: %.2f'% fde_g)\n",
    "\n",
    "\n",
    "print('='*100) \n",
    "# print('Saving ...')\n",
    "# torch.save(net.state_dict(), args.model_path)\n",
    "print('Done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-formula",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "sophisticated-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class myDataset_DE_depth(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, dtype):\n",
    "        \n",
    "        self.args = args\n",
    "        self.dtype = dtype\n",
    "        print(\"Loading\",self.dtype)\n",
    "        \n",
    "        sequence_centric = pd.read_csv(\"sequences_openpifpaf_thres4_wimage_\"+self.dtype+\".csv\")\n",
    "#         sequences_openpifpaf_thres4_wconfsc_wimage_train\n",
    "#         sequence_centric = pd.read_csv(\"sequences_16_overlap_4_thres4_\"+self.dtype+\".csv\")\n",
    "\n",
    "        df = sequence_centric.copy()      \n",
    "        for v in list(df.columns.values):\n",
    "            print(v+' loaded')\n",
    "            try:\n",
    "                df.loc[:,v] = df.loc[:, v].apply(lambda x: literal_eval(x))\n",
    "            except:\n",
    "                continue\n",
    "        sequence_centric[df.columns] = df[df.columns]\n",
    "        self.data = sequence_centric.copy().reset_index(drop=True)\n",
    "        \n",
    "        print('*'*30)\n",
    "        \n",
    "        self.obs={}\n",
    "        self.true={}\n",
    "        self.obs_act={}\n",
    "        self.true_act={}\n",
    "        \n",
    "# #         \n",
    "#             self.obs.append(obs_global)\n",
    "#             self.true.append(true_global)\n",
    "            \n",
    "# #         \n",
    "#         self.obs=np.array(self.obs)\n",
    "#         self.true=np.array(self.true)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "#         print(index)\n",
    "        \n",
    "        outputs=[]\n",
    "            \n",
    "        if(index in self.obs):\n",
    "#             print(\"here\")\n",
    "            obs=torch.tensor(self.obs[index],dtype=torch.double)\n",
    "            true=torch.tensor(self.true[index],dtype=torch.double)\n",
    "            obs_act=torch.tensor(self.obs_act[index],dtype=torch.double)\n",
    "            true_act=torch.tensor(self.true_act[index],dtype=torch.double)\n",
    "    \n",
    "            outputs.append(obs)\n",
    "            outputs.append(true)\n",
    "            outputs.append(obs_act)\n",
    "            outputs.append(true_act)\n",
    "        \n",
    "        else:\n",
    "#             print(\"2nd\")\n",
    "            seq = self.data.iloc[index]\n",
    "            outputs = []        \n",
    "            obs_p=[]\n",
    "            depth=[]\n",
    "            obs_act=[]\n",
    "            true_p=[]\n",
    "            true_act=[]\n",
    "            w=0\n",
    "            h=0\n",
    "            egomotion=np.loadtxt(\"./egomotion/\"+seq.Pose_image[0].replace('/frames/','').replace('/','-').replace('.png','')+'_to_'+seq.Pose_image[-1].replace(\"/frames/\",\"\").replace('/','-').replace('.png','.txt'))\n",
    "            for i in range(0,self.args.input,self.args.skip):\n",
    "                image = seq.Pose_image[i].replace(\"/frames\",\"./frames\")\n",
    "                depth_image=image.replace(\"frames\",\"depth\").replace(\".png\",\"_disp.png\")\n",
    "                im=cv2.imread(image,0)\n",
    "                imd=cv2.imread(depth_image,0)\n",
    "                h,w=im.shape\n",
    "                hd,wd=imd.shape\n",
    "                pose=seq.Pose[i][:2]    \n",
    "#                 px=pose[0]/w\n",
    "#                 py=pose[1]/h\n",
    "                x=int(pose[0]*wd/w)\n",
    "                y=int(pose[1]*hd/h)\n",
    "#                 pose=[px,py]\n",
    "                pose.append(imd[y,x])\n",
    "                pose.extend(egomotion[i])\n",
    "                obs_p.append(pose)\n",
    "                obs_act.append([w,h])\n",
    "                \n",
    "                true=seq.Future_Pose[i][:2]\n",
    "                true_p.append([true[0]/w, true[1]/h])\n",
    "                true_act.append(true)\n",
    "\n",
    "#             obs_global=obs_p\n",
    "#             true = np.array([seq.Future_Pose[i] for i in range(0,self.args.output,self.args.skip)])\n",
    "\n",
    "#             true_resh = true.reshape(true.shape[0],17,2)\n",
    "#             true_global=true_resh[:,0]\n",
    "#             true_global=[true_global[0]/w,true_global[1]/h]\n",
    "#             print(obs_p)\n",
    "#             print(obs_act)\n",
    "            self.obs[index]=obs_p\n",
    "            self.true[index]=true_p\n",
    "            self.obs_act[index]=obs_act\n",
    "            self.true_act[index]=true_act\n",
    "            \n",
    "#             print(self.obs[index])\n",
    "#             print(self.true[index])\n",
    "#             print(self.obs_act[index])\n",
    "#             print(self.true_act[index])\n",
    "            \n",
    "            obs=torch.tensor(self.obs[index],dtype=torch.double)\n",
    "            true=torch.tensor(self.true[index],dtype=torch.double)\n",
    "            obs_act=torch.tensor(self.obs_act[index],dtype=torch.double)\n",
    "            true_act=torch.tensor(self.true_act[index],dtype=torch.double)\n",
    "    \n",
    "            outputs.append(obs)\n",
    "            outputs.append(true)\n",
    "            outputs.append(obs_act)\n",
    "            outputs.append(true_act)\n",
    "\n",
    "       \n",
    "        return tuple(outputs)    \n",
    "    \n",
    "    \n",
    "def data_loader_DE_depth(args,data):\n",
    "    dataset = myDataset_DE_depth(args,data)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=args.batch_size, shuffle=args.loader_shuffle,\n",
    "        pin_memory=args.pin_memory)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "#Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eligible-diana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train\n",
      "Pose loaded\n",
      "Future_Pose loaded\n",
      "Pose_image loaded\n",
      "Future_image loaded\n",
      "******************************\n",
      "Loading val\n",
      "Pose loaded\n",
      "Future_Pose loaded\n",
      "Pose_image loaded\n",
      "Future_image loaded\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class args():\n",
    "    def __init__(self):\n",
    "       \n",
    "        self.dtype        = 'train'\n",
    "        self.loader_workers = 1\n",
    "        self.loader_shuffle = True\n",
    "        self.pin_memory     = False\n",
    "        self.device         = 'cuda'\n",
    "        self.batch_size     = 50\n",
    "        self.input  = 16\n",
    "        self.output = 16\n",
    "        self.stride = 16\n",
    "        self.skip   = 1\n",
    "        \n",
    "args = args()\n",
    "\n",
    "train_loader=data_loader_DE_depth(args,\"train\")\n",
    "val_loader=data_loader_DE_depth(args,\"val\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "sophisticated-howard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    70.0000,      1.0000,     -0.0002,      0.0000,     -0.0002,\n",
      "             0.0002,      1.0000,     -0.0001,      0.0003,     -0.0000,\n",
      "             0.0001,      1.0000,     -0.0010], dtype=torch.float64)\n",
      "tensor([    73.0000,      1.0000,     -0.0007,      0.0001,     -0.0003,\n",
      "             0.0007,      1.0000,     -0.0002,      0.0007,     -0.0001,\n",
      "             0.0002,      1.0000,     -0.0017], dtype=torch.float64)\n",
      "tensor([    75.0000,      1.0000,     -0.0003,     -0.0003,     -0.0000,\n",
      "             0.0003,      1.0000,      0.0001,      0.0001,      0.0003,\n",
      "            -0.0001,      1.0000,     -0.0045], dtype=torch.float64)\n",
      "tensor([    76.0000,      1.0000,     -0.0005,     -0.0005,     -0.0001,\n",
      "             0.0005,      1.0000,      0.0002,      0.0003,      0.0005,\n",
      "            -0.0002,      1.0000,     -0.0090], dtype=torch.float64)\n",
      "tensor([    22.0000,      1.0000,     -0.0005,      0.0000,     -0.0001,\n",
      "             0.0005,      1.0000,     -0.0003,      0.0004,     -0.0000,\n",
      "             0.0003,      1.0000,     -0.0010], dtype=torch.float64)\n",
      "tensor([    20.0000,      1.0000,     -0.0007,      0.0001,     -0.0001,\n",
      "             0.0007,      1.0000,     -0.0007,      0.0007,     -0.0001,\n",
      "             0.0007,      1.0000,     -0.0023], dtype=torch.float64)\n",
      "tensor([    84.0000,      1.0000,     -0.0003,     -0.0002,     -0.0001,\n",
      "             0.0003,      1.0000,      0.0002,      0.0001,      0.0002,\n",
      "            -0.0002,      1.0000,     -0.0045], dtype=torch.float64)\n",
      "tensor([    83.0000,      1.0000,     -0.0007,     -0.0004,     -0.0001,\n",
      "             0.0007,      1.0000,      0.0003,      0.0002,      0.0004,\n",
      "            -0.0003,      1.0000,     -0.0089], dtype=torch.float64)\n",
      "tensor([    61.0000,      1.0000,     -0.0001,     -0.0004,     -0.0000,\n",
      "             0.0001,      1.0000,      0.0006,     -0.0001,      0.0004,\n",
      "            -0.0006,      1.0000,     -0.0024], dtype=torch.float64)\n",
      "tensor([    58.0000,      1.0000,     -0.0004,     -0.0009,     -0.0001,\n",
      "             0.0004,      1.0000,      0.0012,     -0.0002,      0.0009,\n",
      "            -0.0012,      1.0000,     -0.0055], dtype=torch.float64)\n",
      "tensor([    71.0000,      1.0000,     -0.0004,     -0.0001,     -0.0001,\n",
      "             0.0004,      1.0000,      0.0001,      0.0001,      0.0001,\n",
      "            -0.0001,      1.0000,     -0.0043], dtype=torch.float64)\n",
      "tensor([    72.0000,      1.0000,     -0.0007,     -0.0002,     -0.0001,\n",
      "             0.0007,      1.0000,      0.0003,      0.0002,      0.0002,\n",
      "            -0.0003,      1.0000,     -0.0087], dtype=torch.float64)\n",
      "tensor([   202.0000,      1.0000,     -0.0001,      0.0004,     -0.0002,\n",
      "             0.0001,      1.0000,     -0.0002,      0.0004,     -0.0004,\n",
      "             0.0002,      1.0000,     -0.0024], dtype=torch.float64)\n",
      "tensor([   216.0000,      1.0000,     -0.0000,      0.0009,     -0.0004,\n",
      "             0.0000,      1.0000,     -0.0005,      0.0008,     -0.0009,\n",
      "             0.0005,      1.0000,     -0.0049], dtype=torch.float64)\n",
      "tensor([    73.0000,      1.0000,     -0.0001,      0.0015,     -0.0002,\n",
      "             0.0001,      1.0000,      0.0000,      0.0005,     -0.0015,\n",
      "            -0.0000,      1.0000,     -0.0000], dtype=torch.float64)\n",
      "tensor([    74.0000,      1.0000,     -0.0002,      0.0033,     -0.0003,\n",
      "             0.0002,      1.0000,      0.0001,      0.0010,     -0.0033,\n",
      "            -0.0001,      1.0000,      0.0002], dtype=torch.float64)\n",
      "tensor([   103.0000,      1.0000,      0.0001,      0.0005,     -0.0001,\n",
      "            -0.0001,      1.0000,      0.0005,     -0.0000,     -0.0005,\n",
      "            -0.0005,      1.0000,     -0.0054], dtype=torch.float64)\n",
      "tensor([   105.0000,      1.0000,      0.0002,      0.0011,     -0.0002,\n",
      "            -0.0002,      1.0000,      0.0011,     -0.0000,     -0.0011,\n",
      "            -0.0011,      1.0000,     -0.0103], dtype=torch.float64)\n",
      "tensor([    61.0000,      1.0000,      0.0012,     -0.0075,     -0.0001,\n",
      "            -0.0012,      1.0000,      0.0010,      0.0002,      0.0075,\n",
      "            -0.0010,      1.0000,     -0.0023], dtype=torch.float64)\n",
      "tensor([    57.0000,      0.9999,      0.0027,     -0.0151,     -0.0003,\n",
      "            -0.0027,      1.0000,      0.0020,      0.0004,      0.0151,\n",
      "            -0.0020,      0.9999,     -0.0045], dtype=torch.float64)\n",
      "tensor([    71.0000,      1.0000,     -0.0002,     -0.0000,     -0.0001,\n",
      "             0.0002,      1.0000,      0.0003,      0.0001,      0.0000,\n",
      "            -0.0003,      1.0000,     -0.0042], dtype=torch.float64)\n",
      "tensor([    70.0000,      1.0000,     -0.0003,     -0.0001,     -0.0002,\n",
      "             0.0003,      1.0000,      0.0006,      0.0002,      0.0001,\n",
      "            -0.0006,      1.0000,     -0.0082], dtype=torch.float64)\n",
      "tensor([    90.0000,      1.0000,     -0.0002,      0.0002,     -0.0001,\n",
      "             0.0002,      1.0000,      0.0006,      0.0004,     -0.0002,\n",
      "            -0.0006,      1.0000,      0.0013], dtype=torch.float64)\n",
      "tensor([    88.0000,      1.0000,     -0.0005,      0.0003,     -0.0002,\n",
      "             0.0005,      1.0000,      0.0012,      0.0008,     -0.0003,\n",
      "            -0.0012,      1.0000,      0.0027], dtype=torch.float64)\n",
      "tensor([   154.0000,      1.0000,     -0.0006,      0.0005,     -0.0001,\n",
      "             0.0006,      1.0000,     -0.0001,      0.0004,     -0.0005,\n",
      "             0.0001,      1.0000,     -0.0036], dtype=torch.float64)\n",
      "tensor([   158.0000,      1.0000,     -0.0013,      0.0011,     -0.0002,\n",
      "             0.0013,      1.0000,     -0.0003,      0.0008,     -0.0011,\n",
      "             0.0003,      1.0000,     -0.0072], dtype=torch.float64)\n",
      "tensor([    80.0000,      1.0000,     -0.0003,     -0.0003,     -0.0000,\n",
      "             0.0003,      1.0000,      0.0001,      0.0001,      0.0003,\n",
      "            -0.0001,      1.0000,     -0.0045], dtype=torch.float64)\n",
      "tensor([    80.0000,      1.0000,     -0.0006,     -0.0006,     -0.0001,\n",
      "             0.0006,      1.0000,      0.0002,      0.0003,      0.0006,\n",
      "            -0.0002,      1.0000,     -0.0090], dtype=torch.float64)\n",
      "tensor([    64.0000,      1.0000,      0.0000,     -0.0003,      0.0000,\n",
      "            -0.0000,      1.0000,     -0.0008,      0.0003,      0.0003,\n",
      "             0.0008,      1.0000,      0.0021], dtype=torch.float64)\n",
      "tensor([    66.0000,      1.0000,      0.0005,     -0.0008,      0.0000,\n",
      "            -0.0005,      1.0000,     -0.0016,      0.0005,      0.0008,\n",
      "             0.0016,      1.0000,      0.0045], dtype=torch.float64)\n",
      "tensor([   166.0000,      1.0000,      0.0001,      0.0003,     -0.0002,\n",
      "            -0.0001,      1.0000,     -0.0003,      0.0004,     -0.0003,\n",
      "             0.0003,      1.0000,     -0.0020], dtype=torch.float64)\n",
      "tensor([   164.0000,      1.0000,      0.0002,      0.0006,     -0.0004,\n",
      "            -0.0002,      1.0000,     -0.0007,      0.0007,     -0.0006,\n",
      "             0.0007,      1.0000,     -0.0041], dtype=torch.float64)\n",
      "tensor([   104.0000,      1.0000,     -0.0004,      0.0004,     -0.0001,\n",
      "             0.0004,      1.0000,      0.0001,      0.0002,     -0.0004,\n",
      "            -0.0001,      1.0000,     -0.0007], dtype=torch.float64)\n",
      "tensor([    99.0000,      1.0000,     -0.0009,      0.0008,     -0.0002,\n",
      "             0.0009,      1.0000,      0.0004,      0.0005,     -0.0008,\n",
      "            -0.0004,      1.0000,     -0.0015], dtype=torch.float64)\n",
      "tensor([    84.0000,      1.0000,      0.0000,      0.0007,     -0.0000,\n",
      "            -0.0000,      1.0000,      0.0005,     -0.0000,     -0.0007,\n",
      "            -0.0005,      1.0000,     -0.0041], dtype=torch.float64)\n",
      "tensor([    86.0000,      1.0000,      0.0000,      0.0015,     -0.0001,\n",
      "            -0.0000,      1.0000,      0.0011,     -0.0001,     -0.0015,\n",
      "            -0.0011,      1.0000,     -0.0080], dtype=torch.float64)\n",
      "tensor([    92.0000,      1.0000,     -0.0010,      0.0030,      0.0000,\n",
      "             0.0010,      1.0000,     -0.0000,      0.0003,     -0.0030,\n",
      "             0.0000,      1.0000,      0.0007], dtype=torch.float64)\n",
      "tensor([    93.0000,      1.0000,     -0.0020,      0.0059,      0.0001,\n",
      "             0.0020,      1.0000,     -0.0001,      0.0006,     -0.0059,\n",
      "             0.0001,      1.0000,      0.0011], dtype=torch.float64)\n",
      "tensor([    80.0000,      1.0000,     -0.0003,      0.0018,     -0.0001,\n",
      "             0.0003,      1.0000,     -0.0003,      0.0005,     -0.0018,\n",
      "             0.0003,      1.0000,     -0.0008], dtype=torch.float64)\n",
      "tensor([    83.0000,      1.0000,     -0.0005,      0.0036,     -0.0001,\n",
      "             0.0006,      1.0000,     -0.0005,      0.0009,     -0.0036,\n",
      "             0.0005,      1.0000,     -0.0015], dtype=torch.float64)\n",
      "tensor([    54.0000,      1.0000,     -0.0003,     -0.0006,     -0.0000,\n",
      "             0.0003,      1.0000,      0.0003,     -0.0001,      0.0006,\n",
      "            -0.0003,      1.0000,     -0.0018], dtype=torch.float64)\n",
      "tensor([    51.0000,      1.0000,     -0.0005,     -0.0011,     -0.0001,\n",
      "             0.0005,      1.0000,      0.0006,     -0.0002,      0.0011,\n",
      "            -0.0006,      1.0000,     -0.0036], dtype=torch.float64)\n",
      "tensor([    59.0000,      1.0000,     -0.0011,      0.0002,     -0.0000,\n",
      "             0.0011,      1.0000,     -0.0003,      0.0004,     -0.0002,\n",
      "             0.0003,      1.0000,      0.0001], dtype=torch.float64)\n",
      "tensor([    65.0000,      1.0000,     -0.0020,      0.0006,     -0.0001,\n",
      "             0.0020,      1.0000,     -0.0006,      0.0008,     -0.0006,\n",
      "             0.0006,      1.0000,      0.0001], dtype=torch.float64)\n",
      "tensor([   161.0000,      1.0000,     -0.0006,      0.0005,     -0.0001,\n",
      "             0.0006,      1.0000,     -0.0002,      0.0004,     -0.0005,\n",
      "             0.0002,      1.0000,     -0.0035], dtype=torch.float64)\n",
      "tensor([   160.0000,      1.0000,     -0.0013,      0.0010,     -0.0002,\n",
      "             0.0013,      1.0000,     -0.0004,      0.0008,     -0.0010,\n",
      "             0.0004,      1.0000,     -0.0070], dtype=torch.float64)\n",
      "tensor([   161.0000,      1.0000,     -0.0008,      0.0000,     -0.0001,\n",
      "             0.0008,      1.0000,      0.0000,      0.0004,     -0.0000,\n",
      "            -0.0000,      1.0000,     -0.0007], dtype=torch.float64)\n",
      "tensor([   160.0000,      1.0000,     -0.0013,      0.0001,     -0.0003,\n",
      "             0.0013,      1.0000,      0.0000,      0.0007,     -0.0001,\n",
      "            -0.0000,      1.0000,     -0.0016], dtype=torch.float64)\n",
      "tensor([    90.0000,      1.0000,     -0.0001,      0.0043,      0.0000,\n",
      "             0.0001,      1.0000,     -0.0004,      0.0003,     -0.0043,\n",
      "             0.0004,      1.0000,      0.0023], dtype=torch.float64)\n",
      "tensor([    99.0000,      1.0000,     -0.0001,      0.0086,      0.0001,\n",
      "             0.0001,      1.0000,     -0.0009,      0.0005,     -0.0086,\n",
      "             0.0009,      1.0000,      0.0061], dtype=torch.float64)\n",
      "tensor([    94.0000,      1.0000,      0.0005,      0.0001,     -0.0002,\n",
      "            -0.0005,      1.0000,      0.0006,      0.0003,     -0.0001,\n",
      "            -0.0006,      1.0000,      0.0006], dtype=torch.float64)\n",
      "tensor([    95.0000,      1.0000,      0.0010,      0.0002,     -0.0005,\n",
      "            -0.0010,      1.0000,      0.0009,      0.0006,     -0.0002,\n",
      "            -0.0009,      1.0000,      0.0010], dtype=torch.float64)\n",
      "tensor([   108.0000,      1.0000,     -0.0001,      0.0001,     -0.0002,\n",
      "             0.0001,      1.0000,      0.0000,      0.0003,     -0.0001,\n",
      "            -0.0000,      1.0000,     -0.0003], dtype=torch.float64)\n",
      "tensor([   113.0000,      1.0000,     -0.0004,      0.0002,     -0.0005,\n",
      "             0.0004,      1.0000,      0.0000,      0.0007,     -0.0002,\n",
      "            -0.0000,      1.0000,     -0.0009], dtype=torch.float64)\n",
      "tensor([   217.0000,      1.0000,     -0.0012,     -0.0001,     -0.0001,\n",
      "             0.0012,      1.0000,      0.0004,      0.0003,      0.0001,\n",
      "            -0.0004,      1.0000,     -0.0029], dtype=torch.float64)\n",
      "tensor([   221.0000,      1.0000,     -0.0025,     -0.0002,     -0.0001,\n",
      "             0.0025,      1.0000,      0.0008,      0.0005,      0.0002,\n",
      "            -0.0008,      1.0000,     -0.0059], dtype=torch.float64)\n",
      "tensor([    94.0000,      1.0000,     -0.0005,     -0.0001,      0.0000,\n",
      "             0.0005,      1.0000,     -0.0003,      0.0002,      0.0001,\n",
      "             0.0003,      1.0000,     -0.0028], dtype=torch.float64)\n",
      "tensor([    95.0000,      1.0000,     -0.0017,     -0.0002,      0.0001,\n",
      "             0.0017,      1.0000,     -0.0005,      0.0004,      0.0002,\n",
      "             0.0005,      1.0000,     -0.0055], dtype=torch.float64)\n",
      "tensor([    52.0000,      1.0000,      0.0013,      0.0068,     -0.0000,\n",
      "            -0.0012,      1.0000,     -0.0004,      0.0004,     -0.0068,\n",
      "             0.0004,      1.0000,     -0.0004], dtype=torch.float64)\n",
      "tensor([    53.0000,      0.9999,      0.0029,      0.0143,     -0.0001,\n",
      "            -0.0029,      1.0000,     -0.0008,      0.0008,     -0.0143,\n",
      "             0.0008,      0.9999,     -0.0007], dtype=torch.float64)\n",
      "tensor([   157.0000,      1.0000,     -0.0005,      0.0006,     -0.0001,\n",
      "             0.0005,      1.0000,      0.0004,     -0.0001,     -0.0006,\n",
      "            -0.0004,      1.0000,     -0.0031], dtype=torch.float64)\n",
      "tensor([   163.0000,      1.0000,     -0.0010,      0.0013,     -0.0002,\n",
      "             0.0010,      1.0000,      0.0008,     -0.0001,     -0.0013,\n",
      "            -0.0008,      1.0000,     -0.0063], dtype=torch.float64)\n",
      "tensor([    79.0000,      1.0000,     -0.0003,     -0.0004,     -0.0000,\n",
      "             0.0003,      1.0000,      0.0001,      0.0001,      0.0004,\n",
      "            -0.0001,      1.0000,     -0.0047], dtype=torch.float64)\n",
      "tensor([    84.0000,      1.0000,     -0.0005,     -0.0008,     -0.0000,\n",
      "             0.0005,      1.0000,      0.0002,      0.0003,      0.0008,\n",
      "            -0.0002,      1.0000,     -0.0093], dtype=torch.float64)\n",
      "tensor([    78.0000,      1.0000,     -0.0007,      0.0012,     -0.0001,\n",
      "             0.0007,      1.0000,      0.0002,      0.0002,     -0.0012,\n",
      "            -0.0002,      1.0000,      0.0009], dtype=torch.float64)\n",
      "tensor([    72.0000,      1.0000,     -0.0012,      0.0023,     -0.0002,\n",
      "             0.0012,      1.0000,      0.0003,      0.0003,     -0.0023,\n",
      "            -0.0003,      1.0000,      0.0023], dtype=torch.float64)\n",
      "tensor([    60.0000,      1.0000,     -0.0004,     -0.0004,     -0.0000,\n",
      "             0.0004,      1.0000,      0.0004,     -0.0001,      0.0004,\n",
      "            -0.0004,      1.0000,     -0.0040], dtype=torch.float64)\n",
      "tensor([    61.0000,      1.0000,     -0.0004,     -0.0009,     -0.0001,\n",
      "             0.0004,      1.0000,      0.0008,     -0.0003,      0.0009,\n",
      "            -0.0008,      1.0000,     -0.0078], dtype=torch.float64)\n",
      "tensor([    77.0000,      1.0000,     -0.0005,      0.0001,     -0.0001,\n",
      "             0.0005,      1.0000,     -0.0003,      0.0005,     -0.0001,\n",
      "             0.0003,      1.0000,     -0.0018], dtype=torch.float64)\n",
      "tensor([    80.0000,      1.0000,     -0.0010,      0.0003,     -0.0001,\n",
      "             0.0010,      1.0000,     -0.0006,      0.0010,     -0.0003,\n",
      "             0.0006,      1.0000,     -0.0035], dtype=torch.float64)\n",
      "tensor([    29.0000,      1.0000,      0.0001,      0.0021,     -0.0001,\n",
      "            -0.0001,      1.0000,     -0.0000,      0.0003,     -0.0021,\n",
      "             0.0000,      1.0000,     -0.0000], dtype=torch.float64)\n",
      "tensor([    29.0000,      1.0000,      0.0002,      0.0044,     -0.0001,\n",
      "            -0.0002,      1.0000,     -0.0001,      0.0006,     -0.0044,\n",
      "             0.0001,      1.0000,     -0.0000], dtype=torch.float64)\n",
      "tensor([    74.0000,      1.0000,     -0.0005,      0.0001,     -0.0001,\n",
      "             0.0005,      1.0000,     -0.0003,      0.0005,     -0.0001,\n",
      "             0.0003,      1.0000,     -0.0016], dtype=torch.float64)\n",
      "tensor([    77.0000,      1.0000,     -0.0009,      0.0003,     -0.0001,\n",
      "             0.0009,      1.0000,     -0.0007,      0.0010,     -0.0003,\n",
      "             0.0007,      1.0000,     -0.0032], dtype=torch.float64)\n",
      "tensor([   118.0000,      1.0000,     -0.0008,      0.0003,     -0.0001,\n",
      "             0.0008,      1.0000,      0.0002,      0.0003,     -0.0003,\n",
      "            -0.0002,      1.0000,     -0.0011], dtype=torch.float64)\n",
      "tensor([   122.0000,      1.0000,     -0.0016,      0.0006,     -0.0001,\n",
      "             0.0016,      1.0000,      0.0005,      0.0006,     -0.0006,\n",
      "            -0.0005,      1.0000,     -0.0024], dtype=torch.float64)\n",
      "tensor([    23.0000,      1.0000,     -0.0002,      0.0001,     -0.0001,\n",
      "             0.0002,      1.0000,     -0.0003,      0.0004,     -0.0001,\n",
      "             0.0003,      1.0000,     -0.0016], dtype=torch.float64)\n",
      "tensor([    24.0000,      1.0000,     -0.0004,      0.0001,     -0.0002,\n",
      "             0.0004,      1.0000,     -0.0006,      0.0008,     -0.0001,\n",
      "             0.0006,      1.0000,     -0.0032], dtype=torch.float64)\n",
      "tensor([   116.0000,      1.0000,     -0.0002,      0.0002,     -0.0002,\n",
      "             0.0002,      1.0000,     -0.0002,      0.0004,     -0.0002,\n",
      "             0.0002,      1.0000,     -0.0027], dtype=torch.float64)\n",
      "tensor([   151.0000,      1.0000,     -0.0005,      0.0005,     -0.0003,\n",
      "             0.0005,      1.0000,     -0.0003,      0.0009,     -0.0005,\n",
      "             0.0003,      1.0000,     -0.0056], dtype=torch.float64)\n",
      "tensor([   179.0000,      1.0000,     -0.0002,      0.0003,     -0.0002,\n",
      "             0.0002,      1.0000,     -0.0003,      0.0004,     -0.0003,\n",
      "             0.0003,      1.0000,     -0.0024], dtype=torch.float64)\n",
      "tensor([   178.0000,      1.0000,     -0.0003,      0.0006,     -0.0004,\n",
      "             0.0003,      1.0000,     -0.0006,      0.0008,     -0.0006,\n",
      "             0.0006,      1.0000,     -0.0049], dtype=torch.float64)\n",
      "tensor([    72.0000,      1.0000,      0.0001,      0.0025,     -0.0001,\n",
      "            -0.0001,      1.0000,     -0.0001,      0.0005,     -0.0025,\n",
      "             0.0001,      1.0000,      0.0005], dtype=torch.float64)\n",
      "tensor([    71.0000,      1.0000,      0.0002,      0.0051,     -0.0002,\n",
      "            -0.0002,      1.0000,     -0.0002,      0.0009,     -0.0051,\n",
      "             0.0002,      1.0000,      0.0010], dtype=torch.float64)\n",
      "tensor([    83.0000,      1.0000,     -0.0002,      0.0007,     -0.0002,\n",
      "             0.0002,      1.0000,      0.0000,      0.0003,     -0.0007,\n",
      "            -0.0000,      1.0000,     -0.0021], dtype=torch.float64)\n",
      "tensor([    79.0000,      1.0000,     -0.0005,      0.0013,     -0.0004,\n",
      "             0.0005,      1.0000,      0.0001,      0.0007,     -0.0013,\n",
      "            -0.0001,      1.0000,     -0.0042], dtype=torch.float64)\n",
      "tensor([    39.0000,      1.0000,     -0.0004,      0.0002,     -0.0002,\n",
      "             0.0004,      1.0000,     -0.0000,      0.0004,     -0.0002,\n",
      "             0.0000,      1.0000,     -0.0006], dtype=torch.float64)\n",
      "tensor([    39.0000,      1.0000,     -0.0008,      0.0005,     -0.0004,\n",
      "             0.0008,      1.0000,     -0.0000,      0.0009,     -0.0005,\n",
      "             0.0000,      1.0000,     -0.0013], dtype=torch.float64)\n",
      "tensor([    72.0000,      1.0000,      0.0000,     -0.0013,      0.0000,\n",
      "            -0.0000,      1.0000,     -0.0003,      0.0002,      0.0013,\n",
      "             0.0003,      1.0000,     -0.0046], dtype=torch.float64)\n",
      "tensor([    75.0000,      1.0000,     -0.0001,     -0.0026,      0.0001,\n",
      "             0.0001,      1.0000,     -0.0006,      0.0004,      0.0026,\n",
      "             0.0006,      1.0000,     -0.0092], dtype=torch.float64)\n",
      "tensor([    87.0000,      1.0000,     -0.0000,      0.0006,     -0.0001,\n",
      "             0.0000,      1.0000,      0.0005,     -0.0000,     -0.0006,\n",
      "            -0.0005,      1.0000,     -0.0051], dtype=torch.float64)\n",
      "tensor([    88.0000,      1.0000,      0.0000,      0.0011,     -0.0003,\n",
      "            -0.0000,      1.0000,      0.0010,     -0.0001,     -0.0011,\n",
      "            -0.0010,      1.0000,     -0.0099], dtype=torch.float64)\n",
      "tensor([   108.0000,      1.0000,      0.0002,      0.0025,     -0.0001,\n",
      "            -0.0002,      1.0000,     -0.0000,      0.0005,     -0.0025,\n",
      "             0.0000,      1.0000,      0.0007], dtype=torch.float64)\n",
      "tensor([   109.0000,      1.0000,      0.0005,      0.0051,     -0.0002,\n",
      "            -0.0005,      1.0000,     -0.0001,      0.0009,     -0.0051,\n",
      "             0.0001,      1.0000,      0.0015], dtype=torch.float64)\n",
      "tensor([    39.0000,      1.0000,     -0.0004,      0.0003,     -0.0001,\n",
      "             0.0004,      1.0000,      0.0000,      0.0003,     -0.0003,\n",
      "            -0.0000,      1.0000,     -0.0023], dtype=torch.float64)\n",
      "tensor([    42.0000,      1.0000,     -0.0009,      0.0007,     -0.0002,\n",
      "             0.0009,      1.0000,      0.0001,      0.0006,     -0.0007,\n",
      "            -0.0001,      1.0000,     -0.0045], dtype=torch.float64)\n",
      "tensor([   110.0000,      1.0000,      0.0003,      0.0025,     -0.0001,\n",
      "            -0.0003,      1.0000,     -0.0001,      0.0005,     -0.0025,\n",
      "             0.0001,      1.0000,      0.0002], dtype=torch.float64)\n",
      "tensor([   107.0000,      1.0000,      0.0006,      0.0050,     -0.0002,\n",
      "            -0.0006,      1.0000,     -0.0001,      0.0009,     -0.0050,\n",
      "             0.0001,      1.0000,      0.0007], dtype=torch.float64)\n",
      "tensor([   125.0000,      1.0000,      0.0002,      0.0003,     -0.0003,\n",
      "            -0.0002,      1.0000,      0.0002,      0.0003,     -0.0003,\n",
      "            -0.0002,      1.0000,      0.0002], dtype=torch.float64)\n",
      "tensor([   129.0000,      1.0000,      0.0003,      0.0008,     -0.0005,\n",
      "            -0.0003,      1.0000,      0.0003,      0.0006,     -0.0008,\n",
      "            -0.0003,      1.0000,     -0.0002], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-201-ccf1441ab8d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msci_mode\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mobs_pose_global\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_pose_global\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobs_pose_act\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_pose_act\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mobs_pose_global\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs_pose_global\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtarget_pose_global\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_pose_global\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mpreds_g\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobs_pose_global\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=4,sci_mode =False)\n",
    "for idx, (obs_pose_global, target_pose_global,obs_pose_act,target_pose_act) in enumerate(val_loader):\n",
    "    obs_pose_global = obs_pose_global.to(device='cuda')\n",
    "    target_pose_global = target_pose_global.to(device='cuda')\n",
    "    (preds_g,) = net(pose=obs_pose_global)\n",
    "    print(obs_pose_global[0])\n",
    "#     print(target_pose_act[1][0])\n",
    "#     print(preds_g[1][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "concerned-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class glob_LSTM_DE(nn.Module):    \n",
    "    def __init__(self, args):\n",
    "        '''\n",
    "           input: \n",
    "           output: \n",
    "        '''\n",
    "        super(glob_LSTM_DE, self).__init__()\n",
    "         \n",
    "        \n",
    "        self.DE_encoder = nn.LSTM(input_size=2, hidden_size=100)\n",
    "        self.DE_decoder = nn.LSTMCell(input_size=2, hidden_size=100)\n",
    "#         self.vel_decoder = nn.LSTMCell(input_size=self.encoded_size, hidden_size=args.hidden_size)\n",
    "\n",
    "        self.fc_DE  = nn.Linear(in_features=100, out_features=2)\n",
    "#         self.fc2_DE  = nn.Linear(in_features=15, out_features=2)\n",
    "        \n",
    "#         self.hardtanh = nn.Hardtanh(min_val=-1*100,max_val=100)\n",
    "        self.relu = nn.ReLU() \n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "    def forward(self, pose=None):\n",
    "        \n",
    "        outputs = []        \n",
    "        _, (hidden_dec,cell_dec) = self.DE_encoder(pose.permute(1,0,2))    \n",
    "        hidden_dec=hidden_dec.squeeze(0)\n",
    "        cell_dec=cell_dec.squeeze(0)\n",
    "#         print(cell_dec.size())\n",
    "#         print(pose.shape)\n",
    "#         print(pose[1,-1])\n",
    "        DEDec_inp = pose[:,-1,:]\n",
    "       \n",
    "        DE_outputs = torch.tensor([], device=self.args.device,dtype=torch.double)\n",
    "        \n",
    "#         print(DEDec_inp.shape)\n",
    "        for i in range(self.args.output//self.args.skip):\n",
    "#             print(hidden_dec[0].shape,hidden_dec[1].shape)\n",
    "            (hidden_dec,cell_dec) = self.DE_decoder(DEDec_inp, (hidden_dec,cell_dec)) #decoder_output, \n",
    "            DE_output_t  = self.hardtanh(self.fc_DE(hidden_dec))\n",
    "#             DE_output  = self.fc2_DE(DE_output_t)\n",
    "#             print(DE_output_t.shape)\n",
    "            DE_outputs = torch.cat((DE_outputs, DE_output_t.unsqueeze(1)), dim = 1)\n",
    "            DEDec_inp  = DE_output_t.detach()\n",
    "            \n",
    "            \n",
    "#         print(DE_outputs.shape).permute(1,0,2)\n",
    "        outputs.append(DE_outputs)\n",
    "            \n",
    "        return tuple(outputs)\n",
    "\n",
    "net=glob_LSTM_DE(args).to(args.device).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "greater-tourist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Training ...\n",
      "e: 0 | loss: 413.96 | val_loss: 45.77 | ade_train_g: 1199.65 | ade_val_g: 211.18 | fde_train_g: 1597.73 | fde_val_g: 490.11\n",
      "e: 1 | loss: 32.98 | val_loss: 15.91 | ade_train_g: 152.85 | ade_val_g: 74.08 | fde_train_g: 364.21 | fde_val_g: 135.10\n",
      "e: 2 | loss: 14.46 | val_loss: 14.44 | ade_train_g: 88.14 | ade_val_g: 83.24 | fde_train_g: 176.68 | fde_val_g: 158.60\n",
      "e: 3 | loss: 11.84 | val_loss: 12.59 | ade_train_g: 67.21 | ade_val_g: 61.37 | fde_train_g: 127.66 | fde_val_g: 114.23\n",
      "e: 4 | loss: 9.95 | val_loss: 9.19 | ade_train_g: 64.11 | ade_val_g: 49.58 | fde_train_g: 120.64 | fde_val_g: 87.20\n",
      "e: 5 | loss: 8.15 | val_loss: 9.49 | ade_train_g: 48.75 | ade_val_g: 43.86 | fde_train_g: 94.97 | fde_val_g: 77.51\n",
      "e: 6 | loss: 7.10 | val_loss: 7.03 | ade_train_g: 41.84 | ade_val_g: 45.34 | fde_train_g: 79.40 | fde_val_g: 86.48\n",
      "e: 7 | loss: 6.40 | val_loss: 7.64 | ade_train_g: 46.46 | ade_val_g: 47.79 | fde_train_g: 85.39 | fde_val_g: 91.23\n",
      "e: 8 | loss: 6.26 | val_loss: 8.20 | ade_train_g: 41.73 | ade_val_g: 40.76 | fde_train_g: 77.78 | fde_val_g: 72.54\n",
      "e: 9 | loss: 5.56 | val_loss: 7.41 | ade_train_g: 41.54 | ade_val_g: 43.42 | fde_train_g: 80.97 | fde_val_g: 78.32\n",
      "e: 10 | loss: 5.85 | val_loss: 8.28 | ade_train_g: 42.42 | ade_val_g: 54.20 | fde_train_g: 80.33 | fde_val_g: 106.78\n",
      "e: 11 | loss: 5.35 | val_loss: 5.80 | ade_train_g: 41.62 | ade_val_g: 38.14 | fde_train_g: 80.47 | fde_val_g: 76.02\n",
      "e: 12 | loss: 5.52 | val_loss: 5.49 | ade_train_g: 42.31 | ade_val_g: 35.21 | fde_train_g: 82.02 | fde_val_g: 69.86\n",
      "e: 13 | loss: 4.49 | val_loss: 7.02 | ade_train_g: 35.76 | ade_val_g: 39.97 | fde_train_g: 70.80 | fde_val_g: 73.90\n",
      "e: 14 | loss: 4.72 | val_loss: 7.07 | ade_train_g: 39.33 | ade_val_g: 34.53 | fde_train_g: 76.15 | fde_val_g: 68.29\n",
      "e: 15 | loss: 5.04 | val_loss: 5.41 | ade_train_g: 41.33 | ade_val_g: 33.83 | fde_train_g: 81.34 | fde_val_g: 64.24\n",
      "e: 16 | loss: 5.04 | val_loss: 5.47 | ade_train_g: 36.39 | ade_val_g: 34.85 | fde_train_g: 70.96 | fde_val_g: 67.06\n",
      "e: 17 | loss: 4.00 | val_loss: 5.13 | ade_train_g: 35.74 | ade_val_g: 30.50 | fde_train_g: 71.17 | fde_val_g: 61.33\n",
      "e: 18 | loss: 4.07 | val_loss: 5.73 | ade_train_g: 31.51 | ade_val_g: 31.08 | fde_train_g: 61.85 | fde_val_g: 62.62\n",
      "e: 19 | loss: 3.65 | val_loss: 5.86 | ade_train_g: 30.99 | ade_val_g: 36.62 | fde_train_g: 65.11 | fde_val_g: 65.26\n",
      "e: 20 | loss: 3.49 | val_loss: 7.58 | ade_train_g: 30.71 | ade_val_g: 35.04 | fde_train_g: 60.67 | fde_val_g: 59.90\n",
      "e: 21 | loss: 4.23 | val_loss: 4.96 | ade_train_g: 30.57 | ade_val_g: 31.87 | fde_train_g: 59.38 | fde_val_g: 62.73\n",
      "e: 22 | loss: 3.65 | val_loss: 5.45 | ade_train_g: 31.52 | ade_val_g: 32.00 | fde_train_g: 65.68 | fde_val_g: 55.30\n",
      "e: 23 | loss: 3.04 | val_loss: 4.20 | ade_train_g: 28.16 | ade_val_g: 27.63 | fde_train_g: 53.49 | fde_val_g: 53.67\n",
      "e: 24 | loss: 3.00 | val_loss: 4.42 | ade_train_g: 26.52 | ade_val_g: 30.18 | fde_train_g: 50.75 | fde_val_g: 57.45\n",
      "e: 25 | loss: 4.76 | val_loss: 6.01 | ade_train_g: 41.85 | ade_val_g: 47.46 | fde_train_g: 76.53 | fde_val_g: 77.89\n",
      "e: 26 | loss: 4.14 | val_loss: 4.56 | ade_train_g: 38.03 | ade_val_g: 26.80 | fde_train_g: 69.16 | fde_val_g: 52.29\n",
      "e: 27 | loss: 2.72 | val_loss: 4.37 | ade_train_g: 29.87 | ade_val_g: 28.21 | fde_train_g: 57.29 | fde_val_g: 51.11\n",
      "e: 28 | loss: 2.81 | val_loss: 4.62 | ade_train_g: 29.33 | ade_val_g: 29.72 | fde_train_g: 54.86 | fde_val_g: 51.55\n",
      "e: 29 | loss: 2.36 | val_loss: 3.72 | ade_train_g: 24.18 | ade_val_g: 20.07 | fde_train_g: 45.31 | fde_val_g: 38.06\n",
      "e: 30 | loss: 2.13 | val_loss: 4.06 | ade_train_g: 21.77 | ade_val_g: 20.16 | fde_train_g: 41.32 | fde_val_g: 38.10\n",
      "e: 31 | loss: 2.07 | val_loss: 3.80 | ade_train_g: 20.40 | ade_val_g: 21.83 | fde_train_g: 40.99 | fde_val_g: 41.79\n",
      "e: 32 | loss: 2.54 | val_loss: 4.12 | ade_train_g: 24.06 | ade_val_g: 26.45 | fde_train_g: 47.51 | fde_val_g: 48.23\n",
      "e: 33 | loss: 2.17 | val_loss: 4.35 | ade_train_g: 22.63 | ade_val_g: 24.49 | fde_train_g: 44.51 | fde_val_g: 46.43\n",
      "e: 34 | loss: 2.23 | val_loss: 4.10 | ade_train_g: 21.89 | ade_val_g: 27.57 | fde_train_g: 42.96 | fde_val_g: 47.51\n",
      "e: 35 | loss: 2.09 | val_loss: 3.90 | ade_train_g: 20.60 | ade_val_g: 20.79 | fde_train_g: 40.74 | fde_val_g: 40.89\n",
      "e: 36 | loss: 2.09 | val_loss: 3.73 | ade_train_g: 19.77 | ade_val_g: 24.60 | fde_train_g: 39.81 | fde_val_g: 46.34\n",
      "e: 37 | loss: 2.62 | val_loss: 4.10 | ade_train_g: 29.01 | ade_val_g: 25.74 | fde_train_g: 53.04 | fde_val_g: 49.89\n",
      "e: 38 | loss: 2.65 | val_loss: 3.89 | ade_train_g: 22.50 | ade_val_g: 24.23 | fde_train_g: 43.60 | fde_val_g: 45.89\n",
      "e: 39 | loss: 2.31 | val_loss: 4.11 | ade_train_g: 23.24 | ade_val_g: 22.63 | fde_train_g: 44.21 | fde_val_g: 43.67\n",
      "Epoch    41: reducing learning rate of group 0 to 5.0000e-02.\n",
      "e: 40 | loss: 2.23 | val_loss: 4.05 | ade_train_g: 20.79 | ade_val_g: 28.40 | fde_train_g: 40.65 | fde_val_g: 54.77\n",
      "e: 41 | loss: 1.94 | val_loss: 3.49 | ade_train_g: 20.23 | ade_val_g: 21.87 | fde_train_g: 39.23 | fde_val_g: 41.46\n",
      "e: 42 | loss: 1.81 | val_loss: 3.60 | ade_train_g: 17.78 | ade_val_g: 21.76 | fde_train_g: 35.08 | fde_val_g: 40.93\n",
      "e: 43 | loss: 1.82 | val_loss: 4.06 | ade_train_g: 17.49 | ade_val_g: 21.84 | fde_train_g: 34.55 | fde_val_g: 40.86\n",
      "e: 44 | loss: 1.76 | val_loss: 3.51 | ade_train_g: 16.76 | ade_val_g: 21.20 | fde_train_g: 33.85 | fde_val_g: 40.48\n",
      "e: 45 | loss: 1.94 | val_loss: 3.43 | ade_train_g: 19.85 | ade_val_g: 19.72 | fde_train_g: 38.36 | fde_val_g: 37.19\n",
      "e: 46 | loss: 1.75 | val_loss: 3.30 | ade_train_g: 17.09 | ade_val_g: 17.84 | fde_train_g: 34.81 | fde_val_g: 32.54\n",
      "e: 47 | loss: 1.81 | val_loss: 3.94 | ade_train_g: 18.81 | ade_val_g: 21.12 | fde_train_g: 37.98 | fde_val_g: 38.77\n",
      "e: 48 | loss: 1.81 | val_loss: 3.45 | ade_train_g: 18.33 | ade_val_g: 20.75 | fde_train_g: 37.39 | fde_val_g: 38.89\n",
      "e: 49 | loss: 1.78 | val_loss: 3.46 | ade_train_g: 17.37 | ade_val_g: 21.50 | fde_train_g: 35.12 | fde_val_g: 41.06\n",
      "e: 50 | loss: 1.76 | val_loss: 3.53 | ade_train_g: 17.78 | ade_val_g: 21.77 | fde_train_g: 36.17 | fde_val_g: 42.25\n",
      "e: 51 | loss: 1.75 | val_loss: 3.82 | ade_train_g: 17.17 | ade_val_g: 19.15 | fde_train_g: 34.68 | fde_val_g: 36.03\n",
      "e: 52 | loss: 1.75 | val_loss: 3.36 | ade_train_g: 16.83 | ade_val_g: 18.56 | fde_train_g: 33.31 | fde_val_g: 37.11\n",
      "e: 53 | loss: 1.72 | val_loss: 3.92 | ade_train_g: 16.94 | ade_val_g: 19.09 | fde_train_g: 34.26 | fde_val_g: 35.76\n",
      "e: 54 | loss: 1.68 | val_loss: 3.52 | ade_train_g: 15.68 | ade_val_g: 21.34 | fde_train_g: 31.49 | fde_val_g: 39.21\n",
      "e: 55 | loss: 1.79 | val_loss: 3.28 | ade_train_g: 16.33 | ade_val_g: 18.20 | fde_train_g: 32.37 | fde_val_g: 34.74\n",
      "e: 56 | loss: 1.59 | val_loss: 4.34 | ade_train_g: 15.26 | ade_val_g: 20.24 | fde_train_g: 31.01 | fde_val_g: 36.73\n",
      "e: 57 | loss: 1.71 | val_loss: 3.71 | ade_train_g: 17.04 | ade_val_g: 16.86 | fde_train_g: 34.18 | fde_val_g: 31.87\n",
      "e: 58 | loss: 1.72 | val_loss: 3.17 | ade_train_g: 15.51 | ade_val_g: 15.62 | fde_train_g: 31.31 | fde_val_g: 30.44\n",
      "e: 59 | loss: 1.71 | val_loss: 3.83 | ade_train_g: 17.60 | ade_val_g: 20.21 | fde_train_g: 34.89 | fde_val_g: 37.82\n",
      "e: 60 | loss: 1.88 | val_loss: 3.38 | ade_train_g: 20.51 | ade_val_g: 19.68 | fde_train_g: 40.45 | fde_val_g: 38.35\n",
      "e: 61 | loss: 1.83 | val_loss: 3.99 | ade_train_g: 17.94 | ade_val_g: 22.12 | fde_train_g: 36.29 | fde_val_g: 40.70\n",
      "e: 62 | loss: 1.85 | val_loss: 3.91 | ade_train_g: 18.37 | ade_val_g: 20.23 | fde_train_g: 36.06 | fde_val_g: 37.71\n",
      "e: 63 | loss: 1.94 | val_loss: 3.38 | ade_train_g: 18.63 | ade_val_g: 18.29 | fde_train_g: 37.49 | fde_val_g: 35.09\n",
      "e: 64 | loss: 2.53 | val_loss: 5.05 | ade_train_g: 23.84 | ade_val_g: 35.09 | fde_train_g: 42.35 | fde_val_g: 55.23\n",
      "Epoch    66: reducing learning rate of group 0 to 2.5000e-02.\n",
      "e: 65 | loss: 2.40 | val_loss: 4.23 | ade_train_g: 22.03 | ade_val_g: 21.60 | fde_train_g: 40.01 | fde_val_g: 37.00\n",
      "e: 66 | loss: 1.93 | val_loss: 4.19 | ade_train_g: 18.15 | ade_val_g: 21.48 | fde_train_g: 34.33 | fde_val_g: 37.84\n",
      "e: 67 | loss: 1.97 | val_loss: 3.97 | ade_train_g: 18.83 | ade_val_g: 20.32 | fde_train_g: 36.47 | fde_val_g: 34.99\n",
      "e: 68 | loss: 1.90 | val_loss: 3.39 | ade_train_g: 18.42 | ade_val_g: 18.90 | fde_train_g: 35.20 | fde_val_g: 36.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 69 | loss: 1.70 | val_loss: 3.30 | ade_train_g: 16.31 | ade_val_g: 18.16 | fde_train_g: 32.93 | fde_val_g: 34.07\n",
      "e: 70 | loss: 1.58 | val_loss: 3.79 | ade_train_g: 15.14 | ade_val_g: 17.38 | fde_train_g: 30.62 | fde_val_g: 32.48\n",
      "e: 71 | loss: 1.58 | val_loss: 3.71 | ade_train_g: 14.99 | ade_val_g: 17.08 | fde_train_g: 30.12 | fde_val_g: 31.64\n",
      "e: 72 | loss: 1.57 | val_loss: 3.71 | ade_train_g: 15.27 | ade_val_g: 17.26 | fde_train_g: 30.76 | fde_val_g: 31.71\n",
      "e: 73 | loss: 1.54 | val_loss: 3.36 | ade_train_g: 14.69 | ade_val_g: 18.90 | fde_train_g: 29.80 | fde_val_g: 34.81\n",
      "e: 74 | loss: 1.63 | val_loss: 3.36 | ade_train_g: 14.78 | ade_val_g: 18.53 | fde_train_g: 31.67 | fde_val_g: 34.26\n",
      "e: 75 | loss: 1.56 | val_loss: 4.23 | ade_train_g: 15.27 | ade_val_g: 17.88 | fde_train_g: 30.80 | fde_val_g: 32.17\n",
      "e: 76 | loss: 1.60 | val_loss: 3.42 | ade_train_g: 15.46 | ade_val_g: 19.25 | fde_train_g: 31.27 | fde_val_g: 36.76\n",
      "e: 77 | loss: 1.56 | val_loss: 3.23 | ade_train_g: 14.91 | ade_val_g: 16.69 | fde_train_g: 29.93 | fde_val_g: 31.43\n",
      "e: 78 | loss: 1.57 | val_loss: 3.22 | ade_train_g: 14.91 | ade_val_g: 16.58 | fde_train_g: 29.89 | fde_val_g: 31.10\n",
      "e: 79 | loss: 1.66 | val_loss: 3.38 | ade_train_g: 15.49 | ade_val_g: 18.43 | fde_train_g: 32.74 | fde_val_g: 35.31\n",
      "e: 80 | loss: 1.57 | val_loss: 3.30 | ade_train_g: 14.81 | ade_val_g: 16.42 | fde_train_g: 29.71 | fde_val_g: 31.07\n",
      "e: 81 | loss: 1.59 | val_loss: 3.33 | ade_train_g: 14.69 | ade_val_g: 17.01 | fde_train_g: 29.08 | fde_val_g: 32.00\n",
      "Epoch    83: reducing learning rate of group 0 to 1.2500e-02.\n",
      "e: 82 | loss: 1.58 | val_loss: 3.25 | ade_train_g: 14.54 | ade_val_g: 16.38 | fde_train_g: 28.96 | fde_val_g: 31.71\n",
      "e: 83 | loss: 1.55 | val_loss: 3.84 | ade_train_g: 14.91 | ade_val_g: 17.87 | fde_train_g: 30.27 | fde_val_g: 33.84\n",
      "e: 84 | loss: 1.57 | val_loss: 4.29 | ade_train_g: 14.77 | ade_val_g: 17.61 | fde_train_g: 29.42 | fde_val_g: 32.43\n",
      "e: 85 | loss: 1.61 | val_loss: 3.65 | ade_train_g: 16.63 | ade_val_g: 15.88 | fde_train_g: 31.53 | fde_val_g: 29.66\n",
      "e: 86 | loss: 1.49 | val_loss: 3.29 | ade_train_g: 13.75 | ade_val_g: 17.16 | fde_train_g: 27.82 | fde_val_g: 33.24\n",
      "e: 87 | loss: 1.48 | val_loss: 3.77 | ade_train_g: 13.83 | ade_val_g: 16.30 | fde_train_g: 28.11 | fde_val_g: 30.23\n",
      "e: 88 | loss: 1.48 | val_loss: 3.27 | ade_train_g: 13.93 | ade_val_g: 17.12 | fde_train_g: 28.04 | fde_val_g: 32.45\n",
      "e: 89 | loss: 1.51 | val_loss: 4.21 | ade_train_g: 14.20 | ade_val_g: 17.12 | fde_train_g: 28.78 | fde_val_g: 30.89\n",
      "e: 90 | loss: 1.48 | val_loss: 3.20 | ade_train_g: 13.94 | ade_val_g: 15.70 | fde_train_g: 28.31 | fde_val_g: 28.88\n",
      "e: 91 | loss: 1.58 | val_loss: 3.18 | ade_train_g: 14.65 | ade_val_g: 15.47 | fde_train_g: 29.19 | fde_val_g: 29.25\n",
      "e: 92 | loss: 1.47 | val_loss: 4.14 | ade_train_g: 13.82 | ade_val_g: 15.97 | fde_train_g: 28.03 | fde_val_g: 28.74\n",
      "e: 93 | loss: 1.49 | val_loss: 3.18 | ade_train_g: 13.87 | ade_val_g: 15.94 | fde_train_g: 28.15 | fde_val_g: 30.58\n",
      "e: 94 | loss: 1.46 | val_loss: 3.68 | ade_train_g: 13.83 | ade_val_g: 16.59 | fde_train_g: 27.83 | fde_val_g: 30.45\n",
      "e: 95 | loss: 1.45 | val_loss: 3.19 | ade_train_g: 13.46 | ade_val_g: 15.97 | fde_train_g: 27.45 | fde_val_g: 30.82\n",
      "e: 96 | loss: 1.46 | val_loss: 3.32 | ade_train_g: 13.40 | ade_val_g: 18.10 | fde_train_g: 27.35 | fde_val_g: 34.96\n",
      "e: 97 | loss: 1.46 | val_loss: 3.22 | ade_train_g: 13.64 | ade_val_g: 16.68 | fde_train_g: 27.65 | fde_val_g: 31.65\n",
      "e: 98 | loss: 1.46 | val_loss: 3.73 | ade_train_g: 13.69 | ade_val_g: 16.87 | fde_train_g: 27.49 | fde_val_g: 31.99\n",
      "e: 99 | loss: 1.49 | val_loss: 3.22 | ade_train_g: 14.07 | ade_val_g: 17.04 | fde_train_g: 28.67 | fde_val_g: 31.95\n",
      "e: 100 | loss: 1.50 | val_loss: 3.21 | ade_train_g: 13.86 | ade_val_g: 16.46 | fde_train_g: 27.76 | fde_val_g: 31.28\n",
      "e: 101 | loss: 1.45 | val_loss: 3.15 | ade_train_g: 13.59 | ade_val_g: 15.93 | fde_train_g: 27.58 | fde_val_g: 30.24\n",
      "e: 102 | loss: 1.44 | val_loss: 3.73 | ade_train_g: 13.28 | ade_val_g: 16.99 | fde_train_g: 26.99 | fde_val_g: 31.88\n",
      "e: 103 | loss: 1.43 | val_loss: 3.26 | ade_train_g: 13.30 | ade_val_g: 17.69 | fde_train_g: 27.02 | fde_val_g: 33.28\n",
      "e: 104 | loss: 1.44 | val_loss: 3.26 | ade_train_g: 13.39 | ade_val_g: 17.14 | fde_train_g: 27.21 | fde_val_g: 32.75\n",
      "e: 105 | loss: 1.44 | val_loss: 3.61 | ade_train_g: 13.41 | ade_val_g: 15.63 | fde_train_g: 27.37 | fde_val_g: 28.99\n",
      "e: 106 | loss: 1.46 | val_loss: 3.24 | ade_train_g: 13.54 | ade_val_g: 16.95 | fde_train_g: 27.64 | fde_val_g: 32.66\n",
      "e: 107 | loss: 1.44 | val_loss: 3.76 | ade_train_g: 13.26 | ade_val_g: 17.50 | fde_train_g: 27.18 | fde_val_g: 32.99\n",
      "e: 108 | loss: 1.51 | val_loss: 3.23 | ade_train_g: 13.82 | ade_val_g: 16.77 | fde_train_g: 28.11 | fde_val_g: 31.51\n",
      "e: 109 | loss: 1.44 | val_loss: 3.82 | ade_train_g: 13.46 | ade_val_g: 17.68 | fde_train_g: 27.19 | fde_val_g: 33.94\n",
      "e: 110 | loss: 1.46 | val_loss: 3.62 | ade_train_g: 13.61 | ade_val_g: 15.68 | fde_train_g: 27.61 | fde_val_g: 29.33\n",
      "e: 111 | loss: 1.43 | val_loss: 3.28 | ade_train_g: 13.32 | ade_val_g: 16.96 | fde_train_g: 26.78 | fde_val_g: 33.38\n",
      "e: 112 | loss: 1.42 | val_loss: 3.15 | ade_train_g: 13.09 | ade_val_g: 15.86 | fde_train_g: 26.38 | fde_val_g: 30.24\n",
      "e: 113 | loss: 1.43 | val_loss: 3.75 | ade_train_g: 13.19 | ade_val_g: 16.96 | fde_train_g: 26.78 | fde_val_g: 32.16\n",
      "e: 114 | loss: 1.53 | val_loss: 3.70 | ade_train_g: 15.30 | ade_val_g: 16.60 | fde_train_g: 28.88 | fde_val_g: 31.96\n",
      "e: 115 | loss: 1.43 | val_loss: 3.16 | ade_train_g: 13.15 | ade_val_g: 15.78 | fde_train_g: 26.64 | fde_val_g: 30.17\n",
      "e: 116 | loss: 1.41 | val_loss: 3.15 | ade_train_g: 12.87 | ade_val_g: 15.38 | fde_train_g: 25.94 | fde_val_g: 29.73\n",
      "e: 117 | loss: 1.43 | val_loss: 3.10 | ade_train_g: 12.94 | ade_val_g: 15.13 | fde_train_g: 26.29 | fde_val_g: 28.89\n",
      "e: 118 | loss: 1.43 | val_loss: 3.68 | ade_train_g: 13.01 | ade_val_g: 16.49 | fde_train_g: 26.25 | fde_val_g: 31.03\n",
      "e: 119 | loss: 1.49 | val_loss: 3.26 | ade_train_g: 13.97 | ade_val_g: 16.96 | fde_train_g: 28.11 | fde_val_g: 32.74\n",
      "e: 120 | loss: 1.60 | val_loss: 3.69 | ade_train_g: 15.27 | ade_val_g: 16.80 | fde_train_g: 30.24 | fde_val_g: 30.86\n",
      "e: 121 | loss: 1.60 | val_loss: 3.37 | ade_train_g: 15.20 | ade_val_g: 18.43 | fde_train_g: 30.32 | fde_val_g: 36.07\n",
      "e: 122 | loss: 1.60 | val_loss: 3.67 | ade_train_g: 15.33 | ade_val_g: 16.50 | fde_train_g: 30.39 | fde_val_g: 30.41\n",
      "e: 123 | loss: 1.58 | val_loss: 3.20 | ade_train_g: 14.98 | ade_val_g: 16.20 | fde_train_g: 29.67 | fde_val_g: 30.48\n",
      "e: 124 | loss: 1.60 | val_loss: 3.26 | ade_train_g: 14.48 | ade_val_g: 16.08 | fde_train_g: 30.32 | fde_val_g: 31.11\n",
      "Epoch   126: reducing learning rate of group 0 to 6.2500e-03.\n",
      "e: 125 | loss: 1.53 | val_loss: 3.21 | ade_train_g: 14.68 | ade_val_g: 15.65 | fde_train_g: 29.37 | fde_val_g: 29.36\n",
      "e: 126 | loss: 1.48 | val_loss: 3.67 | ade_train_g: 14.15 | ade_val_g: 15.49 | fde_train_g: 28.07 | fde_val_g: 29.18\n",
      "e: 127 | loss: 1.48 | val_loss: 3.65 | ade_train_g: 13.98 | ade_val_g: 15.63 | fde_train_g: 28.15 | fde_val_g: 29.05\n",
      "e: 128 | loss: 1.49 | val_loss: 3.71 | ade_train_g: 14.12 | ade_val_g: 15.65 | fde_train_g: 28.47 | fde_val_g: 29.55\n",
      "e: 129 | loss: 1.45 | val_loss: 3.85 | ade_train_g: 13.65 | ade_val_g: 17.49 | fde_train_g: 27.41 | fde_val_g: 33.50\n",
      "e: 130 | loss: 1.48 | val_loss: 3.32 | ade_train_g: 13.77 | ade_val_g: 16.19 | fde_train_g: 28.10 | fde_val_g: 31.52\n",
      "e: 131 | loss: 1.45 | val_loss: 3.67 | ade_train_g: 13.47 | ade_val_g: 15.65 | fde_train_g: 27.16 | fde_val_g: 29.54\n",
      "e: 132 | loss: 1.48 | val_loss: 3.35 | ade_train_g: 13.79 | ade_val_g: 16.99 | fde_train_g: 28.12 | fde_val_g: 33.62\n",
      "e: 133 | loss: 1.47 | val_loss: 3.11 | ade_train_g: 13.65 | ade_val_g: 14.76 | fde_train_g: 27.66 | fde_val_g: 28.09\n",
      "Epoch   135: reducing learning rate of group 0 to 3.1250e-03.\n",
      "e: 134 | loss: 1.45 | val_loss: 3.12 | ade_train_g: 13.42 | ade_val_g: 15.14 | fde_train_g: 27.22 | fde_val_g: 28.87\n",
      "e: 135 | loss: 1.45 | val_loss: 3.13 | ade_train_g: 13.44 | ade_val_g: 14.93 | fde_train_g: 27.29 | fde_val_g: 28.27\n",
      "e: 136 | loss: 1.44 | val_loss: 3.14 | ade_train_g: 13.33 | ade_val_g: 14.89 | fde_train_g: 26.90 | fde_val_g: 28.68\n",
      "e: 137 | loss: 1.44 | val_loss: 3.66 | ade_train_g: 13.26 | ade_val_g: 15.69 | fde_train_g: 26.71 | fde_val_g: 29.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 138 | loss: 1.56 | val_loss: 3.66 | ade_train_g: 13.88 | ade_val_g: 15.63 | fde_train_g: 30.08 | fde_val_g: 29.27\n",
      "e: 139 | loss: 1.46 | val_loss: 3.15 | ade_train_g: 13.50 | ade_val_g: 14.98 | fde_train_g: 27.52 | fde_val_g: 28.95\n",
      "e: 140 | loss: 1.44 | val_loss: 3.64 | ade_train_g: 13.40 | ade_val_g: 15.27 | fde_train_g: 27.09 | fde_val_g: 28.69\n",
      "e: 141 | loss: 1.44 | val_loss: 4.13 | ade_train_g: 13.34 | ade_val_g: 15.54 | fde_train_g: 26.95 | fde_val_g: 28.29\n",
      "e: 142 | loss: 1.44 | val_loss: 3.79 | ade_train_g: 13.31 | ade_val_g: 16.43 | fde_train_g: 27.03 | fde_val_g: 31.52\n",
      "Epoch   144: reducing learning rate of group 0 to 1.5625e-03.\n",
      "e: 143 | loss: 1.45 | val_loss: 3.62 | ade_train_g: 13.59 | ade_val_g: 15.22 | fde_train_g: 27.36 | fde_val_g: 28.62\n",
      "e: 144 | loss: 1.47 | val_loss: 3.66 | ade_train_g: 13.62 | ade_val_g: 15.83 | fde_train_g: 27.85 | fde_val_g: 29.91\n",
      "e: 145 | loss: 1.43 | val_loss: 3.22 | ade_train_g: 13.23 | ade_val_g: 15.69 | fde_train_g: 26.90 | fde_val_g: 30.71\n",
      "e: 146 | loss: 1.44 | val_loss: 3.25 | ade_train_g: 13.25 | ade_val_g: 16.34 | fde_train_g: 26.85 | fde_val_g: 31.98\n",
      "e: 147 | loss: 1.43 | val_loss: 3.13 | ade_train_g: 13.14 | ade_val_g: 15.04 | fde_train_g: 26.51 | fde_val_g: 28.67\n",
      "e: 148 | loss: 1.42 | val_loss: 3.14 | ade_train_g: 13.04 | ade_val_g: 15.12 | fde_train_g: 26.46 | fde_val_g: 28.95\n",
      "e: 149 | loss: 1.43 | val_loss: 3.74 | ade_train_g: 13.15 | ade_val_g: 16.70 | fde_train_g: 26.43 | fde_val_g: 31.54\n",
      "e: 150 | loss: 1.51 | val_loss: 3.14 | ade_train_g: 13.24 | ade_val_g: 15.06 | fde_train_g: 28.47 | fde_val_g: 28.96\n",
      "e: 151 | loss: 1.41 | val_loss: 3.19 | ade_train_g: 13.00 | ade_val_g: 15.68 | fde_train_g: 26.25 | fde_val_g: 30.39\n",
      "Epoch   153: reducing learning rate of group 0 to 7.8125e-04.\n",
      "e: 152 | loss: 1.42 | val_loss: 3.15 | ade_train_g: 13.09 | ade_val_g: 15.17 | fde_train_g: 26.44 | fde_val_g: 28.81\n",
      "e: 153 | loss: 1.44 | val_loss: 3.14 | ade_train_g: 13.23 | ade_val_g: 15.09 | fde_train_g: 26.82 | fde_val_g: 28.62\n",
      "e: 154 | loss: 1.42 | val_loss: 3.19 | ade_train_g: 13.05 | ade_val_g: 15.85 | fde_train_g: 26.29 | fde_val_g: 30.15\n",
      "e: 155 | loss: 1.41 | val_loss: 3.15 | ade_train_g: 13.01 | ade_val_g: 15.10 | fde_train_g: 26.21 | fde_val_g: 28.93\n",
      "e: 156 | loss: 1.41 | val_loss: 3.14 | ade_train_g: 12.97 | ade_val_g: 15.07 | fde_train_g: 26.09 | fde_val_g: 28.59\n",
      "e: 157 | loss: 1.43 | val_loss: 3.83 | ade_train_g: 13.11 | ade_val_g: 17.40 | fde_train_g: 26.53 | fde_val_g: 32.80\n",
      "e: 158 | loss: 1.43 | val_loss: 3.68 | ade_train_g: 13.11 | ade_val_g: 16.10 | fde_train_g: 26.55 | fde_val_g: 30.34\n",
      "e: 159 | loss: 1.41 | val_loss: 4.14 | ade_train_g: 12.93 | ade_val_g: 15.89 | fde_train_g: 25.92 | fde_val_g: 28.70\n",
      "e: 160 | loss: 1.41 | val_loss: 3.30 | ade_train_g: 12.93 | ade_val_g: 16.64 | fde_train_g: 25.93 | fde_val_g: 31.91\n",
      "e: 161 | loss: 1.41 | val_loss: 3.67 | ade_train_g: 12.92 | ade_val_g: 15.72 | fde_train_g: 26.00 | fde_val_g: 29.74\n",
      "e: 162 | loss: 1.43 | val_loss: 3.83 | ade_train_g: 13.14 | ade_val_g: 17.26 | fde_train_g: 26.51 | fde_val_g: 33.47\n",
      "e: 163 | loss: 1.41 | val_loss: 3.25 | ade_train_g: 12.92 | ade_val_g: 16.44 | fde_train_g: 25.97 | fde_val_g: 31.72\n",
      "e: 164 | loss: 1.41 | val_loss: 3.67 | ade_train_g: 12.91 | ade_val_g: 15.90 | fde_train_g: 25.96 | fde_val_g: 29.44\n",
      "e: 165 | loss: 1.41 | val_loss: 3.65 | ade_train_g: 12.90 | ade_val_g: 15.50 | fde_train_g: 25.96 | fde_val_g: 28.49\n",
      "e: 166 | loss: 1.42 | val_loss: 3.20 | ade_train_g: 13.00 | ade_val_g: 15.69 | fde_train_g: 26.29 | fde_val_g: 30.49\n",
      "e: 167 | loss: 1.40 | val_loss: 3.17 | ade_train_g: 12.81 | ade_val_g: 15.31 | fde_train_g: 25.75 | fde_val_g: 29.46\n",
      "e: 168 | loss: 1.51 | val_loss: 3.19 | ade_train_g: 13.16 | ade_val_g: 15.53 | fde_train_g: 28.33 | fde_val_g: 29.69\n",
      "e: 169 | loss: 1.41 | val_loss: 3.73 | ade_train_g: 12.91 | ade_val_g: 16.41 | fde_train_g: 25.95 | fde_val_g: 30.43\n",
      "e: 170 | loss: 1.42 | val_loss: 3.14 | ade_train_g: 13.00 | ade_val_g: 15.00 | fde_train_g: 26.22 | fde_val_g: 28.62\n",
      "e: 171 | loss: 1.42 | val_loss: 3.71 | ade_train_g: 13.00 | ade_val_g: 16.19 | fde_train_g: 26.21 | fde_val_g: 30.04\n",
      "e: 172 | loss: 1.41 | val_loss: 3.16 | ade_train_g: 13.02 | ade_val_g: 15.25 | fde_train_g: 26.12 | fde_val_g: 29.01\n",
      "e: 173 | loss: 1.42 | val_loss: 3.25 | ade_train_g: 13.01 | ade_val_g: 16.12 | fde_train_g: 26.18 | fde_val_g: 31.16\n",
      "e: 174 | loss: 1.49 | val_loss: 3.28 | ade_train_g: 12.92 | ade_val_g: 16.27 | fde_train_g: 27.74 | fde_val_g: 31.46\n",
      "e: 175 | loss: 1.41 | val_loss: 3.23 | ade_train_g: 12.93 | ade_val_g: 15.94 | fde_train_g: 25.97 | fde_val_g: 31.11\n",
      "Epoch   177: reducing learning rate of group 0 to 3.9063e-04.\n",
      "e: 176 | loss: 1.42 | val_loss: 3.71 | ade_train_g: 13.00 | ade_val_g: 16.06 | fde_train_g: 26.22 | fde_val_g: 30.56\n",
      "e: 177 | loss: 1.41 | val_loss: 3.21 | ade_train_g: 13.00 | ade_val_g: 15.75 | fde_train_g: 26.19 | fde_val_g: 30.55\n",
      "e: 178 | loss: 1.51 | val_loss: 3.75 | ade_train_g: 13.07 | ade_val_g: 16.45 | fde_train_g: 28.21 | fde_val_g: 31.88\n",
      "e: 179 | loss: 1.41 | val_loss: 3.70 | ade_train_g: 12.94 | ade_val_g: 15.80 | fde_train_g: 26.01 | fde_val_g: 29.81\n",
      "e: 180 | loss: 1.40 | val_loss: 3.70 | ade_train_g: 12.84 | ade_val_g: 15.94 | fde_train_g: 25.74 | fde_val_g: 29.91\n",
      "e: 181 | loss: 1.41 | val_loss: 3.22 | ade_train_g: 12.86 | ade_val_g: 15.87 | fde_train_g: 25.75 | fde_val_g: 30.51\n",
      "e: 182 | loss: 1.41 | val_loss: 3.21 | ade_train_g: 12.99 | ade_val_g: 15.63 | fde_train_g: 26.12 | fde_val_g: 30.08\n",
      "e: 183 | loss: 1.41 | val_loss: 3.70 | ade_train_g: 12.89 | ade_val_g: 16.05 | fde_train_g: 25.85 | fde_val_g: 30.21\n",
      "e: 184 | loss: 1.41 | val_loss: 3.18 | ade_train_g: 12.88 | ade_val_g: 15.29 | fde_train_g: 25.94 | fde_val_g: 29.33\n",
      "Epoch   186: reducing learning rate of group 0 to 1.9531e-04.\n",
      "e: 185 | loss: 1.41 | val_loss: 3.31 | ade_train_g: 12.92 | ade_val_g: 16.59 | fde_train_g: 26.10 | fde_val_g: 32.66\n",
      "e: 186 | loss: 1.40 | val_loss: 3.30 | ade_train_g: 12.83 | ade_val_g: 16.27 | fde_train_g: 25.77 | fde_val_g: 31.85\n",
      "e: 187 | loss: 1.40 | val_loss: 3.78 | ade_train_g: 12.89 | ade_val_g: 16.98 | fde_train_g: 25.89 | fde_val_g: 31.84\n",
      "e: 188 | loss: 1.40 | val_loss: 3.20 | ade_train_g: 12.79 | ade_val_g: 15.59 | fde_train_g: 25.72 | fde_val_g: 29.85\n",
      "e: 189 | loss: 1.40 | val_loss: 3.21 | ade_train_g: 12.84 | ade_val_g: 15.65 | fde_train_g: 25.77 | fde_val_g: 30.23\n",
      "e: 190 | loss: 1.41 | val_loss: 3.73 | ade_train_g: 12.88 | ade_val_g: 16.23 | fde_train_g: 25.98 | fde_val_g: 30.85\n",
      "e: 191 | loss: 1.39 | val_loss: 3.86 | ade_train_g: 12.74 | ade_val_g: 17.48 | fde_train_g: 25.59 | fde_val_g: 33.71\n",
      "e: 192 | loss: 1.40 | val_loss: 3.67 | ade_train_g: 12.86 | ade_val_g: 15.58 | fde_train_g: 25.84 | fde_val_g: 28.96\n",
      "e: 193 | loss: 1.39 | val_loss: 3.23 | ade_train_g: 12.69 | ade_val_g: 15.90 | fde_train_g: 25.41 | fde_val_g: 30.71\n",
      "e: 194 | loss: 1.40 | val_loss: 3.68 | ade_train_g: 12.85 | ade_val_g: 15.61 | fde_train_g: 25.82 | fde_val_g: 29.26\n",
      "e: 195 | loss: 1.41 | val_loss: 3.18 | ade_train_g: 12.90 | ade_val_g: 15.46 | fde_train_g: 26.00 | fde_val_g: 29.43\n",
      "e: 196 | loss: 1.40 | val_loss: 4.23 | ade_train_g: 12.85 | ade_val_g: 16.51 | fde_train_g: 25.81 | fde_val_g: 30.44\n",
      "e: 197 | loss: 1.49 | val_loss: 3.19 | ade_train_g: 12.92 | ade_val_g: 15.45 | fde_train_g: 27.78 | fde_val_g: 29.14\n",
      "e: 198 | loss: 1.40 | val_loss: 3.70 | ade_train_g: 12.88 | ade_val_g: 15.89 | fde_train_g: 25.82 | fde_val_g: 29.76\n",
      "e: 199 | loss: 1.50 | val_loss: 3.19 | ade_train_g: 14.89 | ade_val_g: 15.44 | fde_train_g: 27.79 | fde_val_g: 29.74\n",
      "e: 200 | loss: 1.40 | val_loss: 3.21 | ade_train_g: 12.86 | ade_val_g: 15.64 | fde_train_g: 25.88 | fde_val_g: 29.90\n",
      "e: 201 | loss: 1.40 | val_loss: 3.20 | ade_train_g: 12.79 | ade_val_g: 15.55 | fde_train_g: 25.72 | fde_val_g: 29.87\n",
      "Epoch   203: reducing learning rate of group 0 to 9.7656e-05.\n",
      "e: 202 | loss: 1.41 | val_loss: 3.22 | ade_train_g: 12.95 | ade_val_g: 15.81 | fde_train_g: 26.11 | fde_val_g: 30.42\n",
      "e: 203 | loss: 1.41 | val_loss: 3.23 | ade_train_g: 12.96 | ade_val_g: 15.86 | fde_train_g: 26.13 | fde_val_g: 30.68\n",
      "e: 204 | loss: 1.40 | val_loss: 3.21 | ade_train_g: 12.82 | ade_val_g: 15.68 | fde_train_g: 25.79 | fde_val_g: 30.36\n",
      "e: 205 | loss: 1.40 | val_loss: 3.22 | ade_train_g: 12.79 | ade_val_g: 15.62 | fde_train_g: 25.71 | fde_val_g: 30.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 206 | loss: 1.41 | val_loss: 4.20 | ade_train_g: 12.90 | ade_val_g: 16.41 | fde_train_g: 25.98 | fde_val_g: 30.07\n",
      "e: 207 | loss: 1.40 | val_loss: 3.18 | ade_train_g: 12.85 | ade_val_g: 15.24 | fde_train_g: 25.84 | fde_val_g: 29.27\n",
      "e: 208 | loss: 1.42 | val_loss: 3.41 | ade_train_g: 12.96 | ade_val_g: 17.55 | fde_train_g: 26.14 | fde_val_g: 34.74\n",
      "e: 209 | loss: 1.40 | val_loss: 4.79 | ade_train_g: 12.80 | ade_val_g: 17.51 | fde_train_g: 25.78 | fde_val_g: 32.03\n",
      "e: 210 | loss: 1.40 | val_loss: 3.71 | ade_train_g: 12.80 | ade_val_g: 16.14 | fde_train_g: 25.80 | fde_val_g: 30.21\n",
      "Epoch   212: reducing learning rate of group 0 to 4.8828e-05.\n",
      "e: 211 | loss: 1.50 | val_loss: 3.18 | ade_train_g: 14.87 | ade_val_g: 15.52 | fde_train_g: 27.81 | fde_val_g: 29.35\n",
      "e: 212 | loss: 1.41 | val_loss: 3.75 | ade_train_g: 12.92 | ade_val_g: 16.68 | fde_train_g: 26.09 | fde_val_g: 31.60\n",
      "e: 213 | loss: 1.40 | val_loss: 3.25 | ade_train_g: 12.85 | ade_val_g: 16.05 | fde_train_g: 25.85 | fde_val_g: 31.48\n",
      "e: 214 | loss: 1.41 | val_loss: 3.23 | ade_train_g: 12.92 | ade_val_g: 15.99 | fde_train_g: 26.06 | fde_val_g: 30.80\n",
      "e: 215 | loss: 1.40 | val_loss: 3.22 | ade_train_g: 12.87 | ade_val_g: 15.69 | fde_train_g: 25.83 | fde_val_g: 30.69\n",
      "e: 216 | loss: 1.40 | val_loss: 3.19 | ade_train_g: 12.87 | ade_val_g: 15.47 | fde_train_g: 25.84 | fde_val_g: 29.74\n",
      "e: 217 | loss: 1.40 | val_loss: 3.42 | ade_train_g: 12.79 | ade_val_g: 17.95 | fde_train_g: 25.73 | fde_val_g: 35.00\n",
      "e: 218 | loss: 1.40 | val_loss: 3.71 | ade_train_g: 12.83 | ade_val_g: 16.11 | fde_train_g: 25.79 | fde_val_g: 30.25\n",
      "e: 219 | loss: 1.40 | val_loss: 3.18 | ade_train_g: 12.86 | ade_val_g: 15.39 | fde_train_g: 25.88 | fde_val_g: 29.41\n",
      "Epoch   221: reducing learning rate of group 0 to 2.4414e-05.\n",
      "e: 220 | loss: 1.40 | val_loss: 3.21 | ade_train_g: 12.84 | ade_val_g: 15.58 | fde_train_g: 25.81 | fde_val_g: 30.25\n",
      "e: 221 | loss: 1.41 | val_loss: 4.19 | ade_train_g: 12.85 | ade_val_g: 16.13 | fde_train_g: 25.87 | fde_val_g: 29.80\n",
      "e: 222 | loss: 1.40 | val_loss: 3.30 | ade_train_g: 12.81 | ade_val_g: 16.68 | fde_train_g: 25.75 | fde_val_g: 32.25\n",
      "e: 223 | loss: 1.41 | val_loss: 3.71 | ade_train_g: 12.87 | ade_val_g: 16.12 | fde_train_g: 25.93 | fde_val_g: 30.28\n",
      "e: 224 | loss: 1.40 | val_loss: 3.21 | ade_train_g: 12.80 | ade_val_g: 15.79 | fde_train_g: 25.77 | fde_val_g: 30.86\n",
      "e: 225 | loss: 1.40 | val_loss: 3.17 | ade_train_g: 12.86 | ade_val_g: 15.35 | fde_train_g: 25.87 | fde_val_g: 29.36\n",
      "e: 226 | loss: 1.39 | val_loss: 3.15 | ade_train_g: 12.77 | ade_val_g: 15.09 | fde_train_g: 25.66 | fde_val_g: 28.85\n",
      "e: 227 | loss: 1.40 | val_loss: 3.31 | ade_train_g: 12.84 | ade_val_g: 16.88 | fde_train_g: 25.85 | fde_val_g: 32.50\n",
      "e: 228 | loss: 1.41 | val_loss: 4.30 | ade_train_g: 12.86 | ade_val_g: 17.19 | fde_train_g: 25.88 | fde_val_g: 32.24\n",
      "Epoch   230: reducing learning rate of group 0 to 1.2207e-05.\n",
      "e: 229 | loss: 1.41 | val_loss: 3.76 | ade_train_g: 12.94 | ade_val_g: 16.65 | fde_train_g: 26.04 | fde_val_g: 31.52\n",
      "e: 230 | loss: 1.50 | val_loss: 3.19 | ade_train_g: 13.04 | ade_val_g: 15.80 | fde_train_g: 27.96 | fde_val_g: 29.91\n",
      "e: 231 | loss: 1.41 | val_loss: 3.72 | ade_train_g: 12.99 | ade_val_g: 16.17 | fde_train_g: 26.17 | fde_val_g: 30.65\n",
      "e: 232 | loss: 1.41 | val_loss: 3.78 | ade_train_g: 12.91 | ade_val_g: 16.70 | fde_train_g: 25.97 | fde_val_g: 31.67\n",
      "e: 233 | loss: 1.41 | val_loss: 3.68 | ade_train_g: 12.88 | ade_val_g: 15.80 | fde_train_g: 25.94 | fde_val_g: 29.83\n",
      "e: 234 | loss: 1.41 | val_loss: 3.21 | ade_train_g: 12.87 | ade_val_g: 15.74 | fde_train_g: 25.91 | fde_val_g: 30.28\n",
      "e: 235 | loss: 1.39 | val_loss: 3.68 | ade_train_g: 12.75 | ade_val_g: 15.65 | fde_train_g: 25.57 | fde_val_g: 29.46\n",
      "e: 236 | loss: 1.41 | val_loss: 3.34 | ade_train_g: 12.91 | ade_val_g: 16.93 | fde_train_g: 26.09 | fde_val_g: 33.35\n",
      "e: 237 | loss: 1.40 | val_loss: 3.30 | ade_train_g: 12.78 | ade_val_g: 16.50 | fde_train_g: 25.68 | fde_val_g: 32.03\n",
      "Epoch   239: reducing learning rate of group 0 to 6.1035e-06.\n",
      "e: 238 | loss: 1.41 | val_loss: 3.68 | ade_train_g: 12.93 | ade_val_g: 15.81 | fde_train_g: 26.05 | fde_val_g: 29.54\n",
      "e: 239 | loss: 1.40 | val_loss: 3.20 | ade_train_g: 12.80 | ade_val_g: 15.73 | fde_train_g: 25.77 | fde_val_g: 30.16\n",
      "e: 240 | loss: 1.40 | val_loss: 3.71 | ade_train_g: 12.80 | ade_val_g: 16.14 | fde_train_g: 25.76 | fde_val_g: 30.50\n",
      "e: 241 | loss: 1.40 | val_loss: 3.19 | ade_train_g: 12.84 | ade_val_g: 15.46 | fde_train_g: 25.86 | fde_val_g: 29.90\n",
      "e: 242 | loss: 1.40 | val_loss: 3.70 | ade_train_g: 12.82 | ade_val_g: 15.85 | fde_train_g: 25.83 | fde_val_g: 30.01\n",
      "e: 243 | loss: 1.40 | val_loss: 3.33 | ade_train_g: 12.79 | ade_val_g: 17.03 | fde_train_g: 25.75 | fde_val_g: 33.06\n",
      "e: 244 | loss: 1.39 | val_loss: 3.21 | ade_train_g: 12.76 | ade_val_g: 15.86 | fde_train_g: 25.58 | fde_val_g: 30.27\n",
      "e: 245 | loss: 1.42 | val_loss: 3.22 | ade_train_g: 13.01 | ade_val_g: 16.11 | fde_train_g: 26.24 | fde_val_g: 30.42\n",
      "e: 246 | loss: 1.41 | val_loss: 3.84 | ade_train_g: 12.89 | ade_val_g: 17.35 | fde_train_g: 25.93 | fde_val_g: 33.28\n",
      "Epoch   248: reducing learning rate of group 0 to 3.0518e-06.\n",
      "e: 247 | loss: 1.41 | val_loss: 3.68 | ade_train_g: 12.95 | ade_val_g: 15.90 | fde_train_g: 26.07 | fde_val_g: 29.84\n",
      "e: 248 | loss: 1.40 | val_loss: 3.19 | ade_train_g: 12.81 | ade_val_g: 15.67 | fde_train_g: 25.66 | fde_val_g: 29.84\n",
      "e: 249 | loss: 1.50 | val_loss: 3.19 | ade_train_g: 14.87 | ade_val_g: 15.44 | fde_train_g: 27.81 | fde_val_g: 29.52\n",
      "e: 250 | loss: 1.41 | val_loss: 3.25 | ade_train_g: 12.95 | ade_val_g: 16.23 | fde_train_g: 26.08 | fde_val_g: 31.12\n",
      "e: 251 | loss: 1.49 | val_loss: 3.22 | ade_train_g: 12.97 | ade_val_g: 15.97 | fde_train_g: 27.96 | fde_val_g: 30.57\n",
      "e: 252 | loss: 1.38 | val_loss: 3.78 | ade_train_g: 12.68 | ade_val_g: 16.76 | fde_train_g: 25.46 | fde_val_g: 31.75\n",
      "e: 253 | loss: 1.41 | val_loss: 3.17 | ade_train_g: 12.86 | ade_val_g: 15.37 | fde_train_g: 25.84 | fde_val_g: 29.52\n",
      "e: 254 | loss: 1.40 | val_loss: 3.31 | ade_train_g: 12.87 | ade_val_g: 16.71 | fde_train_g: 25.77 | fde_val_g: 32.54\n",
      "e: 255 | loss: 1.40 | val_loss: 4.34 | ade_train_g: 12.80 | ade_val_g: 17.58 | fde_train_g: 25.72 | fde_val_g: 32.99\n",
      "e: 256 | loss: 1.40 | val_loss: 3.19 | ade_train_g: 12.82 | ade_val_g: 15.40 | fde_train_g: 25.79 | fde_val_g: 29.54\n",
      "e: 257 | loss: 1.40 | val_loss: 3.68 | ade_train_g: 12.83 | ade_val_g: 15.76 | fde_train_g: 25.75 | fde_val_g: 29.71\n",
      "e: 258 | loss: 1.41 | val_loss: 3.30 | ade_train_g: 12.91 | ade_val_g: 16.72 | fde_train_g: 26.00 | fde_val_g: 32.77\n",
      "e: 259 | loss: 1.40 | val_loss: 4.22 | ade_train_g: 12.87 | ade_val_g: 16.58 | fde_train_g: 25.85 | fde_val_g: 30.43\n",
      "e: 260 | loss: 1.40 | val_loss: 3.22 | ade_train_g: 12.87 | ade_val_g: 15.79 | fde_train_g: 25.90 | fde_val_g: 30.70\n",
      "Epoch   262: reducing learning rate of group 0 to 1.5259e-06.\n",
      "e: 261 | loss: 1.40 | val_loss: 3.71 | ade_train_g: 12.80 | ade_val_g: 16.07 | fde_train_g: 25.66 | fde_val_g: 30.26\n",
      "e: 262 | loss: 1.40 | val_loss: 3.17 | ade_train_g: 12.88 | ade_val_g: 15.36 | fde_train_g: 25.93 | fde_val_g: 29.49\n",
      "e: 263 | loss: 1.42 | val_loss: 3.80 | ade_train_g: 12.98 | ade_val_g: 16.80 | fde_train_g: 26.18 | fde_val_g: 32.42\n",
      "e: 264 | loss: 1.40 | val_loss: 3.22 | ade_train_g: 12.84 | ade_val_g: 15.71 | fde_train_g: 25.79 | fde_val_g: 30.50\n",
      "e: 265 | loss: 1.40 | val_loss: 3.78 | ade_train_g: 12.86 | ade_val_g: 16.64 | fde_train_g: 25.83 | fde_val_g: 31.76\n",
      "e: 266 | loss: 1.40 | val_loss: 3.70 | ade_train_g: 12.84 | ade_val_g: 16.05 | fde_train_g: 25.81 | fde_val_g: 30.24\n",
      "e: 267 | loss: 1.40 | val_loss: 3.72 | ade_train_g: 12.87 | ade_val_g: 16.28 | fde_train_g: 25.87 | fde_val_g: 30.27\n",
      "e: 268 | loss: 1.40 | val_loss: 3.23 | ade_train_g: 12.81 | ade_val_g: 15.94 | fde_train_g: 25.72 | fde_val_g: 31.15\n",
      "e: 269 | loss: 1.40 | val_loss: 3.66 | ade_train_g: 12.83 | ade_val_g: 15.66 | fde_train_g: 25.77 | fde_val_g: 29.25\n",
      "Epoch   271: reducing learning rate of group 0 to 7.6294e-07.\n",
      "e: 270 | loss: 1.40 | val_loss: 3.69 | ade_train_g: 12.77 | ade_val_g: 16.10 | fde_train_g: 25.66 | fde_val_g: 30.15\n",
      "e: 271 | loss: 1.40 | val_loss: 3.24 | ade_train_g: 12.80 | ade_val_g: 16.22 | fde_train_g: 25.62 | fde_val_g: 30.91\n",
      "e: 272 | loss: 1.40 | val_loss: 3.65 | ade_train_g: 12.80 | ade_val_g: 15.47 | fde_train_g: 25.72 | fde_val_g: 28.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 273 | loss: 1.41 | val_loss: 4.67 | ade_train_g: 12.93 | ade_val_g: 16.45 | fde_train_g: 26.11 | fde_val_g: 29.48\n",
      "e: 274 | loss: 1.40 | val_loss: 3.35 | ade_train_g: 12.78 | ade_val_g: 17.16 | fde_train_g: 25.70 | fde_val_g: 33.70\n",
      "e: 275 | loss: 1.40 | val_loss: 3.68 | ade_train_g: 12.83 | ade_val_g: 15.67 | fde_train_g: 25.73 | fde_val_g: 29.40\n",
      "e: 276 | loss: 1.41 | val_loss: 3.85 | ade_train_g: 12.84 | ade_val_g: 17.52 | fde_train_g: 25.78 | fde_val_g: 33.57\n",
      "e: 277 | loss: 1.40 | val_loss: 3.68 | ade_train_g: 12.72 | ade_val_g: 15.80 | fde_train_g: 25.59 | fde_val_g: 29.74\n",
      "e: 278 | loss: 1.49 | val_loss: 3.69 | ade_train_g: 14.80 | ade_val_g: 15.95 | fde_train_g: 27.54 | fde_val_g: 29.60\n",
      "Epoch   280: reducing learning rate of group 0 to 3.8147e-07.\n",
      "e: 279 | loss: 1.41 | val_loss: 3.28 | ade_train_g: 12.89 | ade_val_g: 16.48 | fde_train_g: 25.88 | fde_val_g: 32.03\n",
      "e: 280 | loss: 1.42 | val_loss: 3.16 | ade_train_g: 12.99 | ade_val_g: 15.28 | fde_train_g: 26.28 | fde_val_g: 29.25\n",
      "e: 281 | loss: 1.51 | val_loss: 3.89 | ade_train_g: 14.99 | ade_val_g: 17.94 | fde_train_g: 28.06 | fde_val_g: 34.17\n",
      "e: 282 | loss: 1.40 | val_loss: 3.65 | ade_train_g: 12.91 | ade_val_g: 15.46 | fde_train_g: 26.02 | fde_val_g: 28.81\n",
      "e: 283 | loss: 1.40 | val_loss: 3.34 | ade_train_g: 12.85 | ade_val_g: 17.01 | fde_train_g: 25.63 | fde_val_g: 32.85\n",
      "e: 284 | loss: 1.40 | val_loss: 3.76 | ade_train_g: 12.85 | ade_val_g: 16.79 | fde_train_g: 25.84 | fde_val_g: 31.38\n",
      "e: 285 | loss: 1.41 | val_loss: 3.18 | ade_train_g: 12.99 | ade_val_g: 15.50 | fde_train_g: 26.19 | fde_val_g: 29.74\n",
      "e: 286 | loss: 1.41 | val_loss: 3.31 | ade_train_g: 12.87 | ade_val_g: 16.81 | fde_train_g: 25.87 | fde_val_g: 32.71\n",
      "e: 287 | loss: 1.41 | val_loss: 3.39 | ade_train_g: 12.93 | ade_val_g: 17.35 | fde_train_g: 26.10 | fde_val_g: 34.10\n",
      "Epoch   289: reducing learning rate of group 0 to 1.9073e-07.\n",
      "e: 288 | loss: 1.39 | val_loss: 3.35 | ade_train_g: 12.81 | ade_val_g: 17.09 | fde_train_g: 25.66 | fde_val_g: 33.84\n",
      "e: 289 | loss: 1.40 | val_loss: 3.26 | ade_train_g: 12.90 | ade_val_g: 16.28 | fde_train_g: 25.88 | fde_val_g: 31.79\n",
      "e: 290 | loss: 1.40 | val_loss: 3.36 | ade_train_g: 12.82 | ade_val_g: 17.27 | fde_train_g: 25.80 | fde_val_g: 34.00\n",
      "e: 291 | loss: 1.41 | val_loss: 3.19 | ade_train_g: 12.93 | ade_val_g: 15.50 | fde_train_g: 26.09 | fde_val_g: 29.81\n",
      "e: 292 | loss: 1.40 | val_loss: 3.25 | ade_train_g: 12.81 | ade_val_g: 16.15 | fde_train_g: 25.80 | fde_val_g: 31.37\n",
      "e: 293 | loss: 1.41 | val_loss: 3.27 | ade_train_g: 12.84 | ade_val_g: 16.41 | fde_train_g: 25.86 | fde_val_g: 31.74\n",
      "e: 294 | loss: 1.39 | val_loss: 3.71 | ade_train_g: 12.78 | ade_val_g: 15.91 | fde_train_g: 25.68 | fde_val_g: 29.94\n",
      "e: 295 | loss: 1.40 | val_loss: 3.20 | ade_train_g: 12.81 | ade_val_g: 15.59 | fde_train_g: 25.68 | fde_val_g: 29.90\n",
      "e: 296 | loss: 1.41 | val_loss: 3.19 | ade_train_g: 12.89 | ade_val_g: 15.56 | fde_train_g: 25.97 | fde_val_g: 29.54\n",
      "Epoch   298: reducing learning rate of group 0 to 9.5367e-08.\n",
      "e: 297 | loss: 1.40 | val_loss: 3.16 | ade_train_g: 12.84 | ade_val_g: 15.28 | fde_train_g: 25.73 | fde_val_g: 29.12\n",
      "e: 298 | loss: 1.40 | val_loss: 3.31 | ade_train_g: 12.87 | ade_val_g: 16.83 | fde_train_g: 25.88 | fde_val_g: 32.88\n",
      "e: 299 | loss: 1.39 | val_loss: 3.16 | ade_train_g: 12.74 | ade_val_g: 15.35 | fde_train_g: 25.54 | fde_val_g: 29.33\n",
      "====================================================================================================\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "print('='*100)\n",
    "print('Training ...')\n",
    "\n",
    "train_p_scores=[]\n",
    "val_p_scores=[]\n",
    "alpha=1#0.4\n",
    "l1e = nn.L1Loss()\n",
    "train_s_scores = []\n",
    "train_pose_scores=[]\n",
    "val_pose_scores=[]\n",
    "train_c_scores = []\n",
    "val_s_scores   = []\n",
    "val_c_scores   = []\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr= 0.1)\n",
    "# optimizer = optim.Adadelta(net.parameters(),lr= 0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=8, \n",
    "                                                 threshold = 1e-12, verbose=True)\n",
    "\n",
    "for epoch in range(300):\n",
    "    start = time.time()\n",
    "    \n",
    "    avg_epoch_train_p_loss   = 0\n",
    "    avg_epoch_val_p_loss     = 0 \n",
    "    ade_g  = 0\n",
    "    fde_g  = 0\n",
    "    ade_train_g  = 0\n",
    "    fde_train_g  = 0\n",
    "    counter = 0\n",
    "    net.train()\n",
    "    \n",
    "    for idx, (obs_pose_global, target_pose_global,obs_s_global,target_s_global) in enumerate(train_loader):\n",
    "        obs_pose_global = obs_pose_global.to(device='cuda')\n",
    "        target_pose_global = target_pose_global.to(device='cuda')\n",
    "        obs_s_global = obs_s_global.to(device='cuda')\n",
    "        target_s_global = target_s_global.to(device='cuda')\n",
    "        \n",
    "        counter += 1        \n",
    "        \n",
    "        net.zero_grad()\n",
    "#         print(obs_pose_global)\n",
    "        (preds,) = net(pose=obs_s_global[:,:,:2])\n",
    "        preds_g=speed2pos(preds,obs_pose_global)\n",
    "        \n",
    "        ade_train_g += float(ADE_c(preds_g, target_pose_global))\n",
    "        fde_train_g += float(FDE_c(preds_g, target_pose_global))\n",
    "      \n",
    "        loss_g  = l1e(preds, target_s_global)\n",
    "        \n",
    "        loss=alpha*loss_g\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_epoch_train_p_loss += float(loss)\n",
    "\n",
    "    avg_epoch_train_p_loss /= counter\n",
    "    ade_train_g  /= counter\n",
    "    fde_train_g  /= counter   \n",
    "    \n",
    "\n",
    "    counter=0\n",
    "    net.eval()\n",
    "    for idx, (obs_pose_global, target_pose_global,obs_s_global,target_s_global) in enumerate(val_loader):\n",
    "        obs_pose_global = obs_pose_global.to(device='cuda')\n",
    "        target_pose_global = target_pose_global.to(device='cuda')\n",
    "        obs_s_global = obs_s_global.to(device='cuda')\n",
    "        target_s_global = target_s_global.to(device='cuda')\n",
    "    \n",
    "        counter += 1        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            (preds,) = net(pose=obs_s_global[:,:,:2])\n",
    "            preds_g=speed2pos(preds,obs_pose_global)\n",
    "        \n",
    "            ade_g += float(ADE_c(preds_g, target_pose_global))\n",
    "            fde_g += float(FDE_c(preds_g, target_pose_global))\n",
    "\n",
    "            loss_g  = l1e(preds, target_s_global)\n",
    "\n",
    "            val_loss=alpha*loss_g\n",
    "\n",
    "            avg_epoch_val_p_loss += float(val_loss)\n",
    "      \n",
    "    avg_epoch_val_p_loss /= counter\n",
    "    val_p_scores.append(avg_epoch_val_p_loss)\n",
    "    ade_g  /= counter\n",
    "    fde_g  /= counter    \n",
    "   \n",
    "    \n",
    "    \n",
    "    scheduler.step(avg_epoch_train_p_loss)\n",
    "    \n",
    "    print('e:', epoch,'| loss: %.2f'%avg_epoch_train_p_loss,'| val_loss: %.2f'% avg_epoch_val_p_loss, '| ade_train_g: %.2f'% ade_train_g, '| ade_val_g: %.2f'% ade_g, '| fde_train_g: %.2f'% fde_train_g,'| fde_val_g: %.2f'% fde_g)\n",
    "\n",
    "\n",
    "print('='*100) \n",
    "# print('Saving ...')\n",
    "# torch.save(net.state_dict(), args.model_path)\n",
    "print('Done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "julian-tokyo",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 9, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-4b697922716b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mobs_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_pose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_pose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_pose_global\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_pose_global\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs_s_g\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_s_g\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mobs_pose_global\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs_pose_global\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mtarget_pose_global\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_pose_global\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 9, got 4)"
     ]
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    start = time.time()\n",
    "    \n",
    "    avg_epoch_train_p_loss   = 0\n",
    "    avg_epoch_val_p_loss     = 0 \n",
    "    ade_g  = 0\n",
    "    fde_g  = 0\n",
    "    ade_train_g  = 0\n",
    "    fde_train_g  = 0\n",
    "    counter = 0\n",
    "    net.train()\n",
    "    \n",
    "    for idx, ( obs_pose,_,_,target_pose_act) in enumerate(train_loader):\n",
    "            obs_pose_global = obs_pose_global.to(device='cuda')\n",
    "            target_pose_global = target_pose_global.to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "desperate-restaurant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pose loaded\n",
      "Future_Pose loaded\n",
      "Pose_image loaded\n",
      "Future_image loaded\n",
      "******************************\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n"
     ]
    }
   ],
   "source": [
    "dtype=\"train\"\n",
    "sequence_centric = pd.read_csv(\"sequences_openpifpaf_thres4_wimage_\"+dtype+\".csv\")\n",
    "df = sequence_centric.copy()      \n",
    "for v in list(df.columns.values):\n",
    "    print(v+' loaded')\n",
    "    try:\n",
    "        df.loc[:,v] = df.loc[:, v].apply(lambda x: literal_eval(x))\n",
    "    except:\n",
    "        continue\n",
    "sequence_centric[df.columns] = df[df.columns]\n",
    "data = sequence_centric.copy().reset_index(drop=True)\n",
    "print('*'*30)\n",
    "\n",
    "obs_pose=[]\n",
    "true_pose=[]\n",
    "\n",
    "outputs=[]\n",
    "\n",
    "for index in range(len(data)):\n",
    "\n",
    "    seq = data.iloc[index]\n",
    "    obs_p=[]\n",
    "    true_p=[]\n",
    "    w=0\n",
    "    h=0\n",
    "    egomotion=np.loadtxt(\"./egomotion/\"+seq.Pose_image[0].replace('/frames/','').replace('/','-').replace('.png','')+'_to_'+seq.Pose_image[-1].replace(\"/frames/\",\"\").replace('/','-').replace('.png','.txt'))\n",
    "    for i in range(0,16,1):\n",
    "        image = seq.Pose_image[i].replace(\"/frames\",\"./frames\")\n",
    "        depth_image=image.replace(\"frames\",\"depth\").replace(\".png\",\"_disp.png\")\n",
    "        im=cv2.imread(image,0)\n",
    "        imd=cv2.imread(depth_image,0)\n",
    "        h,w=im.shape\n",
    "        hd,wd=imd.shape\n",
    "        pose=seq.Pose[i][:2]    \n",
    "    #                 px=pose[0]/w\n",
    "    #                 py=pose[1]/h\n",
    "        x=int(pose[0]*wd/w)\n",
    "        y=int(pose[1]*hd/h)\n",
    "    #                 pose=[px,py]\n",
    "        pose.append(imd[y,x])\n",
    "        pose.extend(egomotion[i])\n",
    "        obs_p.append(pose)\n",
    "#         obs_act.append([w,h])\n",
    "\n",
    "        true=seq.Future_Pose[i][:2]\n",
    "#         true_p.append([true[0]/w, true[1]/h])\n",
    "        true_p.append(true)\n",
    "    \n",
    "#     print(len(obs_p[0]))\n",
    "#     print(len(true_p))\n",
    "    \n",
    "    obs_pose.append(obs_p)\n",
    "    true_pose.append(true_p)\n",
    "    print(index)\n",
    "    \n",
    "#     break\n",
    "\n",
    "# print(len(obs_pose),len(true_pose))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "pursuant-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Pose': obs_pose,\n",
    "        'Future_Pose': true_pose,\n",
    "        }\n",
    "\n",
    "df_train = pd.DataFrame (data, columns = ['Pose','Future_Pose'])\n",
    "\n",
    "df_train.to_csv(\"./glob_depth_train.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype=\"val\"\n",
    "sequence_centric = pd.read_csv(\"sequences_openpifpaf_thres4_wimage_\"+dtype+\".csv\")\n",
    "df = sequence_centric.copy()      \n",
    "for v in list(df.columns.values):\n",
    "    print(v+' loaded')\n",
    "    try:\n",
    "        df.loc[:,v] = df.loc[:, v].apply(lambda x: literal_eval(x))\n",
    "    except:\n",
    "        continue\n",
    "sequence_centric[df.columns] = df[df.columns]\n",
    "data = sequence_centric.copy().reset_index(drop=True)\n",
    "print('*'*30)\n",
    "\n",
    "obs_pose_val=[]\n",
    "true_pose_val=[]\n",
    "\n",
    "outputs=[]\n",
    "\n",
    "for index in range(len(data)):\n",
    "\n",
    "    seq = data.iloc[index]\n",
    "    obs_p=[]\n",
    "    true_p=[]\n",
    "    w=0\n",
    "    h=0\n",
    "#     print(seq.Pose)\n",
    "    egomotion=np.loadtxt(\"./egomotion/\"+seq.Pose_image[0].replace('/frames/','').replace('/','-').replace('.png','')+'_to_'+seq.Pose_image[-1].replace(\"/frames/\",\"\").replace('/','-').replace('.png','.txt'))\n",
    "    for i in range(0,16,1):\n",
    "        image = seq.Pose_image[i].replace(\"/frames\",\"./frames\")\n",
    "        depth_image=image.replace(\"frames\",\"depth\").replace(\".png\",\"_disp.png\")\n",
    "        im=cv2.imread(image,0)\n",
    "        imd=cv2.imread(depth_image,0)\n",
    "        h,w=im.shape\n",
    "        hd,wd=imd.shape\n",
    "        pose=seq.Pose[i][:2]    \n",
    "    #                 px=pose[0]/w\n",
    "    #                 py=pose[1]/h\n",
    "        x=int(pose[0]*wd/w)\n",
    "        y=int(pose[1]*hd/h)\n",
    "    #                 pose=[px,py]\n",
    "        pose.append(imd[y,x])\n",
    "        pose.extend(egomotion[i])\n",
    "        print(egomotion[i])\n",
    "        obs_p.append(pose)\n",
    "#         obs_act.append([w,h])\n",
    "\n",
    "        true=seq.Future_Pose[i][:2]\n",
    "#         true_p.append([true[0]/w, true[1]/h])\n",
    "        true_p.append(true)\n",
    "    \n",
    "#     print(len(obs_p[0]))\n",
    "#     print(len(true_p))\n",
    "    \n",
    "    obs_pose_val.append(obs_p)\n",
    "    true_pose_val.append(true_p)\n",
    "    print(index)\n",
    "    \n",
    "    break\n",
    "\n",
    "# print(len(obs_pose),len(true_pose))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "pediatric-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = {'Pose': obs_pose_val,\n",
    "        'Future_Pose': true_pose_val,\n",
    "        }\n",
    "df_val = pd.DataFrame (data_val, columns = ['Pose','Future_Pose'])\n",
    "df_val.to_csv(\"./glob_depth_val.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "identical-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class myDataset_DE_depth(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, dtype):\n",
    "        \n",
    "        self.args = args\n",
    "        self.dtype = dtype\n",
    "        print(\"Loading\",self.dtype)\n",
    "        \n",
    "        sequence_centric = pd.read_csv(\"glob_depth_\"+self.dtype+\".csv\")\n",
    "#         sequences_openpifpaf_thres4_wconfsc_wimage_train\n",
    "#         sequence_centric = pd.read_csv(\"sequences_16_overlap_4_thres4_\"+self.dtype+\".csv\")\n",
    "\n",
    "        df = sequence_centric.copy()      \n",
    "        for v in list(df.columns.values):\n",
    "            print(v+' loaded')\n",
    "            try:\n",
    "                df.loc[:,v] = df.loc[:, v].apply(lambda x: literal_eval(x))\n",
    "            except:\n",
    "                continue\n",
    "        sequence_centric[df.columns] = df[df.columns]\n",
    "        self.data = sequence_centric.copy().reset_index(drop=True)\n",
    "        \n",
    "        print('*'*30)\n",
    "        \n",
    "        self.obs=self.data.Pose\n",
    "        self.true=self.data.Future_Pose\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "#         print(index)\n",
    "        \n",
    "        outputs=[]\n",
    "        \n",
    "        obs=torch.tensor(self.obs[index],dtype=torch.double)\n",
    "        true=torch.tensor(self.true[index],dtype=torch.double)\n",
    "        \n",
    "        outputs.append(obs[:,:2])\n",
    "        outputs.append(true)\n",
    "#         print(obs[1,2:])\n",
    "        obs_speed = (obs[1:,:2] - obs[:-1,:2])\n",
    "#         print(obs_speed)\n",
    "        obs_speed=torch.cat((obs_speed,obs[1:,2:]),dim=1)\n",
    "#         print(obs_speed[0,2:])\n",
    "        \n",
    "        outputs.append(obs_speed)\n",
    "        \n",
    "        true_speed = torch.cat(((true[0,:2]-obs[-1,:2]).unsqueeze(0), true[1:,:2]-true[:-1,:2]))\n",
    "        outputs.append(true_speed)\n",
    "#         outputs.append(obs)\n",
    "#         outputs.append(true)\n",
    "       \n",
    "       \n",
    "        return tuple(outputs)    \n",
    "    \n",
    "    \n",
    "def data_loader_DE_depth(args,data):\n",
    "    dataset = myDataset_DE_depth(args,data)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=args.batch_size, shuffle=args.loader_shuffle,\n",
    "        pin_memory=args.pin_memory)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "#Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "printable-ethnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train\n",
      "Pose loaded\n",
      "Future_Pose loaded\n",
      "******************************\n",
      "Loading val\n",
      "Pose loaded\n",
      "Future_Pose loaded\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class args():\n",
    "    def __init__(self):\n",
    "       \n",
    "        self.dtype        = 'train'\n",
    "        self.loader_workers = 1\n",
    "        self.loader_shuffle = True\n",
    "        self.pin_memory     = False\n",
    "        self.device         = 'cuda'\n",
    "        self.batch_size     = 50\n",
    "        self.input  = 16\n",
    "        self.output = 16\n",
    "        self.stride = 16\n",
    "        self.skip   = 1\n",
    "        \n",
    "args = args()\n",
    "\n",
    "train_loader=data_loader_DE_depth(args,\"train\")\n",
    "val_loader=data_loader_DE_depth(args,\"val\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4,sci_mode =False)\n",
    "for idx, (obs_pose_global, target_pose_global,obs_s_global,target_s_global) in enumerate(val_loader):\n",
    "    obs_pose_global = obs_pose_global.to(device='cuda')\n",
    "    target_pose_global = target_pose_global.to(device='cuda')\n",
    "    obs_s_global = obs_s_global.to(device='cuda')\n",
    "    target_s_global = target_s_global.to(device='cuda')\n",
    "#     (preds_g,) = net(pose=obs_pose_global)\n",
    "    print(obs_pose_global[0])\n",
    "    print(target_pose_global[0])\n",
    "    print(obs_s_global[0])\n",
    "    print(target_s_global[0])\n",
    "#     print(target_pose_act[1][0])\n",
    "#     print(preds_g[1][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "covered-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class glob_LSTM_DE(nn.Module):    \n",
    "    def __init__(self, args):\n",
    "        '''\n",
    "           input: \n",
    "           output: \n",
    "        '''\n",
    "        super(glob_LSTM_DE, self).__init__()\n",
    "         \n",
    "        \n",
    "        self.DE_encoder = nn.LSTM(input_size=15, hidden_size=100)\n",
    "        self.DE_decoder = nn.LSTMCell(input_size=15, hidden_size=100)\n",
    "#         self.vel_decoder = nn.LSTMCell(input_size=self.encoded_size, hidden_size=args.hidden_size)\n",
    "\n",
    "        self.fc_DE  = nn.Linear(in_features=100, out_features=15)\n",
    "        self.fc2_DE  = nn.Linear(in_features=15, out_features=2)\n",
    "        \n",
    "        self.hardtanh = nn.Hardtanh(min_val=-1*100,max_val=100)\n",
    "        self.relu = nn.ReLU() \n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "    def forward(self, pose=None):\n",
    "        \n",
    "        outputs = []        \n",
    "        _, (hidden_dec,cell_dec) = self.DE_encoder(pose.permute(1,0,2))    \n",
    "        hidden_dec=hidden_dec.squeeze(0)\n",
    "        cell_dec=cell_dec.squeeze(0)\n",
    "#         print(cell_dec.size())\n",
    "#         print(pose.shape)\n",
    "#         print(pose[1,-1])\n",
    "        DEDec_inp = pose[:,-1,:]\n",
    "       \n",
    "        DE_outputs = torch.tensor([], device=self.args.device,dtype=torch.double)\n",
    "        \n",
    "#         print(DEDec_inp.shape)\n",
    "        for i in range(self.args.output//self.args.skip):\n",
    "#             print(hidden_dec[0].shape,hidden_dec[1].shape)\n",
    "            (hidden_dec,cell_dec) = self.DE_decoder(DEDec_inp, (hidden_dec,cell_dec)) #decoder_output, \n",
    "            DE_output_t  = self.fc_DE(hidden_dec)\n",
    "            DE_output  = self.fc2_DE(DE_output_t)\n",
    "#             print(DE_output_t.shape)\n",
    "            DE_outputs = torch.cat((DE_outputs, DE_output.unsqueeze(1)), dim = 1)\n",
    "            DEDec_inp  = DE_output_t.detach()\n",
    "            \n",
    "            \n",
    "#         print(DE_outputs.shape).permute(1,0,2)\n",
    "        outputs.append(DE_outputs)\n",
    "            \n",
    "        return tuple(outputs)\n",
    "\n",
    "net=glob_LSTM_DE(args).to(args.device).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "indie-richmond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Training ...\n",
      "e: 0 | loss: 3.11 | val_loss: 4.91 | ade_train_g: 39.43 | ade_val_g: 38.73 | fde_train_g: 76.76 | fde_val_g: 72.26\n",
      "e: 1 | loss: 2.84 | val_loss: 4.91 | ade_train_g: 37.63 | ade_val_g: 38.51 | fde_train_g: 72.41 | fde_val_g: 71.96\n",
      "e: 2 | loss: 2.79 | val_loss: 4.42 | ade_train_g: 37.02 | ade_val_g: 38.95 | fde_train_g: 71.30 | fde_val_g: 73.45\n",
      "e: 3 | loss: 2.79 | val_loss: 4.30 | ade_train_g: 36.95 | ade_val_g: 36.71 | fde_train_g: 71.41 | fde_val_g: 69.16\n",
      "e: 4 | loss: 2.73 | val_loss: 4.03 | ade_train_g: 36.19 | ade_val_g: 32.82 | fde_train_g: 69.51 | fde_val_g: 60.27\n",
      "e: 5 | loss: 2.60 | val_loss: 4.79 | ade_train_g: 33.06 | ade_val_g: 36.30 | fde_train_g: 63.94 | fde_val_g: 67.39\n",
      "e: 6 | loss: 2.51 | val_loss: 4.74 | ade_train_g: 31.54 | ade_val_g: 33.07 | fde_train_g: 60.83 | fde_val_g: 62.64\n",
      "e: 7 | loss: 2.60 | val_loss: 5.18 | ade_train_g: 32.33 | ade_val_g: 35.37 | fde_train_g: 63.14 | fde_val_g: 65.37\n",
      "e: 8 | loss: 2.83 | val_loss: 4.39 | ade_train_g: 36.59 | ade_val_g: 36.06 | fde_train_g: 70.42 | fde_val_g: 67.69\n",
      "e: 9 | loss: 2.62 | val_loss: 3.91 | ade_train_g: 33.28 | ade_val_g: 28.47 | fde_train_g: 62.85 | fde_val_g: 53.64\n",
      "e: 10 | loss: 2.45 | val_loss: 3.93 | ade_train_g: 29.16 | ade_val_g: 30.23 | fde_train_g: 57.89 | fde_val_g: 57.02\n",
      "e: 11 | loss: 2.27 | val_loss: 3.96 | ade_train_g: 27.60 | ade_val_g: 30.13 | fde_train_g: 53.98 | fde_val_g: 57.04\n",
      "e: 12 | loss: 2.26 | val_loss: 3.88 | ade_train_g: 27.44 | ade_val_g: 29.07 | fde_train_g: 53.33 | fde_val_g: 55.56\n",
      "e: 13 | loss: 2.33 | val_loss: 3.99 | ade_train_g: 28.81 | ade_val_g: 31.01 | fde_train_g: 55.94 | fde_val_g: 58.50\n",
      "e: 14 | loss: 2.52 | val_loss: 4.70 | ade_train_g: 32.49 | ade_val_g: 34.99 | fde_train_g: 60.53 | fde_val_g: 64.65\n",
      "e: 15 | loss: 2.46 | val_loss: 3.98 | ade_train_g: 29.71 | ade_val_g: 31.23 | fde_train_g: 59.32 | fde_val_g: 58.75\n",
      "e: 16 | loss: 2.36 | val_loss: 3.98 | ade_train_g: 29.20 | ade_val_g: 30.91 | fde_train_g: 56.74 | fde_val_g: 58.60\n",
      "e: 17 | loss: 2.76 | val_loss: 4.62 | ade_train_g: 33.69 | ade_val_g: 30.91 | fde_train_g: 67.15 | fde_val_g: 58.86\n",
      "e: 18 | loss: 2.44 | val_loss: 3.97 | ade_train_g: 28.63 | ade_val_g: 30.03 | fde_train_g: 56.04 | fde_val_g: 56.33\n",
      "e: 19 | loss: 2.31 | val_loss: 3.78 | ade_train_g: 27.65 | ade_val_g: 27.33 | fde_train_g: 53.28 | fde_val_g: 50.23\n",
      "e: 20 | loss: 2.22 | val_loss: 3.91 | ade_train_g: 26.00 | ade_val_g: 27.17 | fde_train_g: 50.26 | fde_val_g: 53.39\n",
      "e: 21 | loss: 2.28 | val_loss: 4.50 | ade_train_g: 27.12 | ade_val_g: 29.87 | fde_train_g: 53.07 | fde_val_g: 57.55\n",
      "e: 22 | loss: 2.34 | val_loss: 4.01 | ade_train_g: 28.42 | ade_val_g: 30.23 | fde_train_g: 55.81 | fde_val_g: 58.32\n",
      "e: 23 | loss: 2.30 | val_loss: 3.82 | ade_train_g: 27.65 | ade_val_g: 27.64 | fde_train_g: 54.47 | fde_val_g: 53.00\n",
      "e: 24 | loss: 2.27 | val_loss: 4.38 | ade_train_g: 26.81 | ade_val_g: 27.50 | fde_train_g: 53.09 | fde_val_g: 53.50\n",
      "e: 25 | loss: 2.17 | val_loss: 3.67 | ade_train_g: 25.14 | ade_val_g: 24.80 | fde_train_g: 49.94 | fde_val_g: 47.03\n",
      "e: 26 | loss: 2.13 | val_loss: 3.74 | ade_train_g: 24.62 | ade_val_g: 26.26 | fde_train_g: 48.38 | fde_val_g: 49.68\n",
      "e: 27 | loss: 2.29 | val_loss: 3.93 | ade_train_g: 27.97 | ade_val_g: 28.85 | fde_train_g: 54.37 | fde_val_g: 54.59\n",
      "e: 28 | loss: 2.24 | val_loss: 3.62 | ade_train_g: 26.70 | ade_val_g: 24.74 | fde_train_g: 51.91 | fde_val_g: 46.32\n",
      "e: 29 | loss: 2.32 | val_loss: 4.35 | ade_train_g: 28.84 | ade_val_g: 29.27 | fde_train_g: 55.83 | fde_val_g: 53.65\n",
      "e: 30 | loss: 2.27 | val_loss: 3.65 | ade_train_g: 27.75 | ade_val_g: 25.20 | fde_train_g: 53.77 | fde_val_g: 47.05\n",
      "e: 31 | loss: 2.06 | val_loss: 4.15 | ade_train_g: 23.96 | ade_val_g: 23.21 | fde_train_g: 46.32 | fde_val_g: 45.83\n",
      "e: 32 | loss: 2.06 | val_loss: 4.18 | ade_train_g: 23.36 | ade_val_g: 24.24 | fde_train_g: 45.59 | fde_val_g: 46.71\n",
      "e: 33 | loss: 2.04 | val_loss: 4.19 | ade_train_g: 22.93 | ade_val_g: 25.83 | fde_train_g: 44.95 | fde_val_g: 48.51\n",
      "e: 34 | loss: 1.98 | val_loss: 3.60 | ade_train_g: 22.40 | ade_val_g: 24.26 | fde_train_g: 43.79 | fde_val_g: 45.62\n",
      "e: 35 | loss: 2.11 | val_loss: 3.74 | ade_train_g: 24.65 | ade_val_g: 26.19 | fde_train_g: 48.22 | fde_val_g: 50.36\n",
      "e: 36 | loss: 2.17 | val_loss: 3.64 | ade_train_g: 24.36 | ade_val_g: 24.88 | fde_train_g: 49.22 | fde_val_g: 47.03\n",
      "e: 37 | loss: 2.03 | val_loss: 3.59 | ade_train_g: 23.22 | ade_val_g: 23.99 | fde_train_g: 45.70 | fde_val_g: 45.51\n",
      "e: 38 | loss: 1.97 | val_loss: 4.14 | ade_train_g: 22.51 | ade_val_g: 25.07 | fde_train_g: 43.97 | fde_val_g: 47.45\n",
      "e: 39 | loss: 2.10 | val_loss: 3.94 | ade_train_g: 24.87 | ade_val_g: 29.76 | fde_train_g: 48.41 | fde_val_g: 57.02\n",
      "e: 40 | loss: 2.14 | val_loss: 3.69 | ade_train_g: 25.51 | ade_val_g: 25.84 | fde_train_g: 49.88 | fde_val_g: 48.44\n",
      "e: 41 | loss: 2.07 | val_loss: 4.26 | ade_train_g: 24.62 | ade_val_g: 26.70 | fde_train_g: 47.14 | fde_val_g: 50.37\n",
      "e: 42 | loss: 2.05 | val_loss: 4.14 | ade_train_g: 24.04 | ade_val_g: 25.51 | fde_train_g: 46.46 | fde_val_g: 47.41\n",
      "e: 43 | loss: 2.12 | val_loss: 3.73 | ade_train_g: 25.43 | ade_val_g: 26.50 | fde_train_g: 49.00 | fde_val_g: 50.29\n",
      "e: 44 | loss: 2.17 | val_loss: 3.61 | ade_train_g: 26.10 | ade_val_g: 24.61 | fde_train_g: 50.87 | fde_val_g: 45.78\n",
      "e: 45 | loss: 2.02 | val_loss: 3.48 | ade_train_g: 23.49 | ade_val_g: 22.75 | fde_train_g: 45.91 | fde_val_g: 42.64\n",
      "e: 46 | loss: 2.04 | val_loss: 3.67 | ade_train_g: 23.77 | ade_val_g: 25.67 | fde_train_g: 45.99 | fde_val_g: 48.76\n",
      "e: 47 | loss: 2.13 | val_loss: 4.21 | ade_train_g: 24.67 | ade_val_g: 25.63 | fde_train_g: 48.37 | fde_val_g: 48.52\n",
      "e: 48 | loss: 1.90 | val_loss: 4.24 | ade_train_g: 21.37 | ade_val_g: 25.00 | fde_train_g: 41.45 | fde_val_g: 50.21\n",
      "e: 49 | loss: 2.06 | val_loss: 4.39 | ade_train_g: 24.07 | ade_val_g: 29.25 | fde_train_g: 46.93 | fde_val_g: 54.94\n",
      "e: 50 | loss: 2.26 | val_loss: 3.97 | ade_train_g: 27.54 | ade_val_g: 30.46 | fde_train_g: 53.37 | fde_val_g: 57.97\n",
      "e: 51 | loss: 2.29 | val_loss: 4.44 | ade_train_g: 28.22 | ade_val_g: 30.48 | fde_train_g: 55.07 | fde_val_g: 57.65\n",
      "e: 52 | loss: 2.24 | val_loss: 4.29 | ade_train_g: 27.31 | ade_val_g: 27.37 | fde_train_g: 53.48 | fde_val_g: 51.79\n",
      "e: 53 | loss: 2.22 | val_loss: 3.65 | ade_train_g: 26.04 | ade_val_g: 23.84 | fde_train_g: 50.98 | fde_val_g: 45.29\n",
      "Epoch    55: reducing learning rate of group 0 to 5.0000e-02.\n",
      "e: 54 | loss: 2.22 | val_loss: 3.65 | ade_train_g: 26.74 | ade_val_g: 24.89 | fde_train_g: 50.06 | fde_val_g: 47.78\n",
      "e: 55 | loss: 2.02 | val_loss: 4.05 | ade_train_g: 23.25 | ade_val_g: 22.82 | fde_train_g: 45.16 | fde_val_g: 42.46\n",
      "e: 56 | loss: 2.01 | val_loss: 3.96 | ade_train_g: 22.68 | ade_val_g: 21.73 | fde_train_g: 43.95 | fde_val_g: 40.44\n",
      "e: 57 | loss: 1.95 | val_loss: 3.57 | ade_train_g: 21.84 | ade_val_g: 23.25 | fde_train_g: 42.65 | fde_val_g: 44.37\n",
      "e: 58 | loss: 2.09 | val_loss: 3.54 | ade_train_g: 24.64 | ade_val_g: 23.00 | fde_train_g: 47.82 | fde_val_g: 43.43\n",
      "e: 59 | loss: 1.98 | val_loss: 3.42 | ade_train_g: 22.76 | ade_val_g: 21.42 | fde_train_g: 44.24 | fde_val_g: 39.56\n",
      "e: 60 | loss: 1.88 | val_loss: 3.51 | ade_train_g: 21.09 | ade_val_g: 22.63 | fde_train_g: 40.82 | fde_val_g: 42.15\n",
      "e: 61 | loss: 2.01 | val_loss: 4.05 | ade_train_g: 23.22 | ade_val_g: 23.77 | fde_train_g: 44.89 | fde_val_g: 44.02\n",
      "e: 62 | loss: 2.00 | val_loss: 3.67 | ade_train_g: 23.10 | ade_val_g: 25.87 | fde_train_g: 44.78 | fde_val_g: 48.07\n",
      "e: 63 | loss: 2.00 | val_loss: 3.95 | ade_train_g: 23.55 | ade_val_g: 22.17 | fde_train_g: 45.30 | fde_val_g: 40.60\n",
      "e: 64 | loss: 1.92 | val_loss: 3.98 | ade_train_g: 21.30 | ade_val_g: 22.23 | fde_train_g: 41.13 | fde_val_g: 41.35\n",
      "e: 65 | loss: 2.00 | val_loss: 3.69 | ade_train_g: 22.79 | ade_val_g: 25.00 | fde_train_g: 44.29 | fde_val_g: 48.05\n",
      "e: 66 | loss: 2.04 | val_loss: 4.09 | ade_train_g: 23.91 | ade_val_g: 24.37 | fde_train_g: 46.37 | fde_val_g: 45.29\n",
      "e: 67 | loss: 1.95 | val_loss: 4.00 | ade_train_g: 22.39 | ade_val_g: 23.12 | fde_train_g: 43.21 | fde_val_g: 43.34\n",
      "e: 68 | loss: 1.94 | val_loss: 3.27 | ade_train_g: 22.06 | ade_val_g: 18.86 | fde_train_g: 42.97 | fde_val_g: 35.03\n",
      "e: 69 | loss: 1.91 | val_loss: 3.92 | ade_train_g: 21.52 | ade_val_g: 21.32 | fde_train_g: 41.73 | fde_val_g: 40.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 70 | loss: 2.00 | val_loss: 3.69 | ade_train_g: 22.91 | ade_val_g: 25.87 | fde_train_g: 44.81 | fde_val_g: 49.41\n",
      "e: 71 | loss: 2.29 | val_loss: 3.82 | ade_train_g: 26.94 | ade_val_g: 28.22 | fde_train_g: 53.97 | fde_val_g: 53.66\n",
      "e: 72 | loss: 2.18 | val_loss: 4.05 | ade_train_g: 24.71 | ade_val_g: 24.39 | fde_train_g: 50.06 | fde_val_g: 45.55\n",
      "e: 73 | loss: 1.97 | val_loss: 4.06 | ade_train_g: 22.82 | ade_val_g: 24.58 | fde_train_g: 44.76 | fde_val_g: 45.49\n",
      "e: 74 | loss: 2.00 | val_loss: 3.58 | ade_train_g: 23.20 | ade_val_g: 24.20 | fde_train_g: 44.94 | fde_val_g: 45.74\n",
      "e: 75 | loss: 1.96 | val_loss: 3.48 | ade_train_g: 22.53 | ade_val_g: 23.20 | fde_train_g: 44.18 | fde_val_g: 43.16\n",
      "e: 76 | loss: 1.94 | val_loss: 3.51 | ade_train_g: 22.03 | ade_val_g: 22.60 | fde_train_g: 43.63 | fde_val_g: 43.36\n",
      "Epoch    78: reducing learning rate of group 0 to 2.5000e-02.\n",
      "e: 77 | loss: 2.02 | val_loss: 3.55 | ade_train_g: 23.31 | ade_val_g: 23.30 | fde_train_g: 45.91 | fde_val_g: 44.14\n",
      "e: 78 | loss: 1.89 | val_loss: 4.41 | ade_train_g: 21.16 | ade_val_g: 21.77 | fde_train_g: 41.68 | fde_val_g: 39.63\n",
      "e: 79 | loss: 1.85 | val_loss: 3.39 | ade_train_g: 20.88 | ade_val_g: 20.82 | fde_train_g: 40.65 | fde_val_g: 39.36\n",
      "e: 80 | loss: 1.85 | val_loss: 3.83 | ade_train_g: 20.48 | ade_val_g: 20.50 | fde_train_g: 40.06 | fde_val_g: 37.74\n",
      "e: 81 | loss: 1.82 | val_loss: 3.47 | ade_train_g: 20.67 | ade_val_g: 22.34 | fde_train_g: 40.03 | fde_val_g: 42.11\n",
      "e: 82 | loss: 1.84 | val_loss: 3.90 | ade_train_g: 20.46 | ade_val_g: 21.82 | fde_train_g: 39.83 | fde_val_g: 40.38\n",
      "e: 83 | loss: 1.85 | val_loss: 3.39 | ade_train_g: 20.71 | ade_val_g: 21.00 | fde_train_g: 40.11 | fde_val_g: 39.60\n",
      "e: 84 | loss: 1.82 | val_loss: 3.95 | ade_train_g: 20.13 | ade_val_g: 22.70 | fde_train_g: 39.27 | fde_val_g: 41.95\n",
      "e: 85 | loss: 1.78 | val_loss: 3.38 | ade_train_g: 19.62 | ade_val_g: 20.77 | fde_train_g: 38.37 | fde_val_g: 39.45\n",
      "Epoch    87: reducing learning rate of group 0 to 1.2500e-02.\n",
      "e: 86 | loss: 1.77 | val_loss: 3.84 | ade_train_g: 19.49 | ade_val_g: 20.79 | fde_train_g: 38.12 | fde_val_g: 38.25\n",
      "e: 87 | loss: 1.77 | val_loss: 3.32 | ade_train_g: 19.47 | ade_val_g: 20.09 | fde_train_g: 38.24 | fde_val_g: 37.55\n",
      "e: 88 | loss: 1.76 | val_loss: 3.86 | ade_train_g: 19.14 | ade_val_g: 20.84 | fde_train_g: 37.46 | fde_val_g: 38.53\n",
      "e: 89 | loss: 1.75 | val_loss: 3.32 | ade_train_g: 19.23 | ade_val_g: 20.00 | fde_train_g: 37.44 | fde_val_g: 37.53\n",
      "e: 90 | loss: 1.73 | val_loss: 3.73 | ade_train_g: 18.77 | ade_val_g: 19.33 | fde_train_g: 36.73 | fde_val_g: 34.95\n",
      "e: 91 | loss: 1.75 | val_loss: 3.80 | ade_train_g: 19.23 | ade_val_g: 20.14 | fde_train_g: 37.50 | fde_val_g: 37.03\n",
      "e: 92 | loss: 1.72 | val_loss: 3.78 | ade_train_g: 18.87 | ade_val_g: 19.78 | fde_train_g: 36.88 | fde_val_g: 36.14\n",
      "e: 93 | loss: 1.71 | val_loss: 3.79 | ade_train_g: 18.47 | ade_val_g: 19.95 | fde_train_g: 36.00 | fde_val_g: 36.81\n",
      "e: 94 | loss: 1.70 | val_loss: 3.76 | ade_train_g: 18.54 | ade_val_g: 19.83 | fde_train_g: 36.08 | fde_val_g: 35.97\n",
      "e: 95 | loss: 1.70 | val_loss: 3.24 | ade_train_g: 18.51 | ade_val_g: 19.06 | fde_train_g: 36.12 | fde_val_g: 35.18\n",
      "e: 96 | loss: 1.71 | val_loss: 3.77 | ade_train_g: 18.55 | ade_val_g: 19.92 | fde_train_g: 36.26 | fde_val_g: 36.30\n",
      "e: 97 | loss: 1.70 | val_loss: 3.72 | ade_train_g: 18.49 | ade_val_g: 19.09 | fde_train_g: 36.08 | fde_val_g: 34.88\n",
      "e: 98 | loss: 1.69 | val_loss: 3.24 | ade_train_g: 18.31 | ade_val_g: 18.97 | fde_train_g: 35.56 | fde_val_g: 35.22\n",
      "e: 99 | loss: 1.69 | val_loss: 3.28 | ade_train_g: 18.26 | ade_val_g: 19.09 | fde_train_g: 35.44 | fde_val_g: 35.98\n",
      "e: 100 | loss: 1.82 | val_loss: 3.27 | ade_train_g: 18.94 | ade_val_g: 19.06 | fde_train_g: 38.80 | fde_val_g: 35.57\n",
      "e: 101 | loss: 1.71 | val_loss: 3.22 | ade_train_g: 18.54 | ade_val_g: 18.56 | fde_train_g: 36.14 | fde_val_g: 34.48\n",
      "e: 102 | loss: 1.70 | val_loss: 3.24 | ade_train_g: 18.37 | ade_val_g: 18.90 | fde_train_g: 35.92 | fde_val_g: 35.10\n",
      "e: 103 | loss: 1.73 | val_loss: 3.32 | ade_train_g: 18.87 | ade_val_g: 19.71 | fde_train_g: 36.71 | fde_val_g: 37.33\n",
      "e: 104 | loss: 1.71 | val_loss: 3.73 | ade_train_g: 18.36 | ade_val_g: 18.74 | fde_train_g: 35.87 | fde_val_g: 34.20\n",
      "e: 105 | loss: 1.70 | val_loss: 3.68 | ade_train_g: 18.25 | ade_val_g: 18.36 | fde_train_g: 35.65 | fde_val_g: 33.10\n",
      "e: 106 | loss: 1.67 | val_loss: 3.27 | ade_train_g: 17.94 | ade_val_g: 19.26 | fde_train_g: 34.97 | fde_val_g: 36.05\n",
      "e: 107 | loss: 1.69 | val_loss: 3.17 | ade_train_g: 18.11 | ade_val_g: 17.69 | fde_train_g: 35.49 | fde_val_g: 32.63\n",
      "e: 108 | loss: 1.71 | val_loss: 3.78 | ade_train_g: 18.48 | ade_val_g: 19.51 | fde_train_g: 36.03 | fde_val_g: 35.58\n",
      "e: 109 | loss: 1.67 | val_loss: 3.22 | ade_train_g: 17.83 | ade_val_g: 18.50 | fde_train_g: 34.89 | fde_val_g: 34.17\n",
      "e: 110 | loss: 1.68 | val_loss: 3.14 | ade_train_g: 17.99 | ade_val_g: 17.52 | fde_train_g: 34.91 | fde_val_g: 31.88\n",
      "e: 111 | loss: 1.67 | val_loss: 3.25 | ade_train_g: 17.76 | ade_val_g: 18.72 | fde_train_g: 34.67 | fde_val_g: 35.14\n",
      "e: 112 | loss: 1.69 | val_loss: 3.27 | ade_train_g: 18.11 | ade_val_g: 19.08 | fde_train_g: 35.41 | fde_val_g: 35.50\n",
      "e: 113 | loss: 1.67 | val_loss: 3.21 | ade_train_g: 17.76 | ade_val_g: 18.02 | fde_train_g: 34.69 | fde_val_g: 33.41\n",
      "e: 114 | loss: 1.68 | val_loss: 3.68 | ade_train_g: 18.05 | ade_val_g: 18.20 | fde_train_g: 35.11 | fde_val_g: 32.89\n",
      "e: 115 | loss: 1.66 | val_loss: 3.21 | ade_train_g: 17.67 | ade_val_g: 18.28 | fde_train_g: 34.43 | fde_val_g: 33.90\n",
      "e: 116 | loss: 1.65 | val_loss: 3.63 | ade_train_g: 17.53 | ade_val_g: 17.19 | fde_train_g: 34.05 | fde_val_g: 30.99\n",
      "e: 117 | loss: 1.66 | val_loss: 3.62 | ade_train_g: 17.71 | ade_val_g: 16.91 | fde_train_g: 34.44 | fde_val_g: 30.50\n",
      "e: 118 | loss: 1.64 | val_loss: 3.22 | ade_train_g: 17.18 | ade_val_g: 17.78 | fde_train_g: 33.43 | fde_val_g: 33.23\n",
      "Epoch   120: reducing learning rate of group 0 to 6.2500e-03.\n",
      "e: 119 | loss: 1.62 | val_loss: 3.16 | ade_train_g: 16.97 | ade_val_g: 17.54 | fde_train_g: 33.08 | fde_val_g: 32.15\n",
      "e: 120 | loss: 1.66 | val_loss: 3.66 | ade_train_g: 17.40 | ade_val_g: 17.61 | fde_train_g: 33.90 | fde_val_g: 31.41\n",
      "e: 121 | loss: 1.66 | val_loss: 3.20 | ade_train_g: 17.46 | ade_val_g: 17.55 | fde_train_g: 34.04 | fde_val_g: 32.77\n",
      "e: 122 | loss: 1.61 | val_loss: 3.64 | ade_train_g: 16.71 | ade_val_g: 17.26 | fde_train_g: 32.66 | fde_val_g: 31.38\n",
      "e: 123 | loss: 1.62 | val_loss: 3.62 | ade_train_g: 16.91 | ade_val_g: 17.13 | fde_train_g: 33.06 | fde_val_g: 31.47\n",
      "e: 124 | loss: 1.61 | val_loss: 3.12 | ade_train_g: 16.52 | ade_val_g: 16.52 | fde_train_g: 32.11 | fde_val_g: 30.56\n",
      "e: 125 | loss: 1.67 | val_loss: 3.62 | ade_train_g: 16.24 | ade_val_g: 16.69 | fde_train_g: 33.45 | fde_val_g: 30.44\n",
      "e: 126 | loss: 1.59 | val_loss: 3.07 | ade_train_g: 16.21 | ade_val_g: 16.02 | fde_train_g: 31.60 | fde_val_g: 29.59\n",
      "e: 127 | loss: 1.59 | val_loss: 3.10 | ade_train_g: 16.49 | ade_val_g: 16.11 | fde_train_g: 32.08 | fde_val_g: 29.98\n",
      "e: 128 | loss: 1.60 | val_loss: 4.16 | ade_train_g: 16.41 | ade_val_g: 17.67 | fde_train_g: 32.10 | fde_val_g: 31.82\n",
      "e: 129 | loss: 1.61 | val_loss: 3.14 | ade_train_g: 16.62 | ade_val_g: 16.93 | fde_train_g: 32.43 | fde_val_g: 31.21\n",
      "e: 130 | loss: 1.63 | val_loss: 3.22 | ade_train_g: 16.81 | ade_val_g: 17.51 | fde_train_g: 32.82 | fde_val_g: 33.50\n",
      "e: 131 | loss: 1.64 | val_loss: 3.11 | ade_train_g: 17.12 | ade_val_g: 16.74 | fde_train_g: 33.44 | fde_val_g: 30.97\n",
      "e: 132 | loss: 1.63 | val_loss: 3.62 | ade_train_g: 17.03 | ade_val_g: 16.72 | fde_train_g: 33.28 | fde_val_g: 30.53\n",
      "e: 133 | loss: 1.61 | val_loss: 3.14 | ade_train_g: 16.60 | ade_val_g: 17.00 | fde_train_g: 32.52 | fde_val_g: 31.45\n",
      "e: 134 | loss: 1.61 | val_loss: 3.12 | ade_train_g: 16.48 | ade_val_g: 16.23 | fde_train_g: 32.24 | fde_val_g: 30.47\n",
      "Epoch   136: reducing learning rate of group 0 to 3.1250e-03.\n",
      "e: 135 | loss: 1.62 | val_loss: 3.64 | ade_train_g: 16.69 | ade_val_g: 17.14 | fde_train_g: 32.57 | fde_val_g: 31.23\n",
      "e: 136 | loss: 1.60 | val_loss: 3.59 | ade_train_g: 16.41 | ade_val_g: 16.58 | fde_train_g: 32.08 | fde_val_g: 30.27\n",
      "e: 137 | loss: 1.58 | val_loss: 4.10 | ade_train_g: 16.12 | ade_val_g: 16.64 | fde_train_g: 31.57 | fde_val_g: 29.55\n",
      "e: 138 | loss: 1.59 | val_loss: 3.63 | ade_train_g: 16.19 | ade_val_g: 17.08 | fde_train_g: 31.78 | fde_val_g: 31.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 139 | loss: 1.58 | val_loss: 3.59 | ade_train_g: 16.24 | ade_val_g: 16.49 | fde_train_g: 31.72 | fde_val_g: 29.95\n",
      "e: 140 | loss: 1.58 | val_loss: 3.14 | ade_train_g: 16.18 | ade_val_g: 16.63 | fde_train_g: 31.55 | fde_val_g: 31.02\n",
      "e: 141 | loss: 1.57 | val_loss: 3.22 | ade_train_g: 15.94 | ade_val_g: 17.79 | fde_train_g: 31.16 | fde_val_g: 33.76\n",
      "e: 142 | loss: 1.60 | val_loss: 3.11 | ade_train_g: 16.34 | ade_val_g: 16.44 | fde_train_g: 31.99 | fde_val_g: 30.63\n",
      "e: 143 | loss: 1.58 | val_loss: 3.17 | ade_train_g: 16.26 | ade_val_g: 17.15 | fde_train_g: 31.73 | fde_val_g: 32.26\n",
      "Epoch   145: reducing learning rate of group 0 to 1.5625e-03.\n",
      "e: 144 | loss: 1.67 | val_loss: 3.16 | ade_train_g: 16.23 | ade_val_g: 16.86 | fde_train_g: 33.54 | fde_val_g: 31.75\n",
      "e: 145 | loss: 1.57 | val_loss: 3.11 | ade_train_g: 15.96 | ade_val_g: 16.15 | fde_train_g: 31.23 | fde_val_g: 30.21\n",
      "e: 146 | loss: 1.56 | val_loss: 3.15 | ade_train_g: 15.87 | ade_val_g: 16.78 | fde_train_g: 30.90 | fde_val_g: 31.64\n",
      "e: 147 | loss: 1.56 | val_loss: 3.16 | ade_train_g: 15.96 | ade_val_g: 17.26 | fde_train_g: 31.17 | fde_val_g: 32.27\n",
      "e: 148 | loss: 1.57 | val_loss: 3.59 | ade_train_g: 15.93 | ade_val_g: 16.28 | fde_train_g: 31.14 | fde_val_g: 29.62\n",
      "e: 149 | loss: 1.56 | val_loss: 3.08 | ade_train_g: 15.87 | ade_val_g: 15.97 | fde_train_g: 30.96 | fde_val_g: 29.65\n",
      "e: 150 | loss: 1.56 | val_loss: 3.69 | ade_train_g: 15.95 | ade_val_g: 17.65 | fde_train_g: 31.16 | fde_val_g: 32.63\n",
      "e: 151 | loss: 1.56 | val_loss: 3.16 | ade_train_g: 15.74 | ade_val_g: 16.80 | fde_train_g: 30.84 | fde_val_g: 31.60\n",
      "e: 152 | loss: 1.57 | val_loss: 3.59 | ade_train_g: 16.01 | ade_val_g: 16.45 | fde_train_g: 31.42 | fde_val_g: 30.00\n",
      "Epoch   154: reducing learning rate of group 0 to 7.8125e-04.\n",
      "e: 153 | loss: 1.56 | val_loss: 3.09 | ade_train_g: 15.94 | ade_val_g: 16.00 | fde_train_g: 31.08 | fde_val_g: 29.82\n",
      "e: 154 | loss: 1.57 | val_loss: 3.09 | ade_train_g: 15.96 | ade_val_g: 16.09 | fde_train_g: 31.20 | fde_val_g: 30.10\n",
      "e: 155 | loss: 1.56 | val_loss: 3.15 | ade_train_g: 15.86 | ade_val_g: 16.66 | fde_train_g: 31.03 | fde_val_g: 31.63\n",
      "e: 156 | loss: 1.56 | val_loss: 4.17 | ade_train_g: 15.89 | ade_val_g: 17.81 | fde_train_g: 31.04 | fde_val_g: 32.32\n",
      "e: 157 | loss: 1.56 | val_loss: 3.10 | ade_train_g: 15.73 | ade_val_g: 16.41 | fde_train_g: 30.75 | fde_val_g: 30.51\n",
      "e: 158 | loss: 1.56 | val_loss: 3.09 | ade_train_g: 15.77 | ade_val_g: 16.15 | fde_train_g: 30.84 | fde_val_g: 30.07\n",
      "e: 159 | loss: 1.56 | val_loss: 4.19 | ade_train_g: 15.83 | ade_val_g: 17.84 | fde_train_g: 30.89 | fde_val_g: 32.40\n",
      "e: 160 | loss: 1.55 | val_loss: 3.13 | ade_train_g: 15.72 | ade_val_g: 16.62 | fde_train_g: 30.73 | fde_val_g: 31.07\n",
      "e: 161 | loss: 1.57 | val_loss: 3.19 | ade_train_g: 15.84 | ade_val_g: 17.27 | fde_train_g: 30.94 | fde_val_g: 32.81\n",
      "Epoch   163: reducing learning rate of group 0 to 3.9063e-04.\n",
      "e: 162 | loss: 1.65 | val_loss: 3.10 | ade_train_g: 15.84 | ade_val_g: 16.02 | fde_train_g: 32.78 | fde_val_g: 29.86\n",
      "e: 163 | loss: 1.58 | val_loss: 3.14 | ade_train_g: 16.04 | ade_val_g: 16.65 | fde_train_g: 31.23 | fde_val_g: 31.31\n",
      "e: 164 | loss: 1.56 | val_loss: 3.15 | ade_train_g: 15.77 | ade_val_g: 16.72 | fde_train_g: 30.88 | fde_val_g: 31.47\n",
      "e: 165 | loss: 1.56 | val_loss: 3.57 | ade_train_g: 15.81 | ade_val_g: 16.07 | fde_train_g: 30.76 | fde_val_g: 29.29\n",
      "e: 166 | loss: 1.57 | val_loss: 3.12 | ade_train_g: 15.91 | ade_val_g: 16.46 | fde_train_g: 31.15 | fde_val_g: 30.80\n",
      "e: 167 | loss: 1.65 | val_loss: 3.13 | ade_train_g: 15.87 | ade_val_g: 16.29 | fde_train_g: 32.88 | fde_val_g: 30.28\n",
      "e: 168 | loss: 1.56 | val_loss: 3.09 | ade_train_g: 15.77 | ade_val_g: 15.98 | fde_train_g: 30.89 | fde_val_g: 29.83\n",
      "e: 169 | loss: 1.56 | val_loss: 3.08 | ade_train_g: 15.88 | ade_val_g: 15.85 | fde_train_g: 31.00 | fde_val_g: 29.42\n",
      "e: 170 | loss: 1.56 | val_loss: 3.67 | ade_train_g: 15.74 | ade_val_g: 17.39 | fde_train_g: 30.75 | fde_val_g: 32.37\n",
      "Epoch   172: reducing learning rate of group 0 to 1.9531e-04.\n",
      "e: 171 | loss: 1.54 | val_loss: 3.58 | ade_train_g: 15.60 | ade_val_g: 16.21 | fde_train_g: 30.45 | fde_val_g: 29.59\n",
      "e: 172 | loss: 1.56 | val_loss: 3.08 | ade_train_g: 15.83 | ade_val_g: 15.86 | fde_train_g: 31.00 | fde_val_g: 29.64\n",
      "e: 173 | loss: 1.57 | val_loss: 3.55 | ade_train_g: 15.93 | ade_val_g: 15.83 | fde_train_g: 31.09 | fde_val_g: 28.79\n",
      "e: 174 | loss: 1.56 | val_loss: 3.06 | ade_train_g: 15.87 | ade_val_g: 15.57 | fde_train_g: 30.98 | fde_val_g: 28.91\n",
      "e: 175 | loss: 1.66 | val_loss: 3.11 | ade_train_g: 18.02 | ade_val_g: 16.21 | fde_train_g: 33.34 | fde_val_g: 30.36\n",
      "e: 176 | loss: 1.56 | val_loss: 3.09 | ade_train_g: 15.67 | ade_val_g: 15.91 | fde_train_g: 30.68 | fde_val_g: 29.61\n",
      "e: 177 | loss: 1.55 | val_loss: 3.59 | ade_train_g: 15.66 | ade_val_g: 16.36 | fde_train_g: 30.54 | fde_val_g: 29.95\n",
      "e: 178 | loss: 1.56 | val_loss: 3.13 | ade_train_g: 15.83 | ade_val_g: 16.41 | fde_train_g: 30.88 | fde_val_g: 31.05\n",
      "e: 179 | loss: 1.55 | val_loss: 4.14 | ade_train_g: 15.61 | ade_val_g: 17.22 | fde_train_g: 30.51 | fde_val_g: 31.31\n",
      "e: 180 | loss: 1.56 | val_loss: 3.09 | ade_train_g: 15.71 | ade_val_g: 16.00 | fde_train_g: 30.60 | fde_val_g: 29.76\n",
      "e: 181 | loss: 1.54 | val_loss: 3.65 | ade_train_g: 15.57 | ade_val_g: 17.19 | fde_train_g: 30.39 | fde_val_g: 31.61\n",
      "e: 182 | loss: 1.56 | val_loss: 3.19 | ade_train_g: 15.74 | ade_val_g: 17.10 | fde_train_g: 30.80 | fde_val_g: 32.64\n",
      "Epoch   184: reducing learning rate of group 0 to 9.7656e-05.\n",
      "e: 183 | loss: 1.55 | val_loss: 3.12 | ade_train_g: 15.68 | ade_val_g: 16.35 | fde_train_g: 30.63 | fde_val_g: 30.55\n",
      "e: 184 | loss: 1.54 | val_loss: 3.10 | ade_train_g: 15.57 | ade_val_g: 16.08 | fde_train_g: 30.34 | fde_val_g: 30.26\n",
      "e: 185 | loss: 1.56 | val_loss: 3.14 | ade_train_g: 15.84 | ade_val_g: 16.53 | fde_train_g: 31.00 | fde_val_g: 30.99\n",
      "e: 186 | loss: 1.56 | val_loss: 3.13 | ade_train_g: 15.80 | ade_val_g: 16.34 | fde_train_g: 30.85 | fde_val_g: 30.84\n",
      "e: 187 | loss: 1.56 | val_loss: 3.16 | ade_train_g: 15.69 | ade_val_g: 16.79 | fde_train_g: 30.68 | fde_val_g: 31.68\n",
      "e: 188 | loss: 1.54 | val_loss: 3.09 | ade_train_g: 15.58 | ade_val_g: 15.91 | fde_train_g: 30.52 | fde_val_g: 29.60\n",
      "e: 189 | loss: 1.55 | val_loss: 3.06 | ade_train_g: 15.65 | ade_val_g: 15.62 | fde_train_g: 30.49 | fde_val_g: 29.18\n",
      "e: 190 | loss: 1.56 | val_loss: 3.57 | ade_train_g: 15.80 | ade_val_g: 16.11 | fde_train_g: 30.89 | fde_val_g: 29.41\n",
      "e: 191 | loss: 1.54 | val_loss: 3.12 | ade_train_g: 15.60 | ade_val_g: 16.32 | fde_train_g: 30.45 | fde_val_g: 30.56\n",
      "Epoch   193: reducing learning rate of group 0 to 4.8828e-05.\n",
      "e: 192 | loss: 1.55 | val_loss: 3.57 | ade_train_g: 15.65 | ade_val_g: 16.18 | fde_train_g: 30.58 | fde_val_g: 29.38\n",
      "e: 193 | loss: 1.55 | val_loss: 3.17 | ade_train_g: 15.59 | ade_val_g: 16.93 | fde_train_g: 30.37 | fde_val_g: 32.17\n",
      "e: 194 | loss: 1.56 | val_loss: 3.14 | ade_train_g: 15.79 | ade_val_g: 16.71 | fde_train_g: 30.95 | fde_val_g: 31.66\n",
      "e: 195 | loss: 1.54 | val_loss: 3.14 | ade_train_g: 15.56 | ade_val_g: 16.55 | fde_train_g: 30.45 | fde_val_g: 31.54\n",
      "e: 196 | loss: 1.56 | val_loss: 3.13 | ade_train_g: 15.73 | ade_val_g: 16.52 | fde_train_g: 30.73 | fde_val_g: 31.27\n",
      "e: 197 | loss: 1.56 | val_loss: 3.59 | ade_train_g: 15.90 | ade_val_g: 16.37 | fde_train_g: 31.06 | fde_val_g: 29.79\n",
      "e: 198 | loss: 1.55 | val_loss: 3.13 | ade_train_g: 15.60 | ade_val_g: 16.55 | fde_train_g: 30.47 | fde_val_g: 31.14\n",
      "e: 199 | loss: 1.55 | val_loss: 3.56 | ade_train_g: 15.60 | ade_val_g: 16.09 | fde_train_g: 30.50 | fde_val_g: 29.23\n",
      "e: 200 | loss: 1.54 | val_loss: 3.14 | ade_train_g: 15.56 | ade_val_g: 16.49 | fde_train_g: 30.36 | fde_val_g: 31.22\n",
      "Epoch   202: reducing learning rate of group 0 to 2.4414e-05.\n",
      "e: 201 | loss: 1.54 | val_loss: 3.16 | ade_train_g: 15.58 | ade_val_g: 16.94 | fde_train_g: 30.42 | fde_val_g: 32.00\n",
      "e: 202 | loss: 1.55 | val_loss: 3.63 | ade_train_g: 15.63 | ade_val_g: 17.00 | fde_train_g: 30.52 | fde_val_g: 31.16\n",
      "e: 203 | loss: 1.55 | val_loss: 3.59 | ade_train_g: 15.64 | ade_val_g: 16.36 | fde_train_g: 30.60 | fde_val_g: 29.93\n",
      "e: 204 | loss: 1.55 | val_loss: 3.08 | ade_train_g: 15.65 | ade_val_g: 15.91 | fde_train_g: 30.64 | fde_val_g: 29.71\n",
      "e: 205 | loss: 1.56 | val_loss: 3.14 | ade_train_g: 15.76 | ade_val_g: 16.57 | fde_train_g: 30.78 | fde_val_g: 31.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 206 | loss: 1.55 | val_loss: 3.10 | ade_train_g: 15.73 | ade_val_g: 15.97 | fde_train_g: 30.72 | fde_val_g: 30.01\n",
      "e: 207 | loss: 1.56 | val_loss: 3.08 | ade_train_g: 15.75 | ade_val_g: 16.03 | fde_train_g: 30.86 | fde_val_g: 29.92\n",
      "e: 208 | loss: 1.55 | val_loss: 3.62 | ade_train_g: 15.62 | ade_val_g: 16.62 | fde_train_g: 30.45 | fde_val_g: 30.76\n",
      "e: 209 | loss: 1.56 | val_loss: 3.14 | ade_train_g: 15.76 | ade_val_g: 16.72 | fde_train_g: 30.83 | fde_val_g: 31.60\n",
      "Epoch   211: reducing learning rate of group 0 to 1.2207e-05.\n",
      "e: 210 | loss: 1.55 | val_loss: 3.09 | ade_train_g: 15.57 | ade_val_g: 15.97 | fde_train_g: 30.46 | fde_val_g: 29.81\n",
      "e: 211 | loss: 1.54 | val_loss: 3.14 | ade_train_g: 15.54 | ade_val_g: 16.40 | fde_train_g: 30.34 | fde_val_g: 31.00\n",
      "e: 212 | loss: 1.56 | val_loss: 3.07 | ade_train_g: 15.82 | ade_val_g: 15.72 | fde_train_g: 30.96 | fde_val_g: 29.37\n",
      "e: 213 | loss: 1.57 | val_loss: 3.56 | ade_train_g: 15.80 | ade_val_g: 16.09 | fde_train_g: 31.02 | fde_val_g: 29.30\n",
      "e: 214 | loss: 1.55 | val_loss: 3.58 | ade_train_g: 15.68 | ade_val_g: 16.22 | fde_train_g: 30.65 | fde_val_g: 29.78\n",
      "e: 215 | loss: 1.55 | val_loss: 3.59 | ade_train_g: 15.68 | ade_val_g: 16.23 | fde_train_g: 30.63 | fde_val_g: 29.95\n",
      "e: 216 | loss: 1.55 | val_loss: 3.09 | ade_train_g: 15.66 | ade_val_g: 15.99 | fde_train_g: 30.58 | fde_val_g: 29.93\n",
      "e: 217 | loss: 1.56 | val_loss: 3.08 | ade_train_g: 15.78 | ade_val_g: 15.94 | fde_train_g: 30.97 | fde_val_g: 29.87\n",
      "e: 218 | loss: 1.55 | val_loss: 3.10 | ade_train_g: 15.75 | ade_val_g: 16.17 | fde_train_g: 30.80 | fde_val_g: 30.28\n",
      "Epoch   220: reducing learning rate of group 0 to 6.1035e-06.\n",
      "e: 219 | loss: 1.55 | val_loss: 3.06 | ade_train_g: 15.62 | ade_val_g: 15.75 | fde_train_g: 30.56 | fde_val_g: 29.25\n",
      "e: 220 | loss: 1.56 | val_loss: 3.10 | ade_train_g: 15.80 | ade_val_g: 16.20 | fde_train_g: 31.02 | fde_val_g: 30.30\n",
      "e: 221 | loss: 1.56 | val_loss: 4.13 | ade_train_g: 15.70 | ade_val_g: 17.10 | fde_train_g: 30.66 | fde_val_g: 31.09\n",
      "e: 222 | loss: 1.64 | val_loss: 3.14 | ade_train_g: 15.78 | ade_val_g: 16.53 | fde_train_g: 32.74 | fde_val_g: 31.32\n",
      "e: 223 | loss: 1.64 | val_loss: 3.11 | ade_train_g: 15.74 | ade_val_g: 16.09 | fde_train_g: 32.55 | fde_val_g: 30.05\n",
      "e: 224 | loss: 1.55 | val_loss: 3.17 | ade_train_g: 15.59 | ade_val_g: 16.64 | fde_train_g: 30.44 | fde_val_g: 31.78\n",
      "e: 225 | loss: 1.64 | val_loss: 3.66 | ade_train_g: 15.69 | ade_val_g: 17.20 | fde_train_g: 32.49 | fde_val_g: 31.90\n",
      "e: 226 | loss: 1.55 | val_loss: 3.69 | ade_train_g: 15.61 | ade_val_g: 17.54 | fde_train_g: 30.57 | fde_val_g: 32.89\n",
      "e: 227 | loss: 1.54 | val_loss: 3.11 | ade_train_g: 15.57 | ade_val_g: 16.31 | fde_train_g: 30.38 | fde_val_g: 30.82\n",
      "Epoch   229: reducing learning rate of group 0 to 3.0518e-06.\n",
      "e: 228 | loss: 1.55 | val_loss: 3.64 | ade_train_g: 15.64 | ade_val_g: 16.91 | fde_train_g: 30.52 | fde_val_g: 31.40\n",
      "e: 229 | loss: 1.56 | val_loss: 3.10 | ade_train_g: 15.77 | ade_val_g: 16.24 | fde_train_g: 30.83 | fde_val_g: 30.30\n",
      "e: 230 | loss: 1.56 | val_loss: 3.14 | ade_train_g: 15.77 | ade_val_g: 16.59 | fde_train_g: 30.88 | fde_val_g: 31.23\n",
      "e: 231 | loss: 1.63 | val_loss: 3.13 | ade_train_g: 15.69 | ade_val_g: 16.45 | fde_train_g: 32.43 | fde_val_g: 31.14\n",
      "e: 232 | loss: 1.55 | val_loss: 3.16 | ade_train_g: 15.60 | ade_val_g: 16.83 | fde_train_g: 30.49 | fde_val_g: 32.01\n",
      "e: 233 | loss: 1.55 | val_loss: 3.07 | ade_train_g: 15.62 | ade_val_g: 15.71 | fde_train_g: 30.58 | fde_val_g: 29.31\n",
      "e: 234 | loss: 1.54 | val_loss: 4.19 | ade_train_g: 15.60 | ade_val_g: 17.79 | fde_train_g: 30.44 | fde_val_g: 32.61\n",
      "e: 235 | loss: 1.55 | val_loss: 3.18 | ade_train_g: 15.64 | ade_val_g: 16.99 | fde_train_g: 30.51 | fde_val_g: 32.15\n",
      "e: 236 | loss: 1.56 | val_loss: 3.14 | ade_train_g: 15.83 | ade_val_g: 16.32 | fde_train_g: 30.96 | fde_val_g: 30.72\n",
      "Epoch   238: reducing learning rate of group 0 to 1.5259e-06.\n",
      "e: 237 | loss: 1.54 | val_loss: 3.08 | ade_train_g: 15.57 | ade_val_g: 16.01 | fde_train_g: 30.51 | fde_val_g: 29.72\n",
      "e: 238 | loss: 1.57 | val_loss: 3.09 | ade_train_g: 15.82 | ade_val_g: 16.08 | fde_train_g: 31.05 | fde_val_g: 30.05\n",
      "e: 239 | loss: 1.55 | val_loss: 3.07 | ade_train_g: 15.64 | ade_val_g: 15.69 | fde_train_g: 30.59 | fde_val_g: 29.43\n",
      "e: 240 | loss: 1.56 | val_loss: 3.09 | ade_train_g: 15.81 | ade_val_g: 15.85 | fde_train_g: 30.92 | fde_val_g: 29.53\n",
      "e: 241 | loss: 1.56 | val_loss: 3.59 | ade_train_g: 15.76 | ade_val_g: 16.06 | fde_train_g: 30.93 | fde_val_g: 29.39\n",
      "e: 242 | loss: 1.56 | val_loss: 3.69 | ade_train_g: 15.78 | ade_val_g: 17.34 | fde_train_g: 30.84 | fde_val_g: 32.47\n",
      "e: 243 | loss: 1.55 | val_loss: 3.11 | ade_train_g: 15.66 | ade_val_g: 16.21 | fde_train_g: 30.61 | fde_val_g: 30.39\n",
      "e: 244 | loss: 1.55 | val_loss: 3.09 | ade_train_g: 15.63 | ade_val_g: 16.09 | fde_train_g: 30.56 | fde_val_g: 30.36\n",
      "e: 245 | loss: 1.56 | val_loss: 3.62 | ade_train_g: 15.74 | ade_val_g: 16.59 | fde_train_g: 30.84 | fde_val_g: 30.48\n",
      "Epoch   247: reducing learning rate of group 0 to 7.6294e-07.\n",
      "e: 246 | loss: 1.55 | val_loss: 3.65 | ade_train_g: 15.66 | ade_val_g: 17.00 | fde_train_g: 30.55 | fde_val_g: 31.47\n",
      "e: 247 | loss: 1.56 | val_loss: 3.59 | ade_train_g: 15.76 | ade_val_g: 16.26 | fde_train_g: 30.89 | fde_val_g: 29.61\n",
      "e: 248 | loss: 1.56 | val_loss: 3.64 | ade_train_g: 15.75 | ade_val_g: 16.75 | fde_train_g: 30.78 | fde_val_g: 30.94\n",
      "e: 249 | loss: 1.55 | val_loss: 3.12 | ade_train_g: 15.74 | ade_val_g: 16.21 | fde_train_g: 30.73 | fde_val_g: 30.60\n",
      "e: 250 | loss: 1.56 | val_loss: 3.57 | ade_train_g: 15.75 | ade_val_g: 16.08 | fde_train_g: 30.81 | fde_val_g: 29.22\n",
      "e: 251 | loss: 1.56 | val_loss: 3.63 | ade_train_g: 15.71 | ade_val_g: 16.87 | fde_train_g: 30.70 | fde_val_g: 31.29\n",
      "e: 252 | loss: 1.55 | val_loss: 3.08 | ade_train_g: 15.63 | ade_val_g: 15.89 | fde_train_g: 30.52 | fde_val_g: 29.58\n",
      "e: 253 | loss: 1.55 | val_loss: 3.59 | ade_train_g: 15.69 | ade_val_g: 16.33 | fde_train_g: 30.64 | fde_val_g: 29.71\n",
      "e: 254 | loss: 1.55 | val_loss: 4.08 | ade_train_g: 15.63 | ade_val_g: 16.39 | fde_train_g: 30.60 | fde_val_g: 29.33\n",
      "Epoch   256: reducing learning rate of group 0 to 3.8147e-07.\n",
      "e: 255 | loss: 1.56 | val_loss: 3.08 | ade_train_g: 15.81 | ade_val_g: 16.04 | fde_train_g: 31.09 | fde_val_g: 29.91\n",
      "e: 256 | loss: 1.55 | val_loss: 3.10 | ade_train_g: 15.70 | ade_val_g: 16.01 | fde_train_g: 30.67 | fde_val_g: 30.09\n",
      "e: 257 | loss: 1.56 | val_loss: 3.10 | ade_train_g: 15.73 | ade_val_g: 15.97 | fde_train_g: 30.80 | fde_val_g: 29.93\n",
      "e: 258 | loss: 1.56 | val_loss: 3.13 | ade_train_g: 15.90 | ade_val_g: 16.51 | fde_train_g: 31.14 | fde_val_g: 31.33\n",
      "e: 259 | loss: 1.55 | val_loss: 3.64 | ade_train_g: 15.65 | ade_val_g: 17.01 | fde_train_g: 30.59 | fde_val_g: 31.18\n",
      "e: 260 | loss: 1.56 | val_loss: 3.14 | ade_train_g: 15.77 | ade_val_g: 16.50 | fde_train_g: 30.80 | fde_val_g: 31.20\n",
      "e: 261 | loss: 1.54 | val_loss: 3.62 | ade_train_g: 15.53 | ade_val_g: 16.75 | fde_train_g: 30.33 | fde_val_g: 30.86\n",
      "e: 262 | loss: 1.57 | val_loss: 3.57 | ade_train_g: 15.86 | ade_val_g: 16.18 | fde_train_g: 31.10 | fde_val_g: 29.59\n",
      "e: 263 | loss: 1.56 | val_loss: 4.57 | ade_train_g: 15.71 | ade_val_g: 16.73 | fde_train_g: 30.79 | fde_val_g: 29.13\n",
      "Epoch   265: reducing learning rate of group 0 to 1.9073e-07.\n",
      "e: 264 | loss: 1.65 | val_loss: 3.57 | ade_train_g: 17.90 | ade_val_g: 16.06 | fde_train_g: 33.04 | fde_val_g: 29.05\n",
      "e: 265 | loss: 1.55 | val_loss: 3.17 | ade_train_g: 15.70 | ade_val_g: 16.96 | fde_train_g: 30.74 | fde_val_g: 32.13\n",
      "e: 266 | loss: 1.56 | val_loss: 3.07 | ade_train_g: 15.77 | ade_val_g: 15.82 | fde_train_g: 30.86 | fde_val_g: 29.49\n",
      "e: 267 | loss: 1.55 | val_loss: 3.63 | ade_train_g: 15.60 | ade_val_g: 16.71 | fde_train_g: 30.50 | fde_val_g: 30.96\n",
      "e: 268 | loss: 1.56 | val_loss: 3.61 | ade_train_g: 15.81 | ade_val_g: 16.80 | fde_train_g: 30.94 | fde_val_g: 30.63\n",
      "e: 269 | loss: 1.56 | val_loss: 3.17 | ade_train_g: 15.70 | ade_val_g: 17.15 | fde_train_g: 30.75 | fde_val_g: 32.55\n",
      "e: 270 | loss: 1.56 | val_loss: 3.59 | ade_train_g: 15.72 | ade_val_g: 16.26 | fde_train_g: 30.76 | fde_val_g: 29.85\n",
      "e: 271 | loss: 1.55 | val_loss: 3.13 | ade_train_g: 15.63 | ade_val_g: 16.68 | fde_train_g: 30.56 | fde_val_g: 31.39\n",
      "e: 272 | loss: 1.55 | val_loss: 3.14 | ade_train_g: 15.68 | ade_val_g: 16.47 | fde_train_g: 30.74 | fde_val_g: 31.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   274: reducing learning rate of group 0 to 9.5367e-08.\n",
      "e: 273 | loss: 1.54 | val_loss: 3.09 | ade_train_g: 15.55 | ade_val_g: 16.19 | fde_train_g: 30.40 | fde_val_g: 30.25\n",
      "e: 274 | loss: 1.55 | val_loss: 3.11 | ade_train_g: 15.65 | ade_val_g: 16.26 | fde_train_g: 30.63 | fde_val_g: 30.63\n",
      "e: 275 | loss: 1.55 | val_loss: 3.10 | ade_train_g: 15.68 | ade_val_g: 16.17 | fde_train_g: 30.70 | fde_val_g: 30.11\n",
      "e: 276 | loss: 1.57 | val_loss: 3.13 | ade_train_g: 15.91 | ade_val_g: 16.55 | fde_train_g: 31.12 | fde_val_g: 30.90\n",
      "e: 277 | loss: 1.56 | val_loss: 3.08 | ade_train_g: 15.78 | ade_val_g: 15.76 | fde_train_g: 30.88 | fde_val_g: 29.45\n",
      "e: 278 | loss: 1.55 | val_loss: 3.14 | ade_train_g: 15.67 | ade_val_g: 16.52 | fde_train_g: 30.65 | fde_val_g: 31.50\n",
      "e: 279 | loss: 1.55 | val_loss: 3.09 | ade_train_g: 15.57 | ade_val_g: 15.91 | fde_train_g: 30.45 | fde_val_g: 29.94\n",
      "e: 280 | loss: 1.58 | val_loss: 3.11 | ade_train_g: 15.98 | ade_val_g: 16.13 | fde_train_g: 31.40 | fde_val_g: 30.34\n",
      "e: 281 | loss: 1.57 | val_loss: 4.11 | ade_train_g: 15.94 | ade_val_g: 16.85 | fde_train_g: 31.22 | fde_val_g: 30.46\n",
      "Epoch   283: reducing learning rate of group 0 to 4.7684e-08.\n",
      "e: 282 | loss: 1.55 | val_loss: 3.10 | ade_train_g: 15.63 | ade_val_g: 16.05 | fde_train_g: 30.57 | fde_val_g: 29.94\n",
      "e: 283 | loss: 1.54 | val_loss: 3.08 | ade_train_g: 15.48 | ade_val_g: 15.88 | fde_train_g: 30.22 | fde_val_g: 29.73\n",
      "e: 284 | loss: 1.55 | val_loss: 3.08 | ade_train_g: 15.64 | ade_val_g: 15.58 | fde_train_g: 30.46 | fde_val_g: 28.96\n",
      "e: 285 | loss: 1.55 | val_loss: 3.13 | ade_train_g: 15.62 | ade_val_g: 16.52 | fde_train_g: 30.51 | fde_val_g: 31.03\n",
      "e: 286 | loss: 1.55 | val_loss: 3.09 | ade_train_g: 15.70 | ade_val_g: 16.06 | fde_train_g: 30.75 | fde_val_g: 29.99\n",
      "e: 287 | loss: 1.54 | val_loss: 3.12 | ade_train_g: 15.60 | ade_val_g: 16.35 | fde_train_g: 30.45 | fde_val_g: 30.68\n",
      "e: 288 | loss: 1.55 | val_loss: 3.65 | ade_train_g: 15.69 | ade_val_g: 16.84 | fde_train_g: 30.69 | fde_val_g: 31.19\n",
      "e: 289 | loss: 1.55 | val_loss: 3.10 | ade_train_g: 15.65 | ade_val_g: 16.05 | fde_train_g: 30.63 | fde_val_g: 30.12\n",
      "e: 290 | loss: 1.56 | val_loss: 3.08 | ade_train_g: 15.77 | ade_val_g: 15.79 | fde_train_g: 30.83 | fde_val_g: 29.31\n",
      "Epoch   292: reducing learning rate of group 0 to 2.3842e-08.\n",
      "e: 291 | loss: 1.56 | val_loss: 3.10 | ade_train_g: 15.82 | ade_val_g: 15.95 | fde_train_g: 30.92 | fde_val_g: 29.97\n",
      "e: 292 | loss: 1.56 | val_loss: 3.60 | ade_train_g: 15.79 | ade_val_g: 16.33 | fde_train_g: 30.89 | fde_val_g: 30.17\n",
      "e: 293 | loss: 1.64 | val_loss: 4.10 | ade_train_g: 17.73 | ade_val_g: 16.91 | fde_train_g: 32.56 | fde_val_g: 30.32\n",
      "e: 294 | loss: 1.55 | val_loss: 3.14 | ade_train_g: 15.65 | ade_val_g: 16.39 | fde_train_g: 30.58 | fde_val_g: 31.20\n",
      "e: 295 | loss: 1.56 | val_loss: 3.08 | ade_train_g: 15.79 | ade_val_g: 15.76 | fde_train_g: 30.85 | fde_val_g: 29.38\n",
      "e: 296 | loss: 1.56 | val_loss: 3.63 | ade_train_g: 15.75 | ade_val_g: 16.74 | fde_train_g: 30.72 | fde_val_g: 31.09\n",
      "e: 297 | loss: 1.55 | val_loss: 3.09 | ade_train_g: 15.67 | ade_val_g: 16.00 | fde_train_g: 30.57 | fde_val_g: 29.96\n",
      "e: 298 | loss: 1.56 | val_loss: 3.58 | ade_train_g: 15.85 | ade_val_g: 16.26 | fde_train_g: 30.90 | fde_val_g: 29.70\n",
      "e: 299 | loss: 1.56 | val_loss: 3.07 | ade_train_g: 15.80 | ade_val_g: 15.75 | fde_train_g: 30.94 | fde_val_g: 29.31\n",
      "====================================================================================================\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "print('='*100)\n",
    "print('Training ...')\n",
    "\n",
    "train_p_scores=[]\n",
    "val_p_scores=[]\n",
    "alpha=1#0.4\n",
    "l1e = nn.L1Loss()\n",
    "train_s_scores = []\n",
    "train_pose_scores=[]\n",
    "val_pose_scores=[]\n",
    "train_c_scores = []\n",
    "val_s_scores   = []\n",
    "val_c_scores   = []\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr= 0.1)\n",
    "# optimizer = optim.Adadelta(net.parameters(),lr= 0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=8, \n",
    "                                                 threshold = 1e-12, verbose=True)\n",
    "\n",
    "for epoch in range(300):\n",
    "    start = time.time()\n",
    "    \n",
    "    avg_epoch_train_p_loss   = 0\n",
    "    avg_epoch_val_p_loss     = 0 \n",
    "    ade_g  = 0\n",
    "    fde_g  = 0\n",
    "    ade_train_g  = 0\n",
    "    fde_train_g  = 0\n",
    "    counter = 0\n",
    "    net.train()\n",
    "    \n",
    "    for idx, (obs_pose_global, target_pose_global,obs_s_global,target_s_global) in enumerate(train_loader):\n",
    "        obs_pose_global = obs_pose_global.to(device='cuda')\n",
    "        target_pose_global = target_pose_global.to(device='cuda')\n",
    "        obs_s_global = obs_s_global.to(device='cuda')\n",
    "        target_s_global = target_s_global.to(device='cuda')\n",
    "        \n",
    "        counter += 1        \n",
    "        \n",
    "        net.zero_grad()\n",
    "#         print(obs_pose_global)\n",
    "        (preds,) = net(pose=obs_s_global)\n",
    "        preds_g=speed2pos(preds,obs_pose_global)\n",
    "        \n",
    "        ade_train_g += float(ADE_c(preds_g, target_pose_global))\n",
    "        fde_train_g += float(FDE_c(preds_g, target_pose_global))\n",
    "      \n",
    "        loss_g  = l1e(preds, target_s_global)\n",
    "        \n",
    "        loss=alpha*loss_g\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_epoch_train_p_loss += float(loss)\n",
    "\n",
    "    avg_epoch_train_p_loss /= counter\n",
    "    ade_train_g  /= counter\n",
    "    fde_train_g  /= counter   \n",
    "    \n",
    "\n",
    "    counter=0\n",
    "    net.eval()\n",
    "    for idx, (obs_pose_global, target_pose_global,obs_s_global,target_s_global) in enumerate(val_loader):\n",
    "        obs_pose_global = obs_pose_global.to(device='cuda')\n",
    "        target_pose_global = target_pose_global.to(device='cuda')\n",
    "        obs_s_global = obs_s_global.to(device='cuda')\n",
    "        target_s_global = target_s_global.to(device='cuda')\n",
    "    \n",
    "        counter += 1        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            (preds,) = net(pose=obs_s_global)\n",
    "            \n",
    "            preds_g=speed2pos(preds,obs_pose_global)\n",
    "        \n",
    "            ade_g += float(ADE_c(preds_g, target_pose_global))\n",
    "            fde_g += float(FDE_c(preds_g, target_pose_global))\n",
    "\n",
    "            loss_g  = l1e(preds, target_s_global)\n",
    "\n",
    "            val_loss=alpha*loss_g\n",
    "\n",
    "            avg_epoch_val_p_loss += float(val_loss)\n",
    "      \n",
    "    avg_epoch_val_p_loss /= counter\n",
    "    val_p_scores.append(avg_epoch_val_p_loss)\n",
    "    ade_g  /= counter\n",
    "    fde_g  /= counter    \n",
    "   \n",
    "    \n",
    "    \n",
    "    scheduler.step(avg_epoch_val_p_loss)\n",
    "    \n",
    "    print('e:', epoch,'| loss: %.2f'%avg_epoch_train_p_loss,'| val_loss: %.2f'% avg_epoch_val_p_loss, '| ade_train_g: %.2f'% ade_train_g, '| ade_val_g: %.2f'% ade_g, '| fde_train_g: %.2f'% fde_train_g,'| fde_val_g: %.2f'% fde_g)\n",
    "\n",
    "print('='*100) \n",
    "# print('Saving ...')\n",
    "# torch.save(net.state_dict(), args.model_path)\n",
    "print('Done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "junior-capability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (l1): Linear(in_features=225, out_features=300, bias=True)\n",
      "  (l2): Linear(in_features=300, out_features=500, bias=True)\n",
      "  (l3): Linear(in_features=500, out_features=32, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # encoder\n",
    "#         self.drop =  nn.Dropout(p=0.3)\n",
    "        self.l1= nn.Linear(in_features=15*15, out_features=300)\n",
    "        self.l2 = nn.Linear(in_features=300, out_features=500)\n",
    "        self.l3 = nn.Linear(in_features=500, out_features=16*2)\n",
    "#         self.hardtanh = nn.Hardtanh(min_val=-100,max_val=100)\n",
    "        self.relu = nn.ReLU() \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x = self.drop(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "net = MLP().double().to('cuda:0')\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "contemporary-determination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Training ...\n",
      "e: 0 | loss: 71722.16 | val_loss: 36601.91 | ade_train_g: 1179.58 | ade_val_g: 1286.54 | fde_train_g: 1298.53 | fde_val_g: 1240.90\n",
      "e: 1 | loss: 40069.82 | val_loss: 29401.06 | ade_train_g: 1325.71 | ade_val_g: 1313.54 | fde_train_g: 1228.29 | fde_val_g: 1621.92\n",
      "e: 2 | loss: 14716.96 | val_loss: 14183.49 | ade_train_g: 1346.72 | ade_val_g: 1370.28 | fde_train_g: 1499.46 | fde_val_g: 1668.50\n",
      "e: 3 | loss: 5866.26 | val_loss: 3914.52 | ade_train_g: 1198.80 | ade_val_g: 1422.24 | fde_train_g: 1303.69 | fde_val_g: 1548.92\n",
      "e: 4 | loss: 2772.23 | val_loss: 1935.97 | ade_train_g: 1357.47 | ade_val_g: 1188.96 | fde_train_g: 1526.01 | fde_val_g: 1286.61\n",
      "e: 5 | loss: 1008.57 | val_loss: 501.34 | ade_train_g: 1073.39 | ade_val_g: 734.16 | fde_train_g: 1134.06 | fde_val_g: 683.03\n",
      "e: 6 | loss: 313.25 | val_loss: 271.74 | ade_train_g: 764.79 | ade_val_g: 794.18 | fde_train_g: 907.36 | fde_val_g: 985.15\n",
      "e: 7 | loss: 133.70 | val_loss: 120.97 | ade_train_g: 469.92 | ade_val_g: 221.42 | fde_train_g: 621.54 | fde_val_g: 292.65\n",
      "e: 8 | loss: 64.74 | val_loss: 101.49 | ade_train_g: 164.72 | ade_val_g: 151.40 | fde_train_g: 250.28 | fde_val_g: 227.95\n",
      "e: 9 | loss: 44.48 | val_loss: 40.21 | ade_train_g: 109.32 | ade_val_g: 87.44 | fde_train_g: 206.38 | fde_val_g: 172.81\n",
      "e: 10 | loss: 26.14 | val_loss: 28.18 | ade_train_g: 67.26 | ade_val_g: 61.89 | fde_train_g: 131.65 | fde_val_g: 114.73\n",
      "e: 11 | loss: 17.68 | val_loss: 21.26 | ade_train_g: 59.08 | ade_val_g: 46.52 | fde_train_g: 92.58 | fde_val_g: 63.84\n",
      "e: 12 | loss: 19.33 | val_loss: 18.74 | ade_train_g: 45.35 | ade_val_g: 43.32 | fde_train_g: 67.47 | fde_val_g: 64.78\n",
      "e: 13 | loss: 10.63 | val_loss: 11.71 | ade_train_g: 31.50 | ade_val_g: 33.39 | fde_train_g: 54.28 | fde_val_g: 57.32\n",
      "e: 14 | loss: 11.59 | val_loss: 21.62 | ade_train_g: 27.25 | ade_val_g: 36.25 | fde_train_g: 43.51 | fde_val_g: 54.28\n",
      "e: 15 | loss: 10.99 | val_loss: 15.57 | ade_train_g: 28.10 | ade_val_g: 28.19 | fde_train_g: 47.43 | fde_val_g: 42.42\n",
      "e: 16 | loss: 7.15 | val_loss: 14.51 | ade_train_g: 28.08 | ade_val_g: 31.90 | fde_train_g: 47.14 | fde_val_g: 49.84\n",
      "e: 17 | loss: 9.76 | val_loss: 15.65 | ade_train_g: 26.62 | ade_val_g: 34.36 | fde_train_g: 48.23 | fde_val_g: 59.77\n",
      "e: 18 | loss: 7.64 | val_loss: 21.99 | ade_train_g: 24.67 | ade_val_g: 34.27 | fde_train_g: 49.00 | fde_val_g: 57.86\n",
      "e: 19 | loss: 7.37 | val_loss: 15.32 | ade_train_g: 24.48 | ade_val_g: 30.54 | fde_train_g: 48.52 | fde_val_g: 51.22\n",
      "e: 20 | loss: 6.11 | val_loss: 11.14 | ade_train_g: 22.67 | ade_val_g: 23.68 | fde_train_g: 45.85 | fde_val_g: 36.03\n",
      "e: 21 | loss: 5.18 | val_loss: 5.81 | ade_train_g: 20.42 | ade_val_g: 20.18 | fde_train_g: 38.92 | fde_val_g: 29.85\n",
      "e: 22 | loss: 3.92 | val_loss: 6.83 | ade_train_g: 16.95 | ade_val_g: 20.05 | fde_train_g: 33.89 | fde_val_g: 33.40\n",
      "e: 23 | loss: 3.26 | val_loss: 11.37 | ade_train_g: 17.82 | ade_val_g: 30.83 | fde_train_g: 33.64 | fde_val_g: 39.30\n",
      "e: 24 | loss: 5.66 | val_loss: 9.70 | ade_train_g: 22.33 | ade_val_g: 24.47 | fde_train_g: 42.57 | fde_val_g: 41.33\n",
      "e: 25 | loss: 6.18 | val_loss: 8.19 | ade_train_g: 20.72 | ade_val_g: 21.24 | fde_train_g: 40.13 | fde_val_g: 30.72\n",
      "e: 26 | loss: 4.92 | val_loss: 12.42 | ade_train_g: 17.82 | ade_val_g: 22.77 | fde_train_g: 32.43 | fde_val_g: 33.46\n",
      "e: 27 | loss: 5.36 | val_loss: 10.81 | ade_train_g: 21.82 | ade_val_g: 25.93 | fde_train_g: 42.23 | fde_val_g: 49.46\n",
      "e: 28 | loss: 4.03 | val_loss: 13.69 | ade_train_g: 20.30 | ade_val_g: 24.61 | fde_train_g: 42.33 | fde_val_g: 49.24\n",
      "e: 29 | loss: 6.85 | val_loss: 13.37 | ade_train_g: 22.08 | ade_val_g: 27.78 | fde_train_g: 44.91 | fde_val_g: 51.01\n",
      "e: 30 | loss: 4.91 | val_loss: 19.54 | ade_train_g: 23.07 | ade_val_g: 33.99 | fde_train_g: 44.71 | fde_val_g: 61.70\n",
      "e: 31 | loss: 8.61 | val_loss: 12.33 | ade_train_g: 27.43 | ade_val_g: 24.85 | fde_train_g: 56.93 | fde_val_g: 38.11\n",
      "Epoch    33: reducing learning rate of group 0 to 5.0000e-02.\n",
      "e: 32 | loss: 4.24 | val_loss: 8.32 | ade_train_g: 18.30 | ade_val_g: 20.32 | fde_train_g: 37.11 | fde_val_g: 33.56\n",
      "e: 33 | loss: 3.63 | val_loss: 6.12 | ade_train_g: 14.85 | ade_val_g: 18.29 | fde_train_g: 29.55 | fde_val_g: 32.31\n",
      "e: 34 | loss: 3.02 | val_loss: 4.48 | ade_train_g: 13.98 | ade_val_g: 16.16 | fde_train_g: 30.75 | fde_val_g: 27.53\n",
      "e: 35 | loss: 2.52 | val_loss: 5.22 | ade_train_g: 14.44 | ade_val_g: 18.30 | fde_train_g: 29.86 | fde_val_g: 31.45\n",
      "e: 36 | loss: 2.27 | val_loss: 5.41 | ade_train_g: 14.92 | ade_val_g: 17.56 | fde_train_g: 31.38 | fde_val_g: 28.19\n",
      "e: 37 | loss: 2.55 | val_loss: 5.74 | ade_train_g: 14.45 | ade_val_g: 20.68 | fde_train_g: 29.69 | fde_val_g: 33.96\n",
      "e: 38 | loss: 2.60 | val_loss: 5.34 | ade_train_g: 14.14 | ade_val_g: 17.85 | fde_train_g: 28.60 | fde_val_g: 29.25\n",
      "e: 39 | loss: 2.45 | val_loss: 5.78 | ade_train_g: 13.72 | ade_val_g: 20.04 | fde_train_g: 28.51 | fde_val_g: 33.00\n",
      "e: 40 | loss: 2.57 | val_loss: 5.72 | ade_train_g: 15.13 | ade_val_g: 18.33 | fde_train_g: 31.38 | fde_val_g: 29.79\n",
      "e: 41 | loss: 2.16 | val_loss: 4.73 | ade_train_g: 13.67 | ade_val_g: 19.59 | fde_train_g: 28.62 | fde_val_g: 33.97\n",
      "e: 42 | loss: 2.20 | val_loss: 5.51 | ade_train_g: 13.15 | ade_val_g: 18.10 | fde_train_g: 27.08 | fde_val_g: 30.55\n",
      "e: 43 | loss: 2.33 | val_loss: 5.23 | ade_train_g: 13.43 | ade_val_g: 18.79 | fde_train_g: 28.14 | fde_val_g: 30.18\n",
      "e: 44 | loss: 2.07 | val_loss: 4.79 | ade_train_g: 13.74 | ade_val_g: 20.22 | fde_train_g: 27.98 | fde_val_g: 33.62\n",
      "e: 45 | loss: 3.30 | val_loss: 4.97 | ade_train_g: 15.34 | ade_val_g: 17.99 | fde_train_g: 30.82 | fde_val_g: 30.95\n",
      "e: 46 | loss: 3.73 | val_loss: 5.24 | ade_train_g: 17.61 | ade_val_g: 19.30 | fde_train_g: 38.22 | fde_val_g: 36.18\n",
      "e: 47 | loss: 3.13 | val_loss: 6.65 | ade_train_g: 15.43 | ade_val_g: 21.78 | fde_train_g: 32.37 | fde_val_g: 36.43\n",
      "e: 48 | loss: 2.51 | val_loss: 5.57 | ade_train_g: 14.15 | ade_val_g: 19.63 | fde_train_g: 28.84 | fde_val_g: 32.55\n",
      "e: 49 | loss: 2.12 | val_loss: 6.87 | ade_train_g: 14.24 | ade_val_g: 24.86 | fde_train_g: 28.80 | fde_val_g: 36.95\n",
      "e: 50 | loss: 2.26 | val_loss: 5.04 | ade_train_g: 13.69 | ade_val_g: 16.02 | fde_train_g: 28.23 | fde_val_g: 26.30\n",
      "e: 51 | loss: 2.04 | val_loss: 5.83 | ade_train_g: 12.27 | ade_val_g: 20.77 | fde_train_g: 26.69 | fde_val_g: 34.12\n",
      "e: 52 | loss: 2.74 | val_loss: 5.73 | ade_train_g: 13.59 | ade_val_g: 20.72 | fde_train_g: 28.56 | fde_val_g: 34.54\n",
      "e: 53 | loss: 2.23 | val_loss: 5.86 | ade_train_g: 12.83 | ade_val_g: 19.43 | fde_train_g: 28.09 | fde_val_g: 32.62\n",
      "e: 54 | loss: 3.11 | val_loss: 5.95 | ade_train_g: 13.43 | ade_val_g: 17.12 | fde_train_g: 28.01 | fde_val_g: 29.25\n",
      "e: 55 | loss: 2.65 | val_loss: 6.71 | ade_train_g: 13.11 | ade_val_g: 18.73 | fde_train_g: 27.82 | fde_val_g: 32.50\n",
      "e: 56 | loss: 3.27 | val_loss: 5.02 | ade_train_g: 12.62 | ade_val_g: 16.94 | fde_train_g: 28.29 | fde_val_g: 31.28\n",
      "e: 57 | loss: 3.06 | val_loss: 5.27 | ade_train_g: 12.90 | ade_val_g: 16.94 | fde_train_g: 28.25 | fde_val_g: 28.57\n",
      "e: 58 | loss: 2.68 | val_loss: 5.38 | ade_train_g: 13.36 | ade_val_g: 18.98 | fde_train_g: 27.42 | fde_val_g: 33.24\n",
      "e: 59 | loss: 2.34 | val_loss: 6.03 | ade_train_g: 12.30 | ade_val_g: 16.61 | fde_train_g: 27.45 | fde_val_g: 28.35\n",
      "Epoch    61: reducing learning rate of group 0 to 2.5000e-02.\n",
      "e: 60 | loss: 2.37 | val_loss: 5.44 | ade_train_g: 12.17 | ade_val_g: 18.36 | fde_train_g: 27.35 | fde_val_g: 32.31\n",
      "e: 61 | loss: 2.68 | val_loss: 3.91 | ade_train_g: 13.49 | ade_val_g: 15.08 | fde_train_g: 30.25 | fde_val_g: 25.72\n",
      "e: 62 | loss: 1.64 | val_loss: 4.18 | ade_train_g: 10.91 | ade_val_g: 16.32 | fde_train_g: 25.81 | fde_val_g: 29.07\n",
      "e: 63 | loss: 1.98 | val_loss: 4.12 | ade_train_g: 12.52 | ade_val_g: 17.22 | fde_train_g: 26.79 | fde_val_g: 31.34\n",
      "e: 64 | loss: 1.80 | val_loss: 4.80 | ade_train_g: 10.99 | ade_val_g: 16.38 | fde_train_g: 25.03 | fde_val_g: 27.58\n",
      "e: 65 | loss: 1.85 | val_loss: 3.98 | ade_train_g: 11.67 | ade_val_g: 15.66 | fde_train_g: 29.70 | fde_val_g: 30.86\n",
      "e: 66 | loss: 1.97 | val_loss: 4.00 | ade_train_g: 11.62 | ade_val_g: 17.49 | fde_train_g: 26.73 | fde_val_g: 31.37\n",
      "e: 67 | loss: 1.78 | val_loss: 4.05 | ade_train_g: 11.78 | ade_val_g: 15.65 | fde_train_g: 25.42 | fde_val_g: 26.87\n",
      "e: 68 | loss: 2.00 | val_loss: 4.69 | ade_train_g: 11.55 | ade_val_g: 16.67 | fde_train_g: 24.55 | fde_val_g: 28.88\n",
      "e: 69 | loss: 1.98 | val_loss: 3.76 | ade_train_g: 11.66 | ade_val_g: 16.18 | fde_train_g: 25.46 | fde_val_g: 28.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 70 | loss: 1.63 | val_loss: 3.97 | ade_train_g: 10.78 | ade_val_g: 17.23 | fde_train_g: 24.26 | fde_val_g: 32.05\n",
      "e: 71 | loss: 1.90 | val_loss: 4.32 | ade_train_g: 11.88 | ade_val_g: 19.45 | fde_train_g: 26.02 | fde_val_g: 31.80\n",
      "e: 72 | loss: 1.72 | val_loss: 3.93 | ade_train_g: 11.58 | ade_val_g: 16.45 | fde_train_g: 26.36 | fde_val_g: 28.65\n",
      "e: 73 | loss: 1.73 | val_loss: 3.90 | ade_train_g: 13.14 | ade_val_g: 17.92 | fde_train_g: 26.78 | fde_val_g: 31.51\n",
      "e: 74 | loss: 1.79 | val_loss: 4.15 | ade_train_g: 11.62 | ade_val_g: 16.60 | fde_train_g: 27.46 | fde_val_g: 29.23\n",
      "e: 75 | loss: 1.87 | val_loss: 4.76 | ade_train_g: 12.08 | ade_val_g: 16.42 | fde_train_g: 26.17 | fde_val_g: 29.41\n",
      "e: 76 | loss: 2.00 | val_loss: 3.94 | ade_train_g: 11.56 | ade_val_g: 16.41 | fde_train_g: 26.24 | fde_val_g: 28.67\n",
      "e: 77 | loss: 1.87 | val_loss: 6.03 | ade_train_g: 11.57 | ade_val_g: 18.65 | fde_train_g: 26.34 | fde_val_g: 31.18\n",
      "e: 78 | loss: 2.86 | val_loss: 4.94 | ade_train_g: 12.59 | ade_val_g: 17.27 | fde_train_g: 27.26 | fde_val_g: 30.80\n",
      "Epoch    80: reducing learning rate of group 0 to 1.2500e-02.\n",
      "e: 79 | loss: 1.83 | val_loss: 4.62 | ade_train_g: 11.66 | ade_val_g: 16.30 | fde_train_g: 25.91 | fde_val_g: 28.14\n",
      "e: 80 | loss: 1.78 | val_loss: 3.90 | ade_train_g: 12.01 | ade_val_g: 15.98 | fde_train_g: 26.28 | fde_val_g: 28.33\n",
      "e: 81 | loss: 1.46 | val_loss: 3.56 | ade_train_g: 11.12 | ade_val_g: 14.69 | fde_train_g: 24.87 | fde_val_g: 29.51\n",
      "e: 82 | loss: 1.72 | val_loss: 3.60 | ade_train_g: 11.82 | ade_val_g: 15.77 | fde_train_g: 25.63 | fde_val_g: 27.79\n",
      "e: 83 | loss: 1.50 | val_loss: 4.77 | ade_train_g: 11.48 | ade_val_g: 17.53 | fde_train_g: 25.08 | fde_val_g: 30.43\n",
      "e: 84 | loss: 1.62 | val_loss: 3.45 | ade_train_g: 10.87 | ade_val_g: 15.58 | fde_train_g: 27.02 | fde_val_g: 27.89\n",
      "e: 85 | loss: 1.50 | val_loss: 4.12 | ade_train_g: 10.68 | ade_val_g: 16.96 | fde_train_g: 23.59 | fde_val_g: 28.14\n",
      "e: 86 | loss: 1.49 | val_loss: 3.79 | ade_train_g: 10.68 | ade_val_g: 15.80 | fde_train_g: 24.42 | fde_val_g: 28.25\n",
      "e: 87 | loss: 1.61 | val_loss: 3.50 | ade_train_g: 11.45 | ade_val_g: 16.00 | fde_train_g: 25.10 | fde_val_g: 29.14\n",
      "e: 88 | loss: 1.47 | val_loss: 3.55 | ade_train_g: 10.74 | ade_val_g: 15.94 | fde_train_g: 25.01 | fde_val_g: 28.05\n",
      "e: 89 | loss: 1.44 | val_loss: 3.61 | ade_train_g: 10.88 | ade_val_g: 17.21 | fde_train_g: 24.48 | fde_val_g: 28.88\n",
      "e: 90 | loss: 1.48 | val_loss: 4.79 | ade_train_g: 10.86 | ade_val_g: 19.18 | fde_train_g: 24.70 | fde_val_g: 31.98\n",
      "e: 91 | loss: 1.59 | val_loss: 3.62 | ade_train_g: 11.22 | ade_val_g: 17.33 | fde_train_g: 24.98 | fde_val_g: 30.90\n",
      "e: 92 | loss: 1.54 | val_loss: 3.64 | ade_train_g: 10.55 | ade_val_g: 17.19 | fde_train_g: 23.62 | fde_val_g: 29.86\n",
      "e: 93 | loss: 1.40 | val_loss: 5.12 | ade_train_g: 10.83 | ade_val_g: 21.29 | fde_train_g: 24.71 | fde_val_g: 38.78\n",
      "e: 94 | loss: 1.57 | val_loss: 4.52 | ade_train_g: 11.11 | ade_val_g: 16.82 | fde_train_g: 25.42 | fde_val_g: 28.24\n",
      "e: 95 | loss: 1.55 | val_loss: 3.54 | ade_train_g: 11.08 | ade_val_g: 17.16 | fde_train_g: 24.52 | fde_val_g: 29.47\n",
      "e: 96 | loss: 1.39 | val_loss: 3.67 | ade_train_g: 10.92 | ade_val_g: 14.78 | fde_train_g: 24.66 | fde_val_g: 27.97\n",
      "e: 97 | loss: 1.49 | val_loss: 3.74 | ade_train_g: 11.37 | ade_val_g: 15.90 | fde_train_g: 24.28 | fde_val_g: 27.29\n",
      "e: 98 | loss: 1.65 | val_loss: 4.51 | ade_train_g: 11.58 | ade_val_g: 16.15 | fde_train_g: 25.82 | fde_val_g: 27.45\n",
      "e: 99 | loss: 1.56 | val_loss: 4.32 | ade_train_g: 11.88 | ade_val_g: 22.19 | fde_train_g: 26.10 | fde_val_g: 38.94\n",
      "e: 100 | loss: 1.66 | val_loss: 5.12 | ade_train_g: 11.10 | ade_val_g: 19.64 | fde_train_g: 25.48 | fde_val_g: 30.84\n",
      "e: 101 | loss: 1.63 | val_loss: 3.97 | ade_train_g: 11.77 | ade_val_g: 18.06 | fde_train_g: 26.27 | fde_val_g: 30.89\n",
      "e: 102 | loss: 1.65 | val_loss: 3.66 | ade_train_g: 11.01 | ade_val_g: 16.08 | fde_train_g: 24.54 | fde_val_g: 28.64\n",
      "e: 103 | loss: 1.52 | val_loss: 3.55 | ade_train_g: 11.61 | ade_val_g: 15.82 | fde_train_g: 25.48 | fde_val_g: 29.23\n",
      "e: 104 | loss: 1.48 | val_loss: 3.78 | ade_train_g: 11.06 | ade_val_g: 15.73 | fde_train_g: 25.19 | fde_val_g: 28.09\n",
      "Epoch   106: reducing learning rate of group 0 to 6.2500e-03.\n",
      "e: 105 | loss: 1.91 | val_loss: 4.17 | ade_train_g: 13.82 | ade_val_g: 17.38 | fde_train_g: 27.54 | fde_val_g: 29.24\n",
      "e: 106 | loss: 1.62 | val_loss: 3.93 | ade_train_g: 10.95 | ade_val_g: 18.05 | fde_train_g: 24.54 | fde_val_g: 30.36\n",
      "e: 107 | loss: 1.40 | val_loss: 4.59 | ade_train_g: 10.52 | ade_val_g: 16.28 | fde_train_g: 23.54 | fde_val_g: 27.38\n",
      "e: 108 | loss: 1.53 | val_loss: 3.68 | ade_train_g: 11.57 | ade_val_g: 15.63 | fde_train_g: 25.04 | fde_val_g: 27.70\n",
      "e: 109 | loss: 1.30 | val_loss: 3.49 | ade_train_g: 10.70 | ade_val_g: 16.63 | fde_train_g: 24.20 | fde_val_g: 30.08\n",
      "e: 110 | loss: 1.32 | val_loss: 4.21 | ade_train_g: 10.54 | ade_val_g: 19.98 | fde_train_g: 23.74 | fde_val_g: 32.16\n",
      "e: 111 | loss: 1.42 | val_loss: 3.52 | ade_train_g: 10.31 | ade_val_g: 16.25 | fde_train_g: 23.50 | fde_val_g: 28.53\n",
      "e: 112 | loss: 1.41 | val_loss: 3.57 | ade_train_g: 10.29 | ade_val_g: 15.79 | fde_train_g: 23.28 | fde_val_g: 28.27\n",
      "e: 113 | loss: 1.37 | val_loss: 4.00 | ade_train_g: 10.42 | ade_val_g: 16.57 | fde_train_g: 23.73 | fde_val_g: 29.24\n",
      "e: 114 | loss: 1.31 | val_loss: 4.18 | ade_train_g: 10.39 | ade_val_g: 17.69 | fde_train_g: 23.89 | fde_val_g: 31.40\n",
      "e: 115 | loss: 1.40 | val_loss: 4.17 | ade_train_g: 10.44 | ade_val_g: 20.65 | fde_train_g: 23.89 | fde_val_g: 34.48\n",
      "e: 116 | loss: 1.29 | val_loss: 4.18 | ade_train_g: 10.33 | ade_val_g: 15.98 | fde_train_g: 23.34 | fde_val_g: 27.24\n",
      "e: 117 | loss: 1.40 | val_loss: 3.88 | ade_train_g: 10.45 | ade_val_g: 15.96 | fde_train_g: 23.72 | fde_val_g: 27.51\n",
      "e: 118 | loss: 1.36 | val_loss: 4.25 | ade_train_g: 10.28 | ade_val_g: 17.05 | fde_train_g: 23.92 | fde_val_g: 30.02\n",
      "e: 119 | loss: 1.43 | val_loss: 4.84 | ade_train_g: 10.72 | ade_val_g: 21.79 | fde_train_g: 24.34 | fde_val_g: 35.96\n",
      "e: 120 | loss: 1.41 | val_loss: 3.96 | ade_train_g: 10.73 | ade_val_g: 16.31 | fde_train_g: 24.52 | fde_val_g: 28.75\n",
      "e: 121 | loss: 1.35 | val_loss: 3.56 | ade_train_g: 10.53 | ade_val_g: 16.44 | fde_train_g: 24.39 | fde_val_g: 28.96\n",
      "e: 122 | loss: 1.51 | val_loss: 4.53 | ade_train_g: 10.91 | ade_val_g: 16.92 | fde_train_g: 25.89 | fde_val_g: 27.85\n",
      "e: 123 | loss: 1.56 | val_loss: 3.99 | ade_train_g: 11.39 | ade_val_g: 16.98 | fde_train_g: 24.62 | fde_val_g: 31.42\n",
      "e: 124 | loss: 1.49 | val_loss: 4.56 | ade_train_g: 11.15 | ade_val_g: 16.52 | fde_train_g: 24.63 | fde_val_g: 28.64\n",
      "Epoch   126: reducing learning rate of group 0 to 3.1250e-03.\n",
      "e: 125 | loss: 1.49 | val_loss: 4.04 | ade_train_g: 10.64 | ade_val_g: 16.07 | fde_train_g: 24.54 | fde_val_g: 27.41\n",
      "e: 126 | loss: 1.30 | val_loss: 3.53 | ade_train_g: 10.39 | ade_val_g: 16.17 | fde_train_g: 23.69 | fde_val_g: 28.58\n",
      "e: 127 | loss: 1.27 | val_loss: 3.55 | ade_train_g: 10.02 | ade_val_g: 16.52 | fde_train_g: 22.72 | fde_val_g: 28.73\n",
      "e: 128 | loss: 1.29 | val_loss: 3.47 | ade_train_g: 10.08 | ade_val_g: 16.08 | fde_train_g: 23.45 | fde_val_g: 28.43\n",
      "e: 129 | loss: 1.27 | val_loss: 3.46 | ade_train_g: 10.26 | ade_val_g: 15.70 | fde_train_g: 23.29 | fde_val_g: 27.55\n",
      "e: 130 | loss: 1.25 | val_loss: 3.62 | ade_train_g: 9.87 | ade_val_g: 15.71 | fde_train_g: 22.78 | fde_val_g: 27.33\n",
      "e: 131 | loss: 1.27 | val_loss: 3.49 | ade_train_g: 10.19 | ade_val_g: 16.37 | fde_train_g: 23.60 | fde_val_g: 28.72\n",
      "e: 132 | loss: 1.27 | val_loss: 3.49 | ade_train_g: 10.12 | ade_val_g: 15.93 | fde_train_g: 23.21 | fde_val_g: 27.93\n",
      "e: 133 | loss: 1.25 | val_loss: 3.63 | ade_train_g: 10.09 | ade_val_g: 16.23 | fde_train_g: 22.89 | fde_val_g: 29.27\n",
      "e: 134 | loss: 1.33 | val_loss: 4.78 | ade_train_g: 10.06 | ade_val_g: 20.62 | fde_train_g: 22.84 | fde_val_g: 34.33\n",
      "e: 135 | loss: 1.38 | val_loss: 3.58 | ade_train_g: 10.14 | ade_val_g: 16.51 | fde_train_g: 25.43 | fde_val_g: 28.39\n",
      "e: 136 | loss: 1.30 | val_loss: 4.34 | ade_train_g: 10.29 | ade_val_g: 20.41 | fde_train_g: 23.19 | fde_val_g: 33.92\n",
      "e: 137 | loss: 1.25 | val_loss: 4.05 | ade_train_g: 10.06 | ade_val_g: 16.24 | fde_train_g: 22.88 | fde_val_g: 28.30\n",
      "e: 138 | loss: 1.31 | val_loss: 3.65 | ade_train_g: 10.16 | ade_val_g: 16.44 | fde_train_g: 22.87 | fde_val_g: 29.60\n",
      "Epoch   140: reducing learning rate of group 0 to 1.5625e-03.\n",
      "e: 139 | loss: 1.30 | val_loss: 3.56 | ade_train_g: 10.18 | ade_val_g: 16.62 | fde_train_g: 23.88 | fde_val_g: 28.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 140 | loss: 1.27 | val_loss: 3.47 | ade_train_g: 10.39 | ade_val_g: 15.94 | fde_train_g: 23.54 | fde_val_g: 27.62\n",
      "e: 141 | loss: 1.22 | val_loss: 3.52 | ade_train_g: 9.98 | ade_val_g: 15.98 | fde_train_g: 22.60 | fde_val_g: 28.60\n",
      "e: 142 | loss: 1.20 | val_loss: 4.01 | ade_train_g: 9.76 | ade_val_g: 15.65 | fde_train_g: 22.34 | fde_val_g: 26.95\n",
      "e: 143 | loss: 1.33 | val_loss: 4.09 | ade_train_g: 10.54 | ade_val_g: 16.59 | fde_train_g: 23.25 | fde_val_g: 29.06\n",
      "e: 144 | loss: 1.28 | val_loss: 4.07 | ade_train_g: 9.95 | ade_val_g: 16.50 | fde_train_g: 23.12 | fde_val_g: 28.26\n",
      "e: 145 | loss: 1.25 | val_loss: 3.54 | ade_train_g: 9.80 | ade_val_g: 16.25 | fde_train_g: 22.64 | fde_val_g: 29.13\n",
      "e: 146 | loss: 1.23 | val_loss: 4.10 | ade_train_g: 9.90 | ade_val_g: 16.96 | fde_train_g: 22.70 | fde_val_g: 29.66\n",
      "e: 147 | loss: 1.25 | val_loss: 3.49 | ade_train_g: 10.07 | ade_val_g: 15.81 | fde_train_g: 23.22 | fde_val_g: 27.71\n",
      "e: 148 | loss: 1.22 | val_loss: 4.04 | ade_train_g: 9.86 | ade_val_g: 16.48 | fde_train_g: 22.53 | fde_val_g: 28.41\n",
      "e: 149 | loss: 1.32 | val_loss: 3.54 | ade_train_g: 10.08 | ade_val_g: 15.94 | fde_train_g: 23.24 | fde_val_g: 28.32\n",
      "e: 150 | loss: 1.26 | val_loss: 3.56 | ade_train_g: 10.04 | ade_val_g: 16.06 | fde_train_g: 22.75 | fde_val_g: 28.25\n",
      "Epoch   152: reducing learning rate of group 0 to 7.8125e-04.\n",
      "e: 151 | loss: 1.26 | val_loss: 4.06 | ade_train_g: 10.01 | ade_val_g: 16.75 | fde_train_g: 22.80 | fde_val_g: 28.75\n",
      "e: 152 | loss: 1.23 | val_loss: 3.51 | ade_train_g: 10.02 | ade_val_g: 16.16 | fde_train_g: 23.33 | fde_val_g: 28.24\n",
      "e: 153 | loss: 1.20 | val_loss: 3.52 | ade_train_g: 9.85 | ade_val_g: 16.19 | fde_train_g: 22.77 | fde_val_g: 28.51\n",
      "e: 154 | loss: 1.20 | val_loss: 3.48 | ade_train_g: 9.83 | ade_val_g: 16.04 | fde_train_g: 22.52 | fde_val_g: 28.30\n",
      "e: 155 | loss: 1.29 | val_loss: 3.51 | ade_train_g: 10.03 | ade_val_g: 16.15 | fde_train_g: 24.73 | fde_val_g: 28.72\n",
      "e: 156 | loss: 1.20 | val_loss: 3.51 | ade_train_g: 9.80 | ade_val_g: 15.95 | fde_train_g: 22.46 | fde_val_g: 28.02\n",
      "e: 157 | loss: 1.20 | val_loss: 4.03 | ade_train_g: 9.88 | ade_val_g: 16.42 | fde_train_g: 22.77 | fde_val_g: 28.17\n",
      "e: 158 | loss: 1.20 | val_loss: 4.02 | ade_train_g: 9.86 | ade_val_g: 16.37 | fde_train_g: 22.71 | fde_val_g: 28.29\n",
      "e: 159 | loss: 1.19 | val_loss: 4.52 | ade_train_g: 9.75 | ade_val_g: 16.70 | fde_train_g: 22.57 | fde_val_g: 28.16\n",
      "e: 160 | loss: 1.19 | val_loss: 4.50 | ade_train_g: 9.74 | ade_val_g: 16.99 | fde_train_g: 22.58 | fde_val_g: 28.62\n",
      "e: 161 | loss: 1.21 | val_loss: 3.56 | ade_train_g: 9.78 | ade_val_g: 16.47 | fde_train_g: 22.75 | fde_val_g: 29.71\n",
      "e: 162 | loss: 1.20 | val_loss: 3.50 | ade_train_g: 9.74 | ade_val_g: 15.97 | fde_train_g: 22.41 | fde_val_g: 28.08\n",
      "e: 163 | loss: 1.20 | val_loss: 4.51 | ade_train_g: 9.80 | ade_val_g: 16.57 | fde_train_g: 22.67 | fde_val_g: 27.99\n",
      "e: 164 | loss: 1.19 | val_loss: 3.53 | ade_train_g: 9.64 | ade_val_g: 15.93 | fde_train_g: 22.19 | fde_val_g: 28.14\n",
      "e: 165 | loss: 1.21 | val_loss: 3.51 | ade_train_g: 9.78 | ade_val_g: 15.92 | fde_train_g: 22.72 | fde_val_g: 27.87\n",
      "e: 166 | loss: 1.19 | val_loss: 4.00 | ade_train_g: 9.68 | ade_val_g: 16.48 | fde_train_g: 22.42 | fde_val_g: 28.37\n",
      "e: 167 | loss: 1.31 | val_loss: 4.03 | ade_train_g: 9.91 | ade_val_g: 16.50 | fde_train_g: 24.74 | fde_val_g: 28.75\n",
      "e: 168 | loss: 1.20 | val_loss: 4.06 | ade_train_g: 9.82 | ade_val_g: 16.65 | fde_train_g: 22.56 | fde_val_g: 29.01\n",
      "e: 169 | loss: 1.21 | val_loss: 4.51 | ade_train_g: 9.80 | ade_val_g: 16.56 | fde_train_g: 22.80 | fde_val_g: 27.94\n",
      "e: 170 | loss: 1.20 | val_loss: 4.01 | ade_train_g: 9.80 | ade_val_g: 16.13 | fde_train_g: 22.63 | fde_val_g: 27.60\n",
      "e: 171 | loss: 1.23 | val_loss: 3.54 | ade_train_g: 9.95 | ade_val_g: 16.40 | fde_train_g: 22.82 | fde_val_g: 29.12\n",
      "e: 172 | loss: 1.20 | val_loss: 4.52 | ade_train_g: 9.80 | ade_val_g: 16.80 | fde_train_g: 22.57 | fde_val_g: 28.23\n",
      "e: 173 | loss: 1.21 | val_loss: 4.04 | ade_train_g: 9.80 | ade_val_g: 16.32 | fde_train_g: 22.70 | fde_val_g: 28.10\n",
      "e: 174 | loss: 1.20 | val_loss: 4.03 | ade_train_g: 9.79 | ade_val_g: 16.27 | fde_train_g: 22.75 | fde_val_g: 28.25\n",
      "Epoch   176: reducing learning rate of group 0 to 3.9063e-04.\n",
      "e: 175 | loss: 1.21 | val_loss: 3.55 | ade_train_g: 9.75 | ade_val_g: 16.27 | fde_train_g: 22.36 | fde_val_g: 28.93\n",
      "e: 176 | loss: 1.20 | val_loss: 4.05 | ade_train_g: 9.73 | ade_val_g: 16.72 | fde_train_g: 22.28 | fde_val_g: 29.24\n",
      "e: 177 | loss: 1.19 | val_loss: 3.53 | ade_train_g: 9.70 | ade_val_g: 16.07 | fde_train_g: 22.30 | fde_val_g: 28.46\n",
      "e: 178 | loss: 1.19 | val_loss: 3.51 | ade_train_g: 9.80 | ade_val_g: 15.96 | fde_train_g: 22.57 | fde_val_g: 28.15\n",
      "e: 179 | loss: 1.19 | val_loss: 3.50 | ade_train_g: 9.73 | ade_val_g: 16.01 | fde_train_g: 22.45 | fde_val_g: 28.10\n",
      "e: 180 | loss: 1.20 | val_loss: 3.55 | ade_train_g: 9.85 | ade_val_g: 16.51 | fde_train_g: 22.71 | fde_val_g: 29.17\n",
      "e: 181 | loss: 1.18 | val_loss: 4.12 | ade_train_g: 9.62 | ade_val_g: 19.73 | fde_train_g: 22.20 | fde_val_g: 32.71\n",
      "e: 182 | loss: 1.19 | val_loss: 3.49 | ade_train_g: 9.78 | ade_val_g: 15.76 | fde_train_g: 22.72 | fde_val_g: 27.61\n",
      "e: 183 | loss: 1.18 | val_loss: 4.59 | ade_train_g: 9.70 | ade_val_g: 17.12 | fde_train_g: 22.38 | fde_val_g: 29.51\n",
      "e: 184 | loss: 1.22 | val_loss: 3.58 | ade_train_g: 9.89 | ade_val_g: 16.50 | fde_train_g: 22.82 | fde_val_g: 29.37\n",
      "e: 185 | loss: 1.19 | val_loss: 3.56 | ade_train_g: 9.65 | ade_val_g: 16.39 | fde_train_g: 22.27 | fde_val_g: 29.39\n",
      "e: 186 | loss: 1.28 | val_loss: 4.18 | ade_train_g: 9.89 | ade_val_g: 19.75 | fde_train_g: 24.65 | fde_val_g: 32.65\n",
      "e: 187 | loss: 1.19 | val_loss: 3.52 | ade_train_g: 9.75 | ade_val_g: 16.14 | fde_train_g: 22.54 | fde_val_g: 28.45\n",
      "e: 188 | loss: 1.20 | val_loss: 3.51 | ade_train_g: 9.83 | ade_val_g: 15.94 | fde_train_g: 22.68 | fde_val_g: 27.89\n",
      "e: 189 | loss: 1.19 | val_loss: 4.18 | ade_train_g: 9.77 | ade_val_g: 20.17 | fde_train_g: 22.50 | fde_val_g: 33.59\n",
      "e: 190 | loss: 1.18 | val_loss: 3.55 | ade_train_g: 9.67 | ade_val_g: 15.93 | fde_train_g: 22.24 | fde_val_g: 28.16\n",
      "e: 191 | loss: 1.20 | val_loss: 4.05 | ade_train_g: 9.88 | ade_val_g: 16.52 | fde_train_g: 22.85 | fde_val_g: 28.40\n",
      "e: 192 | loss: 1.19 | val_loss: 4.53 | ade_train_g: 9.77 | ade_val_g: 16.93 | fde_train_g: 22.49 | fde_val_g: 28.65\n",
      "e: 193 | loss: 1.19 | val_loss: 3.51 | ade_train_g: 9.81 | ade_val_g: 15.89 | fde_train_g: 22.51 | fde_val_g: 28.05\n",
      "e: 194 | loss: 1.27 | val_loss: 3.56 | ade_train_g: 9.88 | ade_val_g: 16.46 | fde_train_g: 24.54 | fde_val_g: 29.14\n",
      "e: 195 | loss: 1.19 | val_loss: 4.52 | ade_train_g: 9.73 | ade_val_g: 16.72 | fde_train_g: 22.30 | fde_val_g: 28.09\n",
      "e: 196 | loss: 1.20 | val_loss: 3.53 | ade_train_g: 9.84 | ade_val_g: 16.00 | fde_train_g: 22.70 | fde_val_g: 28.04\n",
      "e: 197 | loss: 1.22 | val_loss: 3.55 | ade_train_g: 9.83 | ade_val_g: 16.62 | fde_train_g: 22.83 | fde_val_g: 29.53\n",
      "e: 198 | loss: 1.20 | val_loss: 3.55 | ade_train_g: 9.78 | ade_val_g: 16.45 | fde_train_g: 22.70 | fde_val_g: 29.27\n",
      "Epoch   200: reducing learning rate of group 0 to 1.9531e-04.\n",
      "e: 199 | loss: 1.19 | val_loss: 4.66 | ade_train_g: 9.78 | ade_val_g: 20.14 | fde_train_g: 22.61 | fde_val_g: 32.67\n",
      "e: 200 | loss: 1.27 | val_loss: 4.65 | ade_train_g: 9.84 | ade_val_g: 20.06 | fde_train_g: 24.49 | fde_val_g: 32.71\n",
      "e: 201 | loss: 1.18 | val_loss: 4.03 | ade_train_g: 9.75 | ade_val_g: 16.70 | fde_train_g: 22.42 | fde_val_g: 28.97\n",
      "e: 202 | loss: 1.18 | val_loss: 3.52 | ade_train_g: 9.79 | ade_val_g: 16.16 | fde_train_g: 22.56 | fde_val_g: 28.44\n",
      "e: 203 | loss: 1.18 | val_loss: 4.03 | ade_train_g: 9.71 | ade_val_g: 16.46 | fde_train_g: 22.43 | fde_val_g: 28.47\n",
      "e: 204 | loss: 1.18 | val_loss: 4.64 | ade_train_g: 9.71 | ade_val_g: 20.44 | fde_train_g: 22.47 | fde_val_g: 33.49\n",
      "e: 205 | loss: 1.18 | val_loss: 3.53 | ade_train_g: 9.66 | ade_val_g: 16.17 | fde_train_g: 22.32 | fde_val_g: 28.72\n",
      "e: 206 | loss: 1.20 | val_loss: 4.16 | ade_train_g: 9.88 | ade_val_g: 20.00 | fde_train_g: 22.74 | fde_val_g: 33.52\n",
      "e: 207 | loss: 1.27 | val_loss: 4.04 | ade_train_g: 9.84 | ade_val_g: 16.78 | fde_train_g: 24.51 | fde_val_g: 29.06\n",
      "e: 208 | loss: 1.20 | val_loss: 3.56 | ade_train_g: 9.91 | ade_val_g: 16.32 | fde_train_g: 22.72 | fde_val_g: 28.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 209 | loss: 1.18 | val_loss: 4.04 | ade_train_g: 9.76 | ade_val_g: 16.57 | fde_train_g: 22.54 | fde_val_g: 28.63\n",
      "e: 210 | loss: 1.18 | val_loss: 5.14 | ade_train_g: 9.72 | ade_val_g: 20.65 | fde_train_g: 22.40 | fde_val_g: 33.20\n",
      "e: 211 | loss: 1.18 | val_loss: 4.53 | ade_train_g: 9.62 | ade_val_g: 16.88 | fde_train_g: 22.16 | fde_val_g: 28.93\n",
      "e: 212 | loss: 1.19 | val_loss: 4.56 | ade_train_g: 9.78 | ade_val_g: 16.74 | fde_train_g: 22.57 | fde_val_g: 28.54\n",
      "e: 213 | loss: 1.18 | val_loss: 4.06 | ade_train_g: 9.71 | ade_val_g: 16.83 | fde_train_g: 22.42 | fde_val_g: 29.15\n",
      "Epoch   215: reducing learning rate of group 0 to 9.7656e-05.\n",
      "e: 214 | loss: 1.20 | val_loss: 4.01 | ade_train_g: 9.96 | ade_val_g: 16.33 | fde_train_g: 22.93 | fde_val_g: 28.17\n",
      "e: 215 | loss: 1.17 | val_loss: 4.19 | ade_train_g: 9.71 | ade_val_g: 20.44 | fde_train_g: 22.28 | fde_val_g: 34.58\n",
      "e: 216 | loss: 1.17 | val_loss: 4.65 | ade_train_g: 9.61 | ade_val_g: 20.15 | fde_train_g: 22.21 | fde_val_g: 32.64\n",
      "e: 217 | loss: 1.18 | val_loss: 3.53 | ade_train_g: 9.74 | ade_val_g: 16.28 | fde_train_g: 22.56 | fde_val_g: 28.67\n",
      "e: 218 | loss: 1.17 | val_loss: 3.53 | ade_train_g: 9.60 | ade_val_g: 16.12 | fde_train_g: 22.14 | fde_val_g: 28.50\n",
      "e: 219 | loss: 1.17 | val_loss: 3.58 | ade_train_g: 9.71 | ade_val_g: 16.68 | fde_train_g: 22.42 | fde_val_g: 29.83\n",
      "e: 220 | loss: 1.18 | val_loss: 3.56 | ade_train_g: 9.75 | ade_val_g: 16.38 | fde_train_g: 22.54 | fde_val_g: 29.05\n",
      "e: 221 | loss: 1.17 | val_loss: 4.15 | ade_train_g: 9.63 | ade_val_g: 19.89 | fde_train_g: 22.16 | fde_val_g: 32.97\n",
      "e: 222 | loss: 1.19 | val_loss: 4.01 | ade_train_g: 9.86 | ade_val_g: 16.37 | fde_train_g: 22.70 | fde_val_g: 28.31\n",
      "e: 223 | loss: 1.17 | val_loss: 3.50 | ade_train_g: 9.65 | ade_val_g: 15.83 | fde_train_g: 22.24 | fde_val_g: 27.91\n",
      "e: 224 | loss: 1.18 | val_loss: 4.03 | ade_train_g: 9.78 | ade_val_g: 16.53 | fde_train_g: 22.51 | fde_val_g: 28.54\n",
      "e: 225 | loss: 1.17 | val_loss: 4.00 | ade_train_g: 9.68 | ade_val_g: 16.18 | fde_train_g: 22.27 | fde_val_g: 27.73\n",
      "e: 226 | loss: 1.18 | val_loss: 4.02 | ade_train_g: 9.77 | ade_val_g: 16.40 | fde_train_g: 22.61 | fde_val_g: 28.38\n",
      "e: 227 | loss: 1.17 | val_loss: 4.05 | ade_train_g: 9.69 | ade_val_g: 16.81 | fde_train_g: 22.43 | fde_val_g: 29.29\n",
      "e: 228 | loss: 1.17 | val_loss: 3.51 | ade_train_g: 9.61 | ade_val_g: 15.95 | fde_train_g: 22.19 | fde_val_g: 28.10\n",
      "e: 229 | loss: 1.27 | val_loss: 4.16 | ade_train_g: 9.80 | ade_val_g: 20.01 | fde_train_g: 24.47 | fde_val_g: 33.29\n",
      "e: 230 | loss: 1.18 | val_loss: 3.55 | ade_train_g: 9.80 | ade_val_g: 16.40 | fde_train_g: 22.49 | fde_val_g: 29.26\n",
      "e: 231 | loss: 1.27 | val_loss: 3.53 | ade_train_g: 9.88 | ade_val_g: 16.16 | fde_train_g: 24.52 | fde_val_g: 28.73\n",
      "e: 232 | loss: 1.18 | val_loss: 4.65 | ade_train_g: 9.73 | ade_val_g: 20.34 | fde_train_g: 22.40 | fde_val_g: 33.08\n",
      "e: 233 | loss: 1.18 | val_loss: 3.55 | ade_train_g: 9.69 | ade_val_g: 16.33 | fde_train_g: 22.34 | fde_val_g: 28.95\n",
      "e: 234 | loss: 1.17 | val_loss: 4.03 | ade_train_g: 9.66 | ade_val_g: 16.25 | fde_train_g: 22.29 | fde_val_g: 27.81\n",
      "e: 235 | loss: 1.19 | val_loss: 3.52 | ade_train_g: 9.84 | ade_val_g: 16.01 | fde_train_g: 22.65 | fde_val_g: 28.29\n",
      "e: 236 | loss: 1.17 | val_loss: 3.56 | ade_train_g: 9.68 | ade_val_g: 16.27 | fde_train_g: 22.29 | fde_val_g: 28.75\n",
      "Epoch   238: reducing learning rate of group 0 to 4.8828e-05.\n",
      "e: 237 | loss: 1.19 | val_loss: 3.53 | ade_train_g: 9.87 | ade_val_g: 16.18 | fde_train_g: 22.81 | fde_val_g: 28.56\n",
      "e: 238 | loss: 1.26 | val_loss: 4.54 | ade_train_g: 9.74 | ade_val_g: 16.87 | fde_train_g: 24.28 | fde_val_g: 28.78\n",
      "e: 239 | loss: 1.17 | val_loss: 4.06 | ade_train_g: 9.63 | ade_val_g: 16.76 | fde_train_g: 22.21 | fde_val_g: 29.30\n",
      "e: 240 | loss: 1.17 | val_loss: 4.04 | ade_train_g: 9.68 | ade_val_g: 16.49 | fde_train_g: 22.35 | fde_val_g: 28.59\n",
      "e: 241 | loss: 1.17 | val_loss: 4.02 | ade_train_g: 9.68 | ade_val_g: 16.33 | fde_train_g: 22.32 | fde_val_g: 28.31\n",
      "e: 242 | loss: 1.16 | val_loss: 3.54 | ade_train_g: 9.58 | ade_val_g: 16.13 | fde_train_g: 22.14 | fde_val_g: 28.66\n",
      "e: 243 | loss: 1.17 | val_loss: 3.51 | ade_train_g: 9.62 | ade_val_g: 15.85 | fde_train_g: 22.26 | fde_val_g: 27.76\n",
      "e: 244 | loss: 1.27 | val_loss: 4.03 | ade_train_g: 9.85 | ade_val_g: 16.47 | fde_train_g: 24.61 | fde_val_g: 28.68\n",
      "e: 245 | loss: 1.17 | val_loss: 4.02 | ade_train_g: 9.74 | ade_val_g: 16.30 | fde_train_g: 22.56 | fde_val_g: 27.97\n",
      "e: 246 | loss: 1.18 | val_loss: 4.70 | ade_train_g: 9.76 | ade_val_g: 20.80 | fde_train_g: 22.59 | fde_val_g: 34.44\n",
      "e: 247 | loss: 1.18 | val_loss: 3.51 | ade_train_g: 9.77 | ade_val_g: 15.94 | fde_train_g: 22.59 | fde_val_g: 28.06\n",
      "e: 248 | loss: 1.17 | val_loss: 3.51 | ade_train_g: 9.63 | ade_val_g: 16.05 | fde_train_g: 22.13 | fde_val_g: 28.28\n",
      "e: 249 | loss: 1.25 | val_loss: 3.54 | ade_train_g: 9.76 | ade_val_g: 16.17 | fde_train_g: 24.26 | fde_val_g: 28.72\n",
      "e: 250 | loss: 1.18 | val_loss: 4.15 | ade_train_g: 9.75 | ade_val_g: 19.90 | fde_train_g: 22.41 | fde_val_g: 33.21\n",
      "Epoch   252: reducing learning rate of group 0 to 2.4414e-05.\n",
      "e: 251 | loss: 1.26 | val_loss: 4.14 | ade_train_g: 9.79 | ade_val_g: 19.75 | fde_train_g: 24.33 | fde_val_g: 32.78\n",
      "e: 252 | loss: 1.17 | val_loss: 3.56 | ade_train_g: 9.69 | ade_val_g: 16.54 | fde_train_g: 22.38 | fde_val_g: 29.61\n",
      "e: 253 | loss: 1.17 | val_loss: 3.56 | ade_train_g: 9.71 | ade_val_g: 16.41 | fde_train_g: 22.42 | fde_val_g: 29.39\n",
      "e: 254 | loss: 1.17 | val_loss: 4.04 | ade_train_g: 9.67 | ade_val_g: 16.61 | fde_train_g: 22.30 | fde_val_g: 28.80\n",
      "e: 255 | loss: 1.16 | val_loss: 3.53 | ade_train_g: 9.61 | ade_val_g: 16.29 | fde_train_g: 22.24 | fde_val_g: 28.82\n",
      "e: 256 | loss: 1.17 | val_loss: 4.50 | ade_train_g: 9.70 | ade_val_g: 16.55 | fde_train_g: 22.35 | fde_val_g: 27.91\n",
      "e: 257 | loss: 1.18 | val_loss: 4.56 | ade_train_g: 9.77 | ade_val_g: 17.08 | fde_train_g: 22.61 | fde_val_g: 29.06\n",
      "e: 258 | loss: 1.17 | val_loss: 3.53 | ade_train_g: 9.61 | ade_val_g: 16.10 | fde_train_g: 22.19 | fde_val_g: 28.57\n",
      "e: 259 | loss: 1.18 | val_loss: 3.51 | ade_train_g: 9.75 | ade_val_g: 15.97 | fde_train_g: 22.48 | fde_val_g: 28.28\n",
      "Epoch   261: reducing learning rate of group 0 to 1.2207e-05.\n",
      "e: 260 | loss: 1.17 | val_loss: 4.05 | ade_train_g: 9.71 | ade_val_g: 16.67 | fde_train_g: 22.35 | fde_val_g: 29.23\n",
      "e: 261 | loss: 1.17 | val_loss: 4.51 | ade_train_g: 9.61 | ade_val_g: 16.59 | fde_train_g: 22.25 | fde_val_g: 27.77\n",
      "e: 262 | loss: 1.17 | val_loss: 3.56 | ade_train_g: 9.62 | ade_val_g: 16.52 | fde_train_g: 22.20 | fde_val_g: 29.51\n",
      "e: 263 | loss: 1.16 | val_loss: 4.05 | ade_train_g: 9.59 | ade_val_g: 16.68 | fde_train_g: 22.12 | fde_val_g: 29.04\n",
      "e: 264 | loss: 1.17 | val_loss: 4.02 | ade_train_g: 9.66 | ade_val_g: 16.45 | fde_train_g: 22.27 | fde_val_g: 28.40\n",
      "e: 265 | loss: 1.16 | val_loss: 3.50 | ade_train_g: 9.63 | ade_val_g: 15.89 | fde_train_g: 22.18 | fde_val_g: 27.88\n",
      "e: 266 | loss: 1.16 | val_loss: 4.66 | ade_train_g: 9.66 | ade_val_g: 20.35 | fde_train_g: 22.21 | fde_val_g: 33.48\n",
      "e: 267 | loss: 1.16 | val_loss: 4.06 | ade_train_g: 9.61 | ade_val_g: 16.76 | fde_train_g: 22.16 | fde_val_g: 29.34\n",
      "e: 268 | loss: 1.17 | val_loss: 4.04 | ade_train_g: 9.76 | ade_val_g: 16.66 | fde_train_g: 22.48 | fde_val_g: 29.00\n",
      "Epoch   270: reducing learning rate of group 0 to 6.1035e-06.\n",
      "e: 269 | loss: 1.16 | val_loss: 3.53 | ade_train_g: 9.61 | ade_val_g: 16.12 | fde_train_g: 22.16 | fde_val_g: 28.61\n",
      "e: 270 | loss: 1.16 | val_loss: 4.52 | ade_train_g: 9.62 | ade_val_g: 16.82 | fde_train_g: 22.17 | fde_val_g: 28.54\n",
      "e: 271 | loss: 1.16 | val_loss: 3.52 | ade_train_g: 9.57 | ade_val_g: 16.18 | fde_train_g: 22.11 | fde_val_g: 28.61\n",
      "e: 272 | loss: 1.17 | val_loss: 4.51 | ade_train_g: 9.72 | ade_val_g: 16.60 | fde_train_g: 22.37 | fde_val_g: 27.81\n",
      "e: 273 | loss: 1.27 | val_loss: 4.53 | ade_train_g: 9.91 | ade_val_g: 16.95 | fde_train_g: 24.69 | fde_val_g: 28.72\n",
      "e: 274 | loss: 1.17 | val_loss: 3.53 | ade_train_g: 9.62 | ade_val_g: 15.99 | fde_train_g: 22.21 | fde_val_g: 28.28\n",
      "e: 275 | loss: 1.16 | val_loss: 4.57 | ade_train_g: 9.63 | ade_val_g: 17.20 | fde_train_g: 22.22 | fde_val_g: 29.31\n",
      "e: 276 | loss: 1.17 | val_loss: 3.55 | ade_train_g: 9.64 | ade_val_g: 16.36 | fde_train_g: 22.25 | fde_val_g: 29.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 277 | loss: 1.17 | val_loss: 3.52 | ade_train_g: 9.66 | ade_val_g: 16.08 | fde_train_g: 22.35 | fde_val_g: 28.43\n",
      "e: 278 | loss: 1.17 | val_loss: 4.16 | ade_train_g: 9.72 | ade_val_g: 19.90 | fde_train_g: 22.43 | fde_val_g: 33.25\n",
      "e: 279 | loss: 1.17 | val_loss: 3.51 | ade_train_g: 9.62 | ade_val_g: 15.85 | fde_train_g: 22.26 | fde_val_g: 28.00\n",
      "Epoch   281: reducing learning rate of group 0 to 3.0518e-06.\n",
      "e: 280 | loss: 1.18 | val_loss: 4.01 | ade_train_g: 9.74 | ade_val_g: 16.21 | fde_train_g: 22.50 | fde_val_g: 27.99\n",
      "e: 281 | loss: 1.18 | val_loss: 3.52 | ade_train_g: 9.77 | ade_val_g: 16.15 | fde_train_g: 22.62 | fde_val_g: 28.36\n",
      "e: 282 | loss: 1.17 | val_loss: 4.05 | ade_train_g: 9.68 | ade_val_g: 16.62 | fde_train_g: 22.40 | fde_val_g: 28.93\n",
      "e: 283 | loss: 1.18 | val_loss: 4.52 | ade_train_g: 9.74 | ade_val_g: 16.75 | fde_train_g: 22.43 | fde_val_g: 28.19\n",
      "e: 284 | loss: 1.16 | val_loss: 3.54 | ade_train_g: 9.59 | ade_val_g: 16.33 | fde_train_g: 22.15 | fde_val_g: 29.00\n",
      "e: 285 | loss: 1.18 | val_loss: 3.50 | ade_train_g: 9.72 | ade_val_g: 15.83 | fde_train_g: 22.45 | fde_val_g: 27.86\n",
      "e: 286 | loss: 1.18 | val_loss: 4.01 | ade_train_g: 9.77 | ade_val_g: 16.30 | fde_train_g: 22.54 | fde_val_g: 28.18\n",
      "e: 287 | loss: 1.17 | val_loss: 3.53 | ade_train_g: 9.66 | ade_val_g: 16.24 | fde_train_g: 22.30 | fde_val_g: 28.84\n",
      "e: 288 | loss: 1.16 | val_loss: 4.15 | ade_train_g: 9.58 | ade_val_g: 20.01 | fde_train_g: 22.03 | fde_val_g: 33.17\n",
      "Epoch   290: reducing learning rate of group 0 to 1.5259e-06.\n",
      "e: 289 | loss: 1.16 | val_loss: 3.53 | ade_train_g: 9.57 | ade_val_g: 16.04 | fde_train_g: 22.17 | fde_val_g: 28.43\n",
      "e: 290 | loss: 1.17 | val_loss: 3.51 | ade_train_g: 9.67 | ade_val_g: 16.01 | fde_train_g: 22.35 | fde_val_g: 28.35\n",
      "e: 291 | loss: 1.17 | val_loss: 3.52 | ade_train_g: 9.72 | ade_val_g: 15.92 | fde_train_g: 22.42 | fde_val_g: 28.15\n",
      "e: 292 | loss: 1.26 | val_loss: 3.55 | ade_train_g: 9.85 | ade_val_g: 16.45 | fde_train_g: 24.54 | fde_val_g: 29.23\n",
      "e: 293 | loss: 1.17 | val_loss: 4.65 | ade_train_g: 9.70 | ade_val_g: 20.21 | fde_train_g: 22.35 | fde_val_g: 33.06\n",
      "e: 294 | loss: 1.18 | val_loss: 3.52 | ade_train_g: 9.84 | ade_val_g: 16.05 | fde_train_g: 22.73 | fde_val_g: 28.13\n",
      "e: 295 | loss: 1.17 | val_loss: 4.03 | ade_train_g: 9.74 | ade_val_g: 16.48 | fde_train_g: 22.49 | fde_val_g: 28.62\n",
      "e: 296 | loss: 1.17 | val_loss: 3.53 | ade_train_g: 9.71 | ade_val_g: 16.19 | fde_train_g: 22.41 | fde_val_g: 28.48\n",
      "e: 297 | loss: 1.16 | val_loss: 3.53 | ade_train_g: 9.62 | ade_val_g: 16.02 | fde_train_g: 22.19 | fde_val_g: 28.63\n",
      "Epoch   299: reducing learning rate of group 0 to 7.6294e-07.\n",
      "e: 298 | loss: 1.16 | val_loss: 4.02 | ade_train_g: 9.64 | ade_val_g: 16.54 | fde_train_g: 22.25 | fde_val_g: 28.48\n",
      "e: 299 | loss: 1.16 | val_loss: 3.56 | ade_train_g: 9.59 | ade_val_g: 16.41 | fde_train_g: 22.09 | fde_val_g: 29.21\n",
      "====================================================================================================\n",
      "Done !\n"
     ]
    }
   ],
   "source": [
    "print('='*100)\n",
    "print('Training ...')\n",
    "\n",
    "l1e = nn.L1Loss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr= 0.1)\n",
    "# optimizer = optim.Adadelta(net.parameters(),lr= 0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=8, \n",
    "                                                 threshold = 1e-12, verbose=True)\n",
    "\n",
    "for epoch in range(300):\n",
    "    start = time.time()\n",
    "    \n",
    "    avg_epoch_train_p_loss   = 0\n",
    "    avg_epoch_val_p_loss     = 0 \n",
    "    ade_g  = 0\n",
    "    fde_g  = 0\n",
    "    ade_train_g  = 0\n",
    "    fde_train_g  = 0\n",
    "    counter = 0\n",
    "    net.train()\n",
    "    \n",
    "    for idx, (obs_pose_global, target_pose_global,obs_s_global,target_s_global) in enumerate(train_loader):\n",
    "        obs_pose_global = obs_pose_global.to(device='cuda')\n",
    "        target_pose_global = target_pose_global.to(device='cuda')\n",
    "        obs_s_global = obs_s_global.reshape(-1,15*15).to(device='cuda')\n",
    "        target_s_global = target_s_global.to(device='cuda')\n",
    "        \n",
    "        counter += 1        \n",
    "        \n",
    "        net.zero_grad()\n",
    "#         print(obs_pose_global.shape)\n",
    "        preds = net(obs_s_global)\n",
    "        preds_c=preds.clone().detach().reshape(-1,16,2)\n",
    "#         print(preds_c.shape,obs_pose_global.shape)\n",
    "        preds_g=speed2pos(preds_c,obs_pose_global)\n",
    "        \n",
    "        ade_train_g += float(ADE_c(preds_g, target_pose_global))\n",
    "        fde_train_g += float(FDE_c(preds_g, target_pose_global))\n",
    "      \n",
    "        loss  = l1e(preds, target_s_global.reshape(-1,16*2))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_epoch_train_p_loss += float(loss)\n",
    "\n",
    "    avg_epoch_train_p_loss /= counter\n",
    "    ade_train_g  /= counter\n",
    "    fde_train_g  /= counter   \n",
    "    \n",
    "\n",
    "    counter=0\n",
    "    net.eval()\n",
    "    for idx, (obs_pose_global, target_pose_global,obs_s_global,target_s_global) in enumerate(val_loader):\n",
    "        obs_pose_global = obs_pose_global.to(device='cuda')\n",
    "        target_pose_global = target_pose_global.to(device='cuda')\n",
    "        obs_s_global = obs_s_global.reshape(-1,15*15).to(device='cuda')\n",
    "        target_s_global = target_s_global.to(device='cuda')\n",
    "    \n",
    "        counter += 1        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = net(obs_s_global)\n",
    "            preds_c=preds.clone().detach().reshape(-1,16,2)\n",
    "            \n",
    "            preds_g=speed2pos(preds_c,obs_pose_global)\n",
    "\n",
    "            ade_g += float(ADE_c(preds_g, target_pose_global))\n",
    "            fde_g += float(FDE_c(preds_g, target_pose_global))\n",
    "\n",
    "            val_loss  = l1e(preds, target_s_global.reshape(-1,16*2))\n",
    "            \n",
    "            avg_epoch_val_p_loss += float(val_loss)\n",
    "      \n",
    "    avg_epoch_val_p_loss /= counter\n",
    "    ade_g  /= counter\n",
    "    fde_g  /= counter    \n",
    "   \n",
    "    \n",
    "    \n",
    "    scheduler.step(avg_epoch_train_p_loss)\n",
    "    \n",
    "    print('e:', epoch,'| loss: %.2f'%avg_epoch_train_p_loss,'| val_loss: %.2f'% avg_epoch_val_p_loss, '| ade_train_g: %.2f'% ade_train_g, '| ade_val_g: %.2f'% ade_g, '| fde_train_g: %.2f'% fde_train_g,'| fde_val_g: %.2f'% fde_g)\n",
    "\n",
    "print('='*100) \n",
    "# print('Saving ...')\n",
    "# torch.save(net.state_dict(), args.model_path)\n",
    "print('Done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "attempted-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./Posetrack/3dpw_train_in.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fitted-minimum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n"
     ]
    }
   ],
   "source": [
    "posesin=[]\n",
    "# posesout=[]\n",
    "for i in data:\n",
    "#     print(len(i))\n",
    "    for j in i:\n",
    "        posesin.append(j)\n",
    "# posesin=np.array(posesin)\n",
    "print(len(posesin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "44f6f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Posetrack/3dpw_train_out.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "      \n",
    "posesout=[]\n",
    "for i in data:\n",
    "    for j in i:\n",
    "        posesout.append(j)\n",
    "# posesout=np.array(posesout)\n",
    "# print(posesout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "adf32dcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Posetrack/3dpw_valid_masks_in.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-67067072a7f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./Posetrack/3dpw_valid_masks_in.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mread_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmaskin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Posetrack/3dpw_valid_masks_in.json'"
     ]
    }
   ],
   "source": [
    "with open(\"./Posetrack/3dpw_valid_masks_in.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "      \n",
    "maskin=[]\n",
    "for i in data:\n",
    "    for j in i:\n",
    "        maskin.append(j)\n",
    "mm=np.array(maskin)\n",
    "print(mm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "80a96002",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Posetrack/3dpw_train_masks_out.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "      \n",
    "maskout=[]\n",
    "for i in data:\n",
    "    for j in i:\n",
    "        maskout.append(j)\n",
    "# posesout=np.array(posesout)\n",
    "# print(maskout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ee3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Posetrack/posetrack_valid_masks_out.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "      \n",
    "maskout=[]\n",
    "for i in data:\n",
    "    for j in i:\n",
    "        maskout.append(j)\n",
    "# posesout=np.array(posesout)\n",
    "# print(maskout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "541bf1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = {'Pose': posesin,\n",
    "            'Future_Pose': posesout,\n",
    "#             'Mask': maskin,\n",
    "#             'Future_Mask': maskout\n",
    "        }\n",
    "df = pd.DataFrame (data_val, columns = ['Pose','Future_Pose'])#,'Mask','Future_Mask'\n",
    "df.to_csv(\"./3dpw_train.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ad058db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
