{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0153f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    " \n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import glob\n",
    "from PIL import Image, ImageDraw\n",
    "import time\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ce1b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_posetrack(nn.Module):\n",
    "    def __init__(self, args):\n",
    "\n",
    "        super(LSTM_posetrack, self).__init__()\n",
    "         \n",
    "        self.pose_encoder = nn.LSTM(input_size=28, hidden_size=args.hidden_size)\n",
    "        self.vel_encoder = nn.LSTM(input_size=28, hidden_size=args.hidden_size)\n",
    "        \n",
    "        self.pose_embedding = nn.Sequential(nn.Linear(in_features=args.hidden_size, out_features=28),\n",
    "                                           nn.ReLU())\n",
    "        \n",
    "        self.vel_decoder = nn.LSTMCell(input_size=28, hidden_size=args.hidden_size)\n",
    "        \n",
    "        self.fc_vel    = nn.Linear(in_features=args.hidden_size, out_features=28)\n",
    "        \n",
    "        self.hardtanh = nn.Hardtanh(min_val=-1*args.hardtanh_limit,max_val=args.hardtanh_limit)\n",
    "        self.relu = nn.LeakyReLU() \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.mask_encoder = nn.LSTM(input_size=14, hidden_size=args.hidden_size)\n",
    "        self.mask_decoder = nn.LSTMCell(input_size=14, hidden_size=args.hidden_size)\n",
    "        self.fc_mask    = nn.Linear(in_features=args.hidden_size, out_features=14)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.args = args\n",
    "        \n",
    "    def forward(self, pose=None, vel=None, mask=None):\n",
    "\n",
    "\n",
    "        _, (hidden_vel, cell_vel) = self.vel_encoder(vel.permute(1,0,2))\n",
    "        hidden_vel = hidden_vel.squeeze(0)\n",
    "        cell_vel = cell_vel.squeeze(0)\n",
    "\n",
    "\n",
    "        _, (hidden_pose, cell_pose) = self.pose_encoder(pose.permute(1,0,2))\n",
    "        hidden_pose = hidden_pose.squeeze(0)\n",
    "        cell_pose = cell_pose.squeeze(0)\n",
    "\n",
    "        \n",
    "        _, (hidden_dec2, cell_dec2) = self.mask_encoder(mask.permute(1,0,2))\n",
    "        hidden_dec2 = hidden_dec2.squeeze(0)\n",
    "        cell_dec2 = cell_dec2.squeeze(0)\n",
    "        \n",
    "        outputs = []\n",
    "        vel_outputs    = torch.tensor([], device=self.args.device)\n",
    "        mask_outputs    = torch.tensor([], device=self.args.device)\n",
    "        \n",
    "        VelDec_inp = vel[:,-1,:]\n",
    "        MaskDec_inp = mask[:,-1,:]\n",
    "        \n",
    "        hidden_dec = hidden_pose + hidden_vel\n",
    "        cell_dec = cell_pose + cell_vel\n",
    "        for i in range(self.args.output//self.args.skip):\n",
    "            hidden_dec, cell_dec = self.vel_decoder(VelDec_inp, (hidden_dec, cell_dec))\n",
    "            vel_output  = self.hardtanh(self.fc_vel(hidden_dec))\n",
    "            vel_outputs = torch.cat((vel_outputs, vel_output.unsqueeze(1)), dim = 1)\n",
    "            VelDec_inp  = vel_output.detach()\n",
    "            \n",
    "            hidden_dec2, cell_dec2 = self.mask_decoder(MaskDec_inp, (hidden_dec2, cell_dec2))\n",
    "            mask_output  = self.sigmoid(self.fc_mask(hidden_dec2))\n",
    "            mask_outputs = torch.cat((mask_outputs, mask_output.unsqueeze(1)), dim = 1)\n",
    "            MaskDec_inp  = mask_output.detach()\n",
    "                    \n",
    "            \n",
    "        outputs.append(vel_outputs)    \n",
    "        outputs.append(mask_outputs) \n",
    "        return tuple(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65ba03dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args():\n",
    "    def __init__(self):\n",
    "        self.jaad_dataset = '/data/smailait-data/JAAD/processed_annotations' #folder containing parsed jaad annotations (used when first time loading data)\n",
    "        self.dtype        = 'train'\n",
    "        self.from_file    = False #read dataset from csv file or reprocess data\n",
    "        self.save         = True\n",
    "        self.file         = '/data/smailait-data/jaad_train_16_16.csv'\n",
    "        self.save_path    = '/data/smailait-data/jaad_train_16_16.csv'\n",
    "        self.model_path    = '/data/smailait-data/models/multitask_pv_lstm_trained.pkl'\n",
    "        self.loader_workers = 1\n",
    "        self.loader_shuffle = False\n",
    "        self.pin_memory     = False\n",
    "        self.image_resize   = [240, 426]\n",
    "        self.device         = 'cuda'\n",
    "        self.batch_size     = 50\n",
    "        self.n_epochs       = 1000\n",
    "        self.hidden_size    = 1000\n",
    "        self.hardtanh_limit = 100\n",
    "        self.input  = 16\n",
    "        self.output = 14\n",
    "        self.stride = 16\n",
    "        self.skip   = 1\n",
    "        # self.task   = 'bounding_box-intention'\n",
    "        self.task   = 'pose'\n",
    "        self.use_scenes = False       \n",
    "        self.lr = 0.01\n",
    "        \n",
    "args = args()\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "l1e = nn.L1Loss()\n",
    "bce = nn.BCELoss()\n",
    "train_s_scores = []\n",
    "train_pose_scores=[]\n",
    "val_pose_scores=[]\n",
    "train_c_scores = []\n",
    "val_s_scores   = []\n",
    "val_c_scores   = []\n",
    "net = LSTM_posetrack(args).to(args.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "374570d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train\n",
      "Pose loaded\n",
      "Future_Pose loaded\n",
      "Mask loaded\n",
      "Future_Mask loaded\n",
      "******************************\n",
      "Loading valid\n",
      "Pose loaded\n",
      "Future_Pose loaded\n",
      "Mask loaded\n",
      "Future_Mask loaded\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "class myDataset_posetrack(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, dtype,fname):\n",
    "        \n",
    "        self.args = args\n",
    "        self.dtype = dtype\n",
    "        self.fname=fname\n",
    "        print(\"Loading\",self.dtype)\n",
    "        sequence_centric = pd.read_csv(self.fname+self.dtype+\".csv\")\n",
    "        df = sequence_centric.copy()      \n",
    "        for v in list(df.columns.values):\n",
    "            print(v+' loaded')\n",
    "            try:\n",
    "                df.loc[:,v] = df.loc[:, v].apply(lambda x: literal_eval(x))\n",
    "            except:\n",
    "                continue\n",
    "        sequence_centric[df.columns] = df[df.columns]\n",
    "        self.data = sequence_centric.copy().reset_index(drop=True)\n",
    "    \n",
    "        print('*'*30)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        seq = self.data.iloc[index]\n",
    "        outputs = []\n",
    "\n",
    "        obs = torch.tensor([seq.Pose[i] for i in range(0,self.args.input,self.args.skip)])\n",
    "        obs_speed = (obs[1:] - obs[:-1])\n",
    "        outputs.append(obs_speed)\n",
    "        true = torch.tensor([seq.Future_Pose[i] for i in range(0,self.args.output,self.args.skip)])\n",
    "        true_speed = torch.cat(((true[0]-obs[-1]).unsqueeze(0), true[1:]-true[:-1]))\n",
    "        outputs.append(true_speed)\n",
    "        outputs.append(obs)\n",
    "        outputs.append(true)\n",
    "        \n",
    "        if \"posetrack\" in self.fname:\n",
    "            obs_mask = torch.tensor([seq.Mask[i] for i in range(0,self.args.output,self.args.skip)])\n",
    "            true_mask = torch.tensor([seq.Future_Mask[i] for i in range(0,self.args.output,self.args.skip)])\n",
    "\n",
    "            outputs.append(obs_mask)\n",
    "            outputs.append(true_mask)\n",
    "        \n",
    "        return tuple(outputs)    \n",
    "    \n",
    "    \n",
    "def data_loader_posetrack(args,data,fname):\n",
    "    dataset = myDataset_posetrack(args,data,fname)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=args.batch_size, shuffle=args.loader_shuffle,\n",
    "        pin_memory=args.pin_memory)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "train_loader=data_loader_posetrack(args,\"train\",'processed_csvs/posetrack_')\n",
    "val_loader=data_loader_posetrack(args,\"valid\" ,'processed_csvs/posetrack_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a281c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADE_c(pred, true):\n",
    "    b,n,p=pred.size()[0],pred.size()[1],pred.size()[2]\n",
    "#     print(b,n,p)\n",
    "    pred = torch.reshape(pred, (b,n,int(p/2),2))\n",
    "    true = torch.reshape(true, (b,n,int(p/2),2))\n",
    "    \n",
    "    displacement=torch.sqrt((pred[:,:,:,0]-true[:,:,:,0])**2+(pred[:,:,:,1]-true[:,:,:,1])**2)\n",
    "    ade = torch.mean(torch.mean(displacement,dim=1))\n",
    "\n",
    "    return ade\n",
    "\n",
    "\n",
    "def FDE_c(pred, true):\n",
    "    b,n,p=pred.size()[0],pred.size()[1],pred.size()[2]\n",
    "#     print(b,n,p)\n",
    "    pred = torch.reshape(pred, (b,n,int(p/2),2))\n",
    "    true = torch.reshape(true, (b,n,int(p/2),2))\n",
    "    \n",
    "    displacement=torch.sqrt((pred[:,-1,:,0]-true[:,-1,:,0])**2+(pred[:,-1,:,1]-true[:,-1,:,1])**2)\n",
    "\n",
    "    fde = torch.mean(torch.mean(displacement,dim=1))\n",
    "    \n",
    "    return fde\n",
    "\n",
    "def speed2pos(preds, obs_p):\n",
    "    pred_pos = torch.zeros(preds.shape[0], preds.shape[1], preds.shape[2]).to('cuda')\n",
    "    current = obs_p[:,-1,:]\n",
    "    \n",
    "    for i in range(preds.shape[1]):\n",
    "        pred_pos[:,i,:] = current + preds[:,i,:]\n",
    "        current = pred_pos[:,i,:]\n",
    "        \n",
    "    for i in range(preds.shape[2]):\n",
    "        pred_pos[:,:,i] = torch.min(pred_pos[:,:,i], 1920*torch.ones(pred_pos.shape[0], pred_pos.shape[1], device='cuda'))\n",
    "        pred_pos[:,:,i] = torch.max(pred_pos[:,:,i], torch.zeros(pred_pos.shape[0], pred_pos.shape[1], device='cuda'))\n",
    "        \n",
    "    return pred_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2baa5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Training ...\n",
      "e: 0 | ts: 9.78 | vs: 11.84 | tm: 0.67 | vm: 0.47 | ade_train: 76.90 | ade_val: 90.18 | fde_train: 128.28 | fde_val: 150.90 | t:3.47\n",
      "e: 1 | ts: 9.68 | vs: 11.87 | tm: 0.45 | vm: 0.43 | ade_train: 75.34 | ade_val: 90.49 | fde_train: 126.30 | fde_val: 152.07 | t:3.32\n",
      "e: 2 | ts: 9.65 | vs: 11.82 | tm: 0.41 | vm: 0.43 | ade_train: 74.49 | ade_val: 89.99 | fde_train: 125.16 | fde_val: 151.50 | t:3.32\n",
      "e: 3 | ts: 9.63 | vs: 11.84 | tm: 0.39 | vm: 0.37 | ade_train: 74.17 | ade_val: 89.93 | fde_train: 124.74 | fde_val: 151.52 | t:3.33\n",
      "e: 4 | ts: 9.60 | vs: 11.80 | tm: 0.33 | vm: 0.33 | ade_train: 73.67 | ade_val: 89.05 | fde_train: 123.91 | fde_val: 149.93 | t:3.33\n",
      "e: 5 | ts: 9.58 | vs: 11.81 | tm: 0.31 | vm: 0.33 | ade_train: 73.10 | ade_val: 89.36 | fde_train: 123.00 | fde_val: 150.51 | t:3.36\n",
      "e: 6 | ts: 9.56 | vs: 11.80 | tm: 0.30 | vm: 0.31 | ade_train: 72.85 | ade_val: 89.50 | fde_train: 122.50 | fde_val: 150.81 | t:3.37\n",
      "e: 7 | ts: 9.54 | vs: 11.79 | tm: 0.28 | vm: 0.32 | ade_train: 72.55 | ade_val: 89.47 | fde_train: 122.00 | fde_val: 150.82 | t:3.38\n",
      "e: 8 | ts: 9.52 | vs: 11.79 | tm: 0.28 | vm: 0.30 | ade_train: 72.16 | ade_val: 88.66 | fde_train: 121.50 | fde_val: 149.75 | t:3.44\n",
      "e: 9 | ts: 9.51 | vs: 11.76 | tm: 0.26 | vm: 0.30 | ade_train: 71.93 | ade_val: 88.41 | fde_train: 121.04 | fde_val: 149.01 | t:3.49\n",
      "e: 10 | ts: 9.48 | vs: 11.75 | tm: 0.26 | vm: 0.30 | ade_train: 71.42 | ade_val: 88.16 | fde_train: 120.02 | fde_val: 148.70 | t:3.49\n",
      "e: 11 | ts: 9.48 | vs: 11.76 | tm: 0.25 | vm: 0.29 | ade_train: 71.60 | ade_val: 88.59 | fde_train: 120.63 | fde_val: 149.80 | t:3.54\n",
      "e: 12 | ts: 9.47 | vs: 11.71 | tm: 0.25 | vm: 0.30 | ade_train: 71.24 | ade_val: 87.88 | fde_train: 119.82 | fde_val: 147.98 | t:3.53\n",
      "e: 13 | ts: 9.47 | vs: 11.70 | tm: 0.24 | vm: 0.28 | ade_train: 71.16 | ade_val: 87.56 | fde_train: 119.86 | fde_val: 147.68 | t:3.55\n",
      "e: 14 | ts: 9.45 | vs: 11.74 | tm: 0.24 | vm: 0.28 | ade_train: 71.06 | ade_val: 87.98 | fde_train: 119.60 | fde_val: 148.71 | t:3.53\n",
      "e: 15 | ts: 9.45 | vs: 11.78 | tm: 0.23 | vm: 0.28 | ade_train: 71.09 | ade_val: 88.91 | fde_train: 119.82 | fde_val: 150.56 | t:3.54\n",
      "e: 16 | ts: 9.44 | vs: 11.75 | tm: 0.22 | vm: 0.27 | ade_train: 71.01 | ade_val: 88.38 | fde_train: 119.52 | fde_val: 149.00 | t:3.58\n",
      "e: 17 | ts: 9.43 | vs: 11.72 | tm: 0.22 | vm: 0.27 | ade_train: 70.77 | ade_val: 87.48 | fde_train: 119.31 | fde_val: 147.99 | t:3.52\n",
      "e: 18 | ts: 9.43 | vs: 11.77 | tm: 0.21 | vm: 0.27 | ade_train: 70.66 | ade_val: 88.16 | fde_train: 119.22 | fde_val: 149.70 | t:3.55\n",
      "e: 19 | ts: 9.39 | vs: 11.79 | tm: 0.21 | vm: 0.28 | ade_train: 70.14 | ade_val: 88.18 | fde_train: 118.38 | fde_val: 149.80 | t:3.57\n",
      "e: 20 | ts: 9.39 | vs: 11.87 | tm: 0.21 | vm: 0.28 | ade_train: 69.97 | ade_val: 89.35 | fde_train: 117.97 | fde_val: 151.85 | t:3.56\n",
      "e: 21 | ts: 9.40 | vs: 11.71 | tm: 0.21 | vm: 0.27 | ade_train: 70.10 | ade_val: 87.47 | fde_train: 118.07 | fde_val: 147.71 | t:3.53\n",
      "e: 22 | ts: 9.40 | vs: 11.71 | tm: 0.21 | vm: 0.28 | ade_train: 70.16 | ade_val: 88.04 | fde_train: 118.49 | fde_val: 148.82 | t:3.58\n",
      "e: 23 | ts: 9.36 | vs: 11.72 | tm: 0.20 | vm: 0.28 | ade_train: 69.56 | ade_val: 87.79 | fde_train: 117.15 | fde_val: 148.48 | t:3.59\n",
      "Epoch    25: reducing learning rate of group 0 to 5.0000e-03.\n",
      "e: 24 | ts: 9.36 | vs: 11.71 | tm: 0.20 | vm: 0.27 | ade_train: 69.55 | ade_val: 87.80 | fde_train: 117.10 | fde_val: 148.31 | t:3.65\n",
      "e: 25 | ts: 9.29 | vs: 11.72 | tm: 0.19 | vm: 0.26 | ade_train: 68.57 | ade_val: 87.58 | fde_train: 115.36 | fde_val: 148.32 | t:3.61\n",
      "e: 26 | ts: 9.23 | vs: 11.66 | tm: 0.19 | vm: 0.26 | ade_train: 68.01 | ade_val: 86.23 | fde_train: 114.38 | fde_val: 146.06 | t:3.56\n",
      "e: 27 | ts: 9.19 | vs: 11.67 | tm: 0.19 | vm: 0.26 | ade_train: 67.39 | ade_val: 86.44 | fde_train: 113.41 | fde_val: 146.57 | t:3.58\n",
      "e: 28 | ts: 9.16 | vs: 11.70 | tm: 0.18 | vm: 0.26 | ade_train: 66.88 | ade_val: 86.46 | fde_train: 112.70 | fde_val: 146.91 | t:3.61\n",
      "e: 29 | ts: 9.14 | vs: 11.67 | tm: 0.18 | vm: 0.26 | ade_train: 66.58 | ade_val: 86.17 | fde_train: 112.06 | fde_val: 146.17 | t:3.59\n",
      "e: 30 | ts: 9.13 | vs: 11.70 | tm: 0.17 | vm: 0.26 | ade_train: 66.43 | ade_val: 86.26 | fde_train: 111.75 | fde_val: 146.46 | t:3.59\n",
      "e: 31 | ts: 9.10 | vs: 11.66 | tm: 0.17 | vm: 0.26 | ade_train: 66.12 | ade_val: 85.50 | fde_train: 111.44 | fde_val: 145.13 | t:3.56\n",
      "e: 32 | ts: 9.09 | vs: 11.68 | tm: 0.17 | vm: 0.27 | ade_train: 65.90 | ade_val: 86.32 | fde_train: 110.99 | fde_val: 146.27 | t:3.59\n",
      "e: 33 | ts: 9.08 | vs: 11.64 | tm: 0.17 | vm: 0.27 | ade_train: 65.66 | ade_val: 85.72 | fde_train: 110.57 | fde_val: 145.19 | t:3.59\n",
      "e: 34 | ts: 9.06 | vs: 11.68 | tm: 0.17 | vm: 0.27 | ade_train: 65.49 | ade_val: 86.40 | fde_train: 110.26 | fde_val: 146.90 | t:3.59\n",
      "e: 35 | ts: 9.04 | vs: 11.68 | tm: 0.16 | vm: 0.27 | ade_train: 65.15 | ade_val: 85.74 | fde_train: 109.71 | fde_val: 145.32 | t:3.60\n",
      "e: 36 | ts: 9.03 | vs: 11.69 | tm: 0.16 | vm: 0.27 | ade_train: 65.07 | ade_val: 86.21 | fde_train: 109.56 | fde_val: 146.04 | t:3.54\n",
      "e: 37 | ts: 9.02 | vs: 11.64 | tm: 0.16 | vm: 0.27 | ade_train: 64.90 | ade_val: 85.48 | fde_train: 109.11 | fde_val: 144.75 | t:3.57\n",
      "e: 38 | ts: 9.00 | vs: 11.67 | tm: 0.16 | vm: 0.26 | ade_train: 64.61 | ade_val: 86.01 | fde_train: 108.71 | fde_val: 145.41 | t:3.73\n",
      "e: 39 | ts: 8.98 | vs: 11.67 | tm: 0.16 | vm: 0.27 | ade_train: 64.42 | ade_val: 85.58 | fde_train: 108.34 | fde_val: 144.84 | t:3.57\n",
      "e: 40 | ts: 8.99 | vs: 11.71 | tm: 0.16 | vm: 0.28 | ade_train: 64.49 | ade_val: 86.36 | fde_train: 108.57 | fde_val: 146.57 | t:3.57\n",
      "e: 41 | ts: 8.98 | vs: 11.69 | tm: 0.16 | vm: 0.28 | ade_train: 64.45 | ade_val: 86.17 | fde_train: 108.52 | fde_val: 146.10 | t:3.62\n",
      "e: 42 | ts: 8.97 | vs: 11.70 | tm: 0.16 | vm: 0.26 | ade_train: 64.22 | ade_val: 85.64 | fde_train: 107.92 | fde_val: 145.21 | t:3.63\n",
      "e: 43 | ts: 8.95 | vs: 11.70 | tm: 0.15 | vm: 0.27 | ade_train: 64.06 | ade_val: 85.61 | fde_train: 107.59 | fde_val: 145.21 | t:3.66\n",
      "e: 44 | ts: 8.95 | vs: 11.69 | tm: 0.15 | vm: 0.27 | ade_train: 64.12 | ade_val: 85.46 | fde_train: 107.57 | fde_val: 145.04 | t:3.74\n",
      "e: 45 | ts: 8.95 | vs: 11.64 | tm: 0.15 | vm: 0.27 | ade_train: 64.12 | ade_val: 84.97 | fde_train: 107.81 | fde_val: 143.60 | t:3.75\n",
      "e: 46 | ts: 8.94 | vs: 11.76 | tm: 0.15 | vm: 0.28 | ade_train: 64.06 | ade_val: 85.84 | fde_train: 107.59 | fde_val: 145.61 | t:3.64\n",
      "e: 47 | ts: 8.94 | vs: 11.71 | tm: 0.14 | vm: 0.28 | ade_train: 63.90 | ade_val: 85.50 | fde_train: 107.41 | fde_val: 145.02 | t:3.61\n",
      "Epoch    49: reducing learning rate of group 0 to 2.5000e-03.\n",
      "e: 48 | ts: 8.92 | vs: 11.75 | tm: 0.14 | vm: 0.29 | ade_train: 63.68 | ade_val: 86.11 | fde_train: 106.92 | fde_val: 146.47 | t:3.65\n",
      "e: 49 | ts: 8.89 | vs: 11.74 | tm: 0.14 | vm: 0.30 | ade_train: 63.39 | ade_val: 86.45 | fde_train: 106.57 | fde_val: 147.18 | t:3.58\n",
      "e: 50 | ts: 8.84 | vs: 11.76 | tm: 0.15 | vm: 0.29 | ade_train: 62.64 | ade_val: 87.03 | fde_train: 105.00 | fde_val: 148.36 | t:3.70\n",
      "e: 51 | ts: 8.79 | vs: 11.72 | tm: 0.14 | vm: 0.27 | ade_train: 62.15 | ade_val: 86.37 | fde_train: 104.13 | fde_val: 146.76 | t:3.64\n",
      "e: 52 | ts: 8.76 | vs: 11.69 | tm: 0.13 | vm: 0.28 | ade_train: 61.79 | ade_val: 86.14 | fde_train: 103.50 | fde_val: 146.03 | t:3.62\n",
      "e: 53 | ts: 8.72 | vs: 11.69 | tm: 0.13 | vm: 0.28 | ade_train: 61.32 | ade_val: 85.70 | fde_train: 102.63 | fde_val: 145.10 | t:3.63\n",
      "e: 54 | ts: 8.71 | vs: 11.73 | tm: 0.13 | vm: 0.28 | ade_train: 61.17 | ade_val: 85.90 | fde_train: 102.44 | fde_val: 145.81 | t:3.63\n",
      "e: 55 | ts: 8.68 | vs: 11.78 | tm: 0.13 | vm: 0.29 | ade_train: 61.00 | ade_val: 86.17 | fde_train: 102.16 | fde_val: 146.47 | t:3.69\n",
      "e: 56 | ts: 8.67 | vs: 11.80 | tm: 0.12 | vm: 0.29 | ade_train: 60.75 | ade_val: 86.50 | fde_train: 101.72 | fde_val: 147.23 | t:3.62\n",
      "e: 57 | ts: 8.65 | vs: 11.80 | tm: 0.12 | vm: 0.30 | ade_train: 60.56 | ade_val: 86.86 | fde_train: 101.56 | fde_val: 148.01 | t:3.62\n",
      "e: 58 | ts: 8.63 | vs: 11.76 | tm: 0.12 | vm: 0.30 | ade_train: 60.41 | ade_val: 86.68 | fde_train: 101.15 | fde_val: 147.67 | t:3.62\n",
      "Epoch    60: reducing learning rate of group 0 to 1.2500e-03.\n",
      "e: 59 | ts: 8.62 | vs: 11.80 | tm: 0.12 | vm: 0.30 | ade_train: 60.20 | ade_val: 87.04 | fde_train: 100.75 | fde_val: 148.44 | t:3.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 60 | ts: 8.59 | vs: 11.79 | tm: 0.12 | vm: 0.29 | ade_train: 60.00 | ade_val: 86.87 | fde_train: 100.49 | fde_val: 148.08 | t:3.90\n",
      "e: 61 | ts: 8.55 | vs: 11.81 | tm: 0.12 | vm: 0.30 | ade_train: 59.66 | ade_val: 86.67 | fde_train: 99.81 | fde_val: 147.81 | t:3.72\n",
      "e: 62 | ts: 8.53 | vs: 11.81 | tm: 0.12 | vm: 0.30 | ade_train: 59.42 | ade_val: 86.75 | fde_train: 99.40 | fde_val: 148.00 | t:3.67\n"
     ]
    }
   ],
   "source": [
    "net = LSTM_posetrack(args).to(args.device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=10, \n",
    "                                                 threshold = 1e-8, verbose=True)\n",
    "print('='*100)\n",
    "print('Training ...')\n",
    "\n",
    "for epoch in range(200):\n",
    "    start = time.time()\n",
    "    \n",
    "    avg_epoch_train_s_loss = 0\n",
    "    avg_epoch_val_s_loss   = 0\n",
    "    avg_epoch_train_p_loss   = 0\n",
    "    avg_epoch_val_p_loss     = 0 \n",
    "    \n",
    "    ade  = 0\n",
    "    fde  = 0\n",
    "    ade_train  = 0\n",
    "    fde_train  = 0\n",
    "    counter = 0\n",
    "    \n",
    "    for idx, (obs_s, target_s, obs_pose, target_pose, obs_mask, target_mask) in enumerate(train_loader):\n",
    "        counter += 1        \n",
    "        \n",
    "        obs_s    = obs_s.to(device='cuda')\n",
    "        target_s = target_s.to(device='cuda')\n",
    "        obs_pose    = obs_pose.to(device='cuda')\n",
    "        target_pose = target_pose.to(device='cuda')\n",
    "        obs_mask    = obs_mask.to(device='cuda')\n",
    "        target_mask = target_mask.to(device='cuda')\n",
    "        \n",
    "#         print(obs_s.shape)\n",
    "        \n",
    "        net.zero_grad()\n",
    "    \n",
    "        (speed_preds,mask_preds) = net(pose=obs_pose, vel=obs_s,mask=obs_mask)\n",
    "\n",
    "        speed_loss  = l1e(speed_preds, target_s)\n",
    "        mask_loss  = bce(mask_preds, target_mask)\n",
    "    \n",
    "        preds_p = speed2pos(speed_preds, obs_pose) \n",
    "        ade_train += float(ADE_c(preds_p, target_pose))\n",
    "        fde_train += float(FDE_c(preds_p, target_pose))\n",
    "        \n",
    "        loss= 0.7*speed_loss+0.3*mask_loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "        avg_epoch_train_s_loss += float(speed_loss)\n",
    "        avg_epoch_train_p_loss += float(mask_loss)\n",
    "\n",
    "    avg_epoch_train_s_loss /= counter\n",
    "    avg_epoch_train_p_loss /= counter\n",
    "    train_s_scores.append(avg_epoch_train_s_loss)\n",
    "    ade_train  /= counter\n",
    "    fde_train  /= counter    \n",
    "\n",
    "    counter=0\n",
    "\n",
    "    for idx, (obs_s, target_s, obs_pose, target_pose, obs_mask, target_mask) in enumerate(val_loader):\n",
    "        counter+=1\n",
    "        obs_s    = obs_s.to(device='cuda')\n",
    "        target_s = target_s.to(device='cuda')\n",
    "        obs_pose    = obs_pose.to(device='cuda')\n",
    "        target_pose = target_pose.to(device='cuda')\n",
    "        obs_mask    = obs_mask.to(device='cuda')\n",
    "        target_mask = target_mask.to(device='cuda')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            (speed_preds,mask_preds) = net(pose=obs_pose,vel=obs_s,mask=obs_mask)\n",
    "\n",
    "            speed_loss  = l1e(speed_preds, target_s)\n",
    "            mask_loss  = bce(mask_preds, target_mask)\n",
    "            \n",
    "            loss= 0.7*speed_loss+0.3*mask_loss\n",
    "            avg_epoch_val_s_loss += float(speed_loss)\n",
    "            avg_epoch_val_p_loss += float(mask_loss)\n",
    "        \n",
    "            preds_p = speed2pos(speed_preds, obs_pose)\n",
    "            ade += float(ADE_c(preds_p, target_pose))\n",
    "            fde += float(FDE_c(preds_p, target_pose))\n",
    "\n",
    "        \n",
    "    avg_epoch_val_s_loss /= counter\n",
    "    avg_epoch_val_p_loss /= counter\n",
    "    val_s_scores.append(avg_epoch_val_s_loss)\n",
    "    \n",
    "    ade  /= counter\n",
    "    fde  /= counter     \n",
    "   \n",
    "    scheduler.step(avg_epoch_val_s_loss)\n",
    "    \n",
    "     \n",
    "    print('e:', epoch, '| ts: %.2f'% avg_epoch_train_s_loss,  '| vs: %.2f'% avg_epoch_val_s_loss,'| tm: %.2f'% avg_epoch_train_p_loss,  '| vm: %.2f'% avg_epoch_val_p_loss, '| ade_train: %.2f'% ade_train, '| ade_val: %.2f'% ade, '| fde_train: %.2f'% fde_train,'| fde_val: %.2f'% fde,\n",
    "          '| t:%.2f'%(time.time()-start))\n",
    "\n",
    "\n",
    "print('='*100) \n",
    "# print('Saving ...')\n",
    "# torch.save(net.state_dict(), args.model_path)\n",
    "print('Done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bab4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"posetrack_test_in.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "\n",
    "with open(\"posetrack_test_masks_in.json\", \"r\") as read_file:\n",
    "    data_m = json.load(read_file)\n",
    "    \n",
    "out_data=[]\n",
    "out_mask=[]\n",
    "for i in range(len(data)):\n",
    "    lp=[]\n",
    "    lm=[]\n",
    "    for j in range(len(data[i])):\n",
    "        pose=torch.tensor(data[i][j]).unsqueeze(0).to(args.device)\n",
    "        mask=torch.tensor(data_m[i][j]).unsqueeze(0).to(args.device)\n",
    "        vel= pose[:,1:] - pose[:,:-1]\n",
    "#         print(vel.shape)\n",
    "#         break\n",
    "        (speed_preds,mask_preds) = net(pose=pose,vel=vel,mask=mask)\n",
    "        \n",
    "        preds_p = speed2pos(speed_preds, pose)\n",
    "        pred=preds_p.squeeze(0)\n",
    "        mask_pred=mask_preds.squeeze(0)\n",
    "        lp.append(pred.tolist())\n",
    "        lm.append(mask_pred.detach().cpu().numpy().round().tolist())\n",
    "#         print(pred.shape)\n",
    "#         break\n",
    "    out_data.append(lp)\n",
    "    out_mask.append(lm)\n",
    "#     break\n",
    "with open('posetrack_test_masks_out.json', 'w') as f:\n",
    "    json.dump(out_mask, f)\n",
    "with open('posetrack_test_out.json', 'w') as f:\n",
    "    json.dump(out_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
